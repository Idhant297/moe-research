{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy moe\n",
    "\n",
    "by claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 num_experts: int = 8,\n",
    "                 num_experts_per_tok: int = 2,\n",
    "                 router_aux_loss_coef: float = 0.01,\n",
    "                 norm_topk_prob: bool = False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.router_aux_loss_coef = router_aux_loss_coef\n",
    "        self.norm_topk_prob = norm_topk_prob\n",
    "\n",
    "        # Router network - maps input to expert selection logits\n",
    "        self.router = nn.Linear(input_dim, num_experts, bias=False)\n",
    "        \n",
    "        # Create experts - each is a simple MLP\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, input_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        router_logits = self.router(x)  # [batch, seq, num_experts]\n",
    "        \n",
    "        # Get routing probabilities \n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts per token\n",
    "        topk_probs, topk_indices = torch.topk(\n",
    "            router_probs, self.num_experts_per_tok, dim=-1\n",
    "        )\n",
    "        \n",
    "        if self.norm_topk_prob:\n",
    "            topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Initialize output with zeros\n",
    "        outputs = torch.zeros_like(x)\n",
    "        router_z_loss = torch.zeros(1, device=x.device)\n",
    "        \n",
    "        # Compute load balancing auxiliary loss\n",
    "        # Measures how evenly experts are utilized\n",
    "        if self.training:\n",
    "            # Mean probability of selecting each expert\n",
    "            expert_usage = router_probs.mean(dim=(0,1))\n",
    "            router_z_loss = self.router_aux_loss_coef * torch.mean(\n",
    "                router_logits * router_probs\n",
    "            )\n",
    "            \n",
    "        # Process tokens through their selected experts\n",
    "        x_flat = x.reshape(-1, self.input_dim)\n",
    "        for i in range(self.num_experts_per_tok):\n",
    "            expert_index = topk_indices[:, :, i]  # [batch, seq]\n",
    "            prob = topk_probs[:, :, i]  # [batch, seq] \n",
    "            \n",
    "            flat_indices = expert_index.reshape(-1)  # [batch * seq]\n",
    "            flat_probs = prob.reshape(-1)  # [batch * seq]\n",
    "            \n",
    "            # Process each expert\n",
    "            for expert_id in range(self.num_experts):\n",
    "                expert_mask = (flat_indices == expert_id)\n",
    "                if not expert_mask.any():\n",
    "                    continue\n",
    "                    \n",
    "                # Get inputs for this expert\n",
    "                expert_input = x_flat[expert_mask]\n",
    "                expert_prob = flat_probs[expert_mask]\n",
    "                \n",
    "                # Process through expert\n",
    "                expert_output = self.experts[expert_id](expert_input)\n",
    "                expert_output = expert_output * expert_prob.unsqueeze(-1)\n",
    "                \n",
    "                # Add to output\n",
    "                outputs_flat = outputs.reshape(-1, self.input_dim)\n",
    "                outputs_flat[expert_mask] += expert_output\n",
    "                \n",
    "        return outputs, router_z_loss\n",
    "\n",
    "class ToyMoETransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 d_model: int = 256,\n",
    "                 nhead: int = 4,\n",
    "                 num_layers: int = 2,\n",
    "                 num_experts: int = 8,\n",
    "                 num_experts_per_tok: int = 2,\n",
    "                 expert_hidden_multiplier: int = 4,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Create transformer layers with MoE\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerMoELayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                num_experts=num_experts,\n",
    "                num_experts_per_tok=num_experts_per_tok,\n",
    "                expert_hidden_dim=d_model * expert_hidden_multiplier,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        x = self.vocab_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        router_aux_loss = 0\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, layer_loss = layer(\n",
    "                x, \n",
    "                src_mask=src_mask,\n",
    "                src_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "            router_aux_loss += layer_loss\n",
    "            \n",
    "        x = self.final_norm(x)\n",
    "        output = self.output_proj(x)\n",
    "        \n",
    "        return output, router_aux_loss\n",
    "\n",
    "class TransformerMoELayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 num_experts: int,\n",
    "                 num_experts_per_tok: int,\n",
    "                 expert_hidden_dim: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.moe = MoELayer(\n",
    "            input_dim=d_model,\n",
    "            hidden_dim=expert_hidden_dim,\n",
    "            num_experts=num_experts,\n",
    "            num_experts_per_tok=num_experts_per_tok\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, src_mask=None, src_key_padding_mask=None):\n",
    "        # Self attention block\n",
    "        x2 = self.norm1(x)\n",
    "        x2 = self.self_attn(\n",
    "            x2, x2, x2,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask\n",
    "        )[0]\n",
    "        x = x + self.dropout(x2)\n",
    "        \n",
    "        # MoE block\n",
    "        x2 = self.norm2(x)\n",
    "        x2, router_loss = self.moe(x2)\n",
    "        x = x + self.dropout(x2)\n",
    "        \n",
    "        return x, router_loss\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a toy model\n",
    "    model = ToyMoETransformer(\n",
    "        vocab_size=1000,\n",
    "        d_model=256,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        num_experts=8,\n",
    "        num_experts_per_tok=2\n",
    "    )\n",
    "    \n",
    "    # Create sample input\n",
    "    src = torch.randint(0, 1000, (10, 32))  # seq_len=10, batch_size=32\n",
    "    \n",
    "    # Forward pass\n",
    "    output, router_loss = model(src)\n",
    "    print(f\"Output shape: {output.shape}\")  # [10, 32, 1000]\n",
    "    print(f\"Router loss: {router_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
