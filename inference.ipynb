{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OlmoeForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import kl_div\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference for olmoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6381803e89474328b07e8364280b222f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OlmoeForCausalLM(\n",
       "  (model): OlmoeModel(\n",
       "    (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x OlmoeDecoderLayer(\n",
       "        (self_attn): OlmoeSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "          (k_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "        (mlp): OlmoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x OlmoeMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (down_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): OlmoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n",
    "\n",
    "# Set the model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to get the router logits and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expert_routing(input_text, model, tokenizer, layer_num):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Print the tokenized input\n",
    "    print(\"Tokenized input:\")\n",
    "    for token_id in inputs.input_ids[0]:\n",
    "        token = tokenizer.decode([token_id])\n",
    "        print(f\"Token: '{token}', ID: {token_id.item()}\")\n",
    "\n",
    "    print(f\"\\nInput shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "    # Forward pass with output_router_logits=True\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_router_logits=True)\n",
    "\n",
    "    # Get the router logits from the specified layer\n",
    "    router_logits = outputs.router_logits[layer_num]\n",
    "\n",
    "    # Print which layer the logits are from\n",
    "    print(f\"\\nRouter logits are from layer {layer_num} of the model\")\n",
    "\n",
    "    # Initialize a dictionary to store the analysis results\n",
    "    analysis_results = {\n",
    "        \"input_text\": input_text,\n",
    "        \"tokens\": []\n",
    "    }\n",
    "\n",
    "    # Print router logits and probabilities for each token\n",
    "    print(\"\\nRouter logits and probabilities for each token:\")\n",
    "    for token_idx, token_id in enumerate(inputs.input_ids[0]):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        logits = router_logits[token_idx]\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        print(f\"Token: '{token}' (ID: {token_id.item()})\")\n",
    "        for expert_idx, (logit, prob) in enumerate(zip(logits, probabilities)):\n",
    "            print(f\"  expert {expert_idx}: logit = {logit.item():.4f}, post-softmax = {prob.item():.4f}\")\n",
    "        print()\n",
    "        \n",
    "        token_data = {\n",
    "            \"token\": token,\n",
    "            \"id\": token_id.item(),\n",
    "            \"router_probability\": probabilities.tolist()\n",
    "        }\n",
    "        analysis_results[\"tokens\"].append(token_data)\n",
    "\n",
    "    # Print the top-k experts for each token\n",
    "    k = 8\n",
    "    print(f\"\\nTop {k} experts for each token:\")\n",
    "    for token_idx, token_id in enumerate(inputs.input_ids[0]):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        probabilities = F.softmax(router_logits[token_idx], dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
    "        \n",
    "        print(f\"Token: '{token}' (ID: {token_id.item()})\")\n",
    "        for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):\n",
    "            print(f\"  {i+1}. expert {idx.item()}: probability = {prob.item():.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Save the analysis results as a JSON file with a unique name\n",
    "    base_filename = \"json/expert_routing_analysis\"\n",
    "    counter = 1\n",
    "    filename = f\"{base_filename}_{counter}.json\"\n",
    "    while os.path.exists(filename):\n",
    "        counter += 1\n",
    "        filename = f\"{base_filename}_{counter}.json\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "    print(f\"Analysis results saved to {filename}\")\n",
    "\n",
    "    return router_logits, analysis_results\n",
    "\n",
    "layer_num = 0 # layer to analyze (0-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input:\n",
      "Token: 'The', ID: 510\n",
      "Token: ' tennis', ID: 23354\n",
      "Token: ' match', ID: 3761\n",
      "Token: ' was', ID: 369\n",
      "Token: ' thrilling', ID: 47330\n",
      "Token: ' to', ID: 281\n",
      "Token: ' watch', ID: 3698\n",
      "Token: '.', ID: 15\n",
      "\n",
      "Input shape: torch.Size([1, 8])\n",
      "\n",
      "Router logits are from layer 0 of the model\n",
      "\n",
      "Router logits and probabilities for each token:\n",
      "Token: 'The' (ID: 510)\n",
      "  expert 0: logit = 0.1502, post-softmax = 0.0232\n",
      "  expert 1: logit = -0.7955, post-softmax = 0.0090\n",
      "  expert 2: logit = -0.2742, post-softmax = 0.0152\n",
      "  expert 3: logit = -0.5139, post-softmax = 0.0119\n",
      "  expert 4: logit = -0.3479, post-softmax = 0.0141\n",
      "  expert 5: logit = 2.1354, post-softmax = 0.1689\n",
      "  expert 6: logit = 0.6977, post-softmax = 0.0401\n",
      "  expert 7: logit = -1.5684, post-softmax = 0.0042\n",
      "  expert 8: logit = -1.7015, post-softmax = 0.0036\n",
      "  expert 9: logit = -0.0581, post-softmax = 0.0188\n",
      "  expert 10: logit = -0.7583, post-softmax = 0.0094\n",
      "  expert 11: logit = -3.2744, post-softmax = 0.0008\n",
      "  expert 12: logit = -0.7148, post-softmax = 0.0098\n",
      "  expert 13: logit = -2.4690, post-softmax = 0.0017\n",
      "  expert 14: logit = 1.5773, post-softmax = 0.0967\n",
      "  expert 15: logit = -0.3817, post-softmax = 0.0136\n",
      "  expert 16: logit = -1.8577, post-softmax = 0.0031\n",
      "  expert 17: logit = 0.1657, post-softmax = 0.0236\n",
      "  expert 18: logit = 0.9428, post-softmax = 0.0513\n",
      "  expert 19: logit = 0.3115, post-softmax = 0.0273\n",
      "  expert 20: logit = -3.7719, post-softmax = 0.0005\n",
      "  expert 21: logit = -0.9706, post-softmax = 0.0076\n",
      "  expert 22: logit = -2.1968, post-softmax = 0.0022\n",
      "  expert 23: logit = -0.5567, post-softmax = 0.0114\n",
      "  expert 24: logit = -1.9000, post-softmax = 0.0030\n",
      "  expert 25: logit = -0.7874, post-softmax = 0.0091\n",
      "  expert 26: logit = -1.0002, post-softmax = 0.0073\n",
      "  expert 27: logit = -0.3801, post-softmax = 0.0137\n",
      "  expert 28: logit = -0.6797, post-softmax = 0.0101\n",
      "  expert 29: logit = -1.2544, post-softmax = 0.0057\n",
      "  expert 30: logit = -0.5793, post-softmax = 0.0112\n",
      "  expert 31: logit = -0.4165, post-softmax = 0.0132\n",
      "  expert 32: logit = -0.7096, post-softmax = 0.0098\n",
      "  expert 33: logit = -1.9046, post-softmax = 0.0030\n",
      "  expert 34: logit = -2.0343, post-softmax = 0.0026\n",
      "  expert 35: logit = -0.9315, post-softmax = 0.0079\n",
      "  expert 36: logit = -0.8641, post-softmax = 0.0084\n",
      "  expert 37: logit = -0.5478, post-softmax = 0.0115\n",
      "  expert 38: logit = 0.6766, post-softmax = 0.0393\n",
      "  expert 39: logit = -0.4943, post-softmax = 0.0122\n",
      "  expert 40: logit = -1.4531, post-softmax = 0.0047\n",
      "  expert 41: logit = 0.1789, post-softmax = 0.0239\n",
      "  expert 42: logit = -1.2848, post-softmax = 0.0055\n",
      "  expert 43: logit = -0.7053, post-softmax = 0.0099\n",
      "  expert 44: logit = -0.9761, post-softmax = 0.0075\n",
      "  expert 45: logit = 0.0690, post-softmax = 0.0214\n",
      "  expert 46: logit = -2.3658, post-softmax = 0.0019\n",
      "  expert 47: logit = -0.9225, post-softmax = 0.0079\n",
      "  expert 48: logit = -0.6294, post-softmax = 0.0106\n",
      "  expert 49: logit = -0.6963, post-softmax = 0.0100\n",
      "  expert 50: logit = -0.8324, post-softmax = 0.0087\n",
      "  expert 51: logit = -0.8328, post-softmax = 0.0087\n",
      "  expert 52: logit = -0.9448, post-softmax = 0.0078\n",
      "  expert 53: logit = -2.4923, post-softmax = 0.0017\n",
      "  expert 54: logit = -1.0505, post-softmax = 0.0070\n",
      "  expert 55: logit = -0.4208, post-softmax = 0.0131\n",
      "  expert 56: logit = -0.6439, post-softmax = 0.0105\n",
      "  expert 57: logit = -3.1679, post-softmax = 0.0008\n",
      "  expert 58: logit = -1.9147, post-softmax = 0.0029\n",
      "  expert 59: logit = -1.3653, post-softmax = 0.0051\n",
      "  expert 60: logit = -1.2059, post-softmax = 0.0060\n",
      "  expert 61: logit = 1.2763, post-softmax = 0.0716\n",
      "  expert 62: logit = -0.5063, post-softmax = 0.0120\n",
      "  expert 63: logit = -1.3773, post-softmax = 0.0050\n",
      "\n",
      "Token: ' tennis' (ID: 23354)\n",
      "  expert 0: logit = -0.8404, post-softmax = 0.0079\n",
      "  expert 1: logit = -0.5546, post-softmax = 0.0106\n",
      "  expert 2: logit = -0.3523, post-softmax = 0.0129\n",
      "  expert 3: logit = -0.6202, post-softmax = 0.0099\n",
      "  expert 4: logit = -0.3214, post-softmax = 0.0133\n",
      "  expert 5: logit = 2.1056, post-softmax = 0.1510\n",
      "  expert 6: logit = 0.5916, post-softmax = 0.0332\n",
      "  expert 7: logit = -3.6364, post-softmax = 0.0005\n",
      "  expert 8: logit = -0.3285, post-softmax = 0.0132\n",
      "  expert 9: logit = 0.8101, post-softmax = 0.0413\n",
      "  expert 10: logit = -0.5712, post-softmax = 0.0104\n",
      "  expert 11: logit = -1.5883, post-softmax = 0.0038\n",
      "  expert 12: logit = -0.3410, post-softmax = 0.0131\n",
      "  expert 13: logit = -0.6221, post-softmax = 0.0099\n",
      "  expert 14: logit = 1.1216, post-softmax = 0.0564\n",
      "  expert 15: logit = 0.0145, post-softmax = 0.0187\n",
      "  expert 16: logit = -1.3786, post-softmax = 0.0046\n",
      "  expert 17: logit = 0.1206, post-softmax = 0.0207\n",
      "  expert 18: logit = 1.5539, post-softmax = 0.0870\n",
      "  expert 19: logit = -2.2012, post-softmax = 0.0020\n",
      "  expert 20: logit = -1.9417, post-softmax = 0.0026\n",
      "  expert 21: logit = -1.8683, post-softmax = 0.0028\n",
      "  expert 22: logit = -1.6379, post-softmax = 0.0036\n",
      "  expert 23: logit = -0.2971, post-softmax = 0.0137\n",
      "  expert 24: logit = -1.1263, post-softmax = 0.0060\n",
      "  expert 25: logit = -0.3769, post-softmax = 0.0126\n",
      "  expert 26: logit = -0.5034, post-softmax = 0.0111\n",
      "  expert 27: logit = -0.8373, post-softmax = 0.0080\n",
      "  expert 28: logit = -1.5231, post-softmax = 0.0040\n",
      "  expert 29: logit = -0.8550, post-softmax = 0.0078\n",
      "  expert 30: logit = -0.4993, post-softmax = 0.0112\n",
      "  expert 31: logit = -0.3378, post-softmax = 0.0131\n",
      "  expert 32: logit = -0.4642, post-softmax = 0.0116\n",
      "  expert 33: logit = -1.2532, post-softmax = 0.0053\n",
      "  expert 34: logit = -1.2005, post-softmax = 0.0055\n",
      "  expert 35: logit = -0.8069, post-softmax = 0.0082\n",
      "  expert 36: logit = -0.3123, post-softmax = 0.0135\n",
      "  expert 37: logit = -0.3065, post-softmax = 0.0135\n",
      "  expert 38: logit = -0.4211, post-softmax = 0.0121\n",
      "  expert 39: logit = -1.4849, post-softmax = 0.0042\n",
      "  expert 40: logit = -0.5069, post-softmax = 0.0111\n",
      "  expert 41: logit = 1.0362, post-softmax = 0.0518\n",
      "  expert 42: logit = -0.7104, post-softmax = 0.0090\n",
      "  expert 43: logit = -1.7177, post-softmax = 0.0033\n",
      "  expert 44: logit = -0.8818, post-softmax = 0.0076\n",
      "  expert 45: logit = -0.4029, post-softmax = 0.0123\n",
      "  expert 46: logit = -1.9497, post-softmax = 0.0026\n",
      "  expert 47: logit = -0.7703, post-softmax = 0.0085\n",
      "  expert 48: logit = -0.4508, post-softmax = 0.0117\n",
      "  expert 49: logit = -0.1552, post-softmax = 0.0157\n",
      "  expert 50: logit = -0.4234, post-softmax = 0.0120\n",
      "  expert 51: logit = -0.8144, post-softmax = 0.0081\n",
      "  expert 52: logit = -0.0531, post-softmax = 0.0174\n",
      "  expert 53: logit = -0.6770, post-softmax = 0.0093\n",
      "  expert 54: logit = -0.9817, post-softmax = 0.0069\n",
      "  expert 55: logit = -0.0218, post-softmax = 0.0180\n",
      "  expert 56: logit = -0.2688, post-softmax = 0.0141\n",
      "  expert 57: logit = -1.4473, post-softmax = 0.0043\n",
      "  expert 58: logit = -0.0123, post-softmax = 0.0182\n",
      "  expert 59: logit = -0.5644, post-softmax = 0.0105\n",
      "  expert 60: logit = -0.6124, post-softmax = 0.0100\n",
      "  expert 61: logit = 0.4774, post-softmax = 0.0296\n",
      "  expert 62: logit = -0.6046, post-softmax = 0.0100\n",
      "  expert 63: logit = -0.9604, post-softmax = 0.0070\n",
      "\n",
      "Token: ' match' (ID: 3761)\n",
      "  expert 0: logit = -2.4277, post-softmax = 0.0017\n",
      "  expert 1: logit = -0.6078, post-softmax = 0.0103\n",
      "  expert 2: logit = -0.5474, post-softmax = 0.0110\n",
      "  expert 3: logit = 0.0511, post-softmax = 0.0199\n",
      "  expert 4: logit = -0.7570, post-softmax = 0.0089\n",
      "  expert 5: logit = -0.8874, post-softmax = 0.0078\n",
      "  expert 6: logit = 0.3998, post-softmax = 0.0283\n",
      "  expert 7: logit = -1.0609, post-softmax = 0.0066\n",
      "  expert 8: logit = -1.6749, post-softmax = 0.0036\n",
      "  expert 9: logit = -0.4576, post-softmax = 0.0120\n",
      "  expert 10: logit = 0.8302, post-softmax = 0.0435\n",
      "  expert 11: logit = -0.9907, post-softmax = 0.0070\n",
      "  expert 12: logit = -1.4366, post-softmax = 0.0045\n",
      "  expert 13: logit = -1.0639, post-softmax = 0.0065\n",
      "  expert 14: logit = -1.8944, post-softmax = 0.0029\n",
      "  expert 15: logit = -0.3717, post-softmax = 0.0131\n",
      "  expert 16: logit = -1.2142, post-softmax = 0.0056\n",
      "  expert 17: logit = 0.4845, post-softmax = 0.0308\n",
      "  expert 18: logit = -1.0952, post-softmax = 0.0063\n",
      "  expert 19: logit = -1.1419, post-softmax = 0.0061\n",
      "  expert 20: logit = -3.1601, post-softmax = 0.0008\n",
      "  expert 21: logit = -2.2532, post-softmax = 0.0020\n",
      "  expert 22: logit = -0.0299, post-softmax = 0.0184\n",
      "  expert 23: logit = -0.2583, post-softmax = 0.0146\n",
      "  expert 24: logit = -0.8033, post-softmax = 0.0085\n",
      "  expert 25: logit = -0.5687, post-softmax = 0.0107\n",
      "  expert 26: logit = -0.6894, post-softmax = 0.0095\n",
      "  expert 27: logit = -0.2834, post-softmax = 0.0143\n",
      "  expert 28: logit = -3.1352, post-softmax = 0.0008\n",
      "  expert 29: logit = 0.0877, post-softmax = 0.0207\n",
      "  expert 30: logit = -0.0605, post-softmax = 0.0178\n",
      "  expert 31: logit = -0.6494, post-softmax = 0.0099\n",
      "  expert 32: logit = -0.7298, post-softmax = 0.0091\n",
      "  expert 33: logit = -1.5083, post-softmax = 0.0042\n",
      "  expert 34: logit = -2.1045, post-softmax = 0.0023\n",
      "  expert 35: logit = -0.7909, post-softmax = 0.0086\n",
      "  expert 36: logit = -0.9379, post-softmax = 0.0074\n",
      "  expert 37: logit = -0.0884, post-softmax = 0.0174\n",
      "  expert 38: logit = -0.5201, post-softmax = 0.0113\n",
      "  expert 39: logit = -0.9015, post-softmax = 0.0077\n",
      "  expert 40: logit = -0.2972, post-softmax = 0.0141\n",
      "  expert 41: logit = 0.9072, post-softmax = 0.0470\n",
      "  expert 42: logit = 1.1837, post-softmax = 0.0619\n",
      "  expert 43: logit = -1.5417, post-softmax = 0.0041\n",
      "  expert 44: logit = 2.2055, post-softmax = 0.1720\n",
      "  expert 45: logit = -0.8304, post-softmax = 0.0083\n",
      "  expert 46: logit = 0.7636, post-softmax = 0.0407\n",
      "  expert 47: logit = -0.7026, post-softmax = 0.0094\n",
      "  expert 48: logit = -0.3358, post-softmax = 0.0135\n",
      "  expert 49: logit = -0.3890, post-softmax = 0.0128\n",
      "  expert 50: logit = -0.7713, post-softmax = 0.0088\n",
      "  expert 51: logit = -0.5101, post-softmax = 0.0114\n",
      "  expert 52: logit = -0.9282, post-softmax = 0.0075\n",
      "  expert 53: logit = -0.5249, post-softmax = 0.0112\n",
      "  expert 54: logit = -0.4149, post-softmax = 0.0125\n",
      "  expert 55: logit = -0.2534, post-softmax = 0.0147\n",
      "  expert 56: logit = -0.3945, post-softmax = 0.0128\n",
      "  expert 57: logit = -1.7400, post-softmax = 0.0033\n",
      "  expert 58: logit = -0.0868, post-softmax = 0.0174\n",
      "  expert 59: logit = -0.7293, post-softmax = 0.0091\n",
      "  expert 60: logit = 1.1166, post-softmax = 0.0579\n",
      "  expert 61: logit = -1.9690, post-softmax = 0.0026\n",
      "  expert 62: logit = -0.8706, post-softmax = 0.0079\n",
      "  expert 63: logit = -1.0334, post-softmax = 0.0067\n",
      "\n",
      "Token: ' was' (ID: 369)\n",
      "  expert 0: logit = -1.9440, post-softmax = 0.0034\n",
      "  expert 1: logit = -0.8791, post-softmax = 0.0098\n",
      "  expert 2: logit = -0.4828, post-softmax = 0.0146\n",
      "  expert 3: logit = -0.4477, post-softmax = 0.0151\n",
      "  expert 4: logit = -1.3382, post-softmax = 0.0062\n",
      "  expert 5: logit = -0.6571, post-softmax = 0.0122\n",
      "  expert 6: logit = 0.9382, post-softmax = 0.0603\n",
      "  expert 7: logit = -0.7004, post-softmax = 0.0117\n",
      "  expert 8: logit = -0.7117, post-softmax = 0.0116\n",
      "  expert 9: logit = -3.3751, post-softmax = 0.0008\n",
      "  expert 10: logit = -0.0333, post-softmax = 0.0228\n",
      "  expert 11: logit = -2.4396, post-softmax = 0.0021\n",
      "  expert 12: logit = -3.1616, post-softmax = 0.0010\n",
      "  expert 13: logit = -0.4161, post-softmax = 0.0156\n",
      "  expert 14: logit = -1.2609, post-softmax = 0.0067\n",
      "  expert 15: logit = -0.5441, post-softmax = 0.0137\n",
      "  expert 16: logit = 0.5446, post-softmax = 0.0407\n",
      "  expert 17: logit = -0.0365, post-softmax = 0.0228\n",
      "  expert 18: logit = -1.1754, post-softmax = 0.0073\n",
      "  expert 19: logit = -0.9628, post-softmax = 0.0090\n",
      "  expert 20: logit = -3.5636, post-softmax = 0.0007\n",
      "  expert 21: logit = -0.8926, post-softmax = 0.0097\n",
      "  expert 22: logit = 1.1214, post-softmax = 0.0725\n",
      "  expert 23: logit = -0.3634, post-softmax = 0.0164\n",
      "  expert 24: logit = 0.1146, post-softmax = 0.0265\n",
      "  expert 25: logit = -0.4563, post-softmax = 0.0150\n",
      "  expert 26: logit = -0.4886, post-softmax = 0.0145\n",
      "  expert 27: logit = 0.1490, post-softmax = 0.0274\n",
      "  expert 28: logit = -3.2320, post-softmax = 0.0009\n",
      "  expert 29: logit = -0.1732, post-softmax = 0.0199\n",
      "  expert 30: logit = -0.8648, post-softmax = 0.0099\n",
      "  expert 31: logit = -1.1287, post-softmax = 0.0076\n",
      "  expert 32: logit = -0.6477, post-softmax = 0.0124\n",
      "  expert 33: logit = -0.1986, post-softmax = 0.0194\n",
      "  expert 34: logit = -2.3854, post-softmax = 0.0022\n",
      "  expert 35: logit = -1.4286, post-softmax = 0.0057\n",
      "  expert 36: logit = -1.0333, post-softmax = 0.0084\n",
      "  expert 37: logit = -1.0807, post-softmax = 0.0080\n",
      "  expert 38: logit = -1.1043, post-softmax = 0.0078\n",
      "  expert 39: logit = -0.7317, post-softmax = 0.0114\n",
      "  expert 40: logit = -0.7992, post-softmax = 0.0106\n",
      "  expert 41: logit = -2.5490, post-softmax = 0.0018\n",
      "  expert 42: logit = -0.3481, post-softmax = 0.0167\n",
      "  expert 43: logit = -0.8167, post-softmax = 0.0104\n",
      "  expert 44: logit = -0.1733, post-softmax = 0.0199\n",
      "  expert 45: logit = -0.8392, post-softmax = 0.0102\n",
      "  expert 46: logit = -0.0499, post-softmax = 0.0225\n",
      "  expert 47: logit = -0.4942, post-softmax = 0.0144\n",
      "  expert 48: logit = -0.7139, post-softmax = 0.0116\n",
      "  expert 49: logit = 0.6802, post-softmax = 0.0466\n",
      "  expert 50: logit = -0.6607, post-softmax = 0.0122\n",
      "  expert 51: logit = -1.5375, post-softmax = 0.0051\n",
      "  expert 52: logit = -0.9384, post-softmax = 0.0092\n",
      "  expert 53: logit = -0.4643, post-softmax = 0.0148\n",
      "  expert 54: logit = -0.2602, post-softmax = 0.0182\n",
      "  expert 55: logit = -0.1669, post-softmax = 0.0200\n",
      "  expert 56: logit = -0.5332, post-softmax = 0.0139\n",
      "  expert 57: logit = -0.0607, post-softmax = 0.0222\n",
      "  expert 58: logit = -3.5368, post-softmax = 0.0007\n",
      "  expert 59: logit = 0.4239, post-softmax = 0.0361\n",
      "  expert 60: logit = 0.8029, post-softmax = 0.0527\n",
      "  expert 61: logit = -0.6445, post-softmax = 0.0124\n",
      "  expert 62: logit = -0.4380, post-softmax = 0.0152\n",
      "  expert 63: logit = -0.2029, post-softmax = 0.0193\n",
      "\n",
      "Token: ' thrilling' (ID: 47330)\n",
      "  expert 0: logit = -1.5412, post-softmax = 0.0048\n",
      "  expert 1: logit = -0.7486, post-softmax = 0.0105\n",
      "  expert 2: logit = -0.4199, post-softmax = 0.0146\n",
      "  expert 3: logit = -0.6969, post-softmax = 0.0111\n",
      "  expert 4: logit = -0.3708, post-softmax = 0.0154\n",
      "  expert 5: logit = -1.0813, post-softmax = 0.0075\n",
      "  expert 6: logit = 0.6148, post-softmax = 0.0412\n",
      "  expert 7: logit = -2.4247, post-softmax = 0.0020\n",
      "  expert 8: logit = -1.3683, post-softmax = 0.0057\n",
      "  expert 9: logit = -0.9724, post-softmax = 0.0084\n",
      "  expert 10: logit = -0.1063, post-softmax = 0.0200\n",
      "  expert 11: logit = -0.5082, post-softmax = 0.0134\n",
      "  expert 12: logit = -0.5454, post-softmax = 0.0129\n",
      "  expert 13: logit = -0.4140, post-softmax = 0.0147\n",
      "  expert 14: logit = -0.4195, post-softmax = 0.0146\n",
      "  expert 15: logit = -0.1798, post-softmax = 0.0186\n",
      "  expert 16: logit = -1.0466, post-softmax = 0.0078\n",
      "  expert 17: logit = -0.3918, post-softmax = 0.0150\n",
      "  expert 18: logit = -1.1576, post-softmax = 0.0070\n",
      "  expert 19: logit = -1.6605, post-softmax = 0.0042\n",
      "  expert 20: logit = -1.3482, post-softmax = 0.0058\n",
      "  expert 21: logit = -1.3801, post-softmax = 0.0056\n",
      "  expert 22: logit = -1.7882, post-softmax = 0.0037\n",
      "  expert 23: logit = -0.1568, post-softmax = 0.0190\n",
      "  expert 24: logit = -0.8518, post-softmax = 0.0095\n",
      "  expert 25: logit = -0.5417, post-softmax = 0.0129\n",
      "  expert 26: logit = -0.2724, post-softmax = 0.0170\n",
      "  expert 27: logit = -0.7457, post-softmax = 0.0106\n",
      "  expert 28: logit = -1.2019, post-softmax = 0.0067\n",
      "  expert 29: logit = -0.4678, post-softmax = 0.0139\n",
      "  expert 30: logit = -0.7222, post-softmax = 0.0108\n",
      "  expert 31: logit = 1.5299, post-softmax = 0.1028\n",
      "  expert 32: logit = -0.4554, post-softmax = 0.0141\n",
      "  expert 33: logit = 0.6742, post-softmax = 0.0437\n",
      "  expert 34: logit = -1.7235, post-softmax = 0.0040\n",
      "  expert 35: logit = -0.9937, post-softmax = 0.0082\n",
      "  expert 36: logit = -0.4272, post-softmax = 0.0145\n",
      "  expert 37: logit = 0.0475, post-softmax = 0.0233\n",
      "  expert 38: logit = -1.7949, post-softmax = 0.0037\n",
      "  expert 39: logit = -2.6215, post-softmax = 0.0016\n",
      "  expert 40: logit = -0.5183, post-softmax = 0.0133\n",
      "  expert 41: logit = 1.0409, post-softmax = 0.0630\n",
      "  expert 42: logit = -0.4570, post-softmax = 0.0141\n",
      "  expert 43: logit = -2.4493, post-softmax = 0.0019\n",
      "  expert 44: logit = -0.5994, post-softmax = 0.0122\n",
      "  expert 45: logit = -0.1975, post-softmax = 0.0183\n",
      "  expert 46: logit = -2.7122, post-softmax = 0.0015\n",
      "  expert 47: logit = -0.5181, post-softmax = 0.0133\n",
      "  expert 48: logit = 0.5714, post-softmax = 0.0394\n",
      "  expert 49: logit = -0.4069, post-softmax = 0.0148\n",
      "  expert 50: logit = -0.5534, post-softmax = 0.0128\n",
      "  expert 51: logit = -0.3401, post-softmax = 0.0158\n",
      "  expert 52: logit = -0.8235, post-softmax = 0.0098\n",
      "  expert 53: logit = -0.8833, post-softmax = 0.0092\n",
      "  expert 54: logit = -0.7783, post-softmax = 0.0102\n",
      "  expert 55: logit = -0.4114, post-softmax = 0.0148\n",
      "  expert 56: logit = 0.8644, post-softmax = 0.0528\n",
      "  expert 57: logit = -1.1021, post-softmax = 0.0074\n",
      "  expert 58: logit = 0.0237, post-softmax = 0.0228\n",
      "  expert 59: logit = -0.2335, post-softmax = 0.0176\n",
      "  expert 60: logit = -0.2169, post-softmax = 0.0179\n",
      "  expert 61: logit = -0.7919, post-softmax = 0.0101\n",
      "  expert 62: logit = -0.3060, post-softmax = 0.0164\n",
      "  expert 63: logit = -0.8315, post-softmax = 0.0097\n",
      "\n",
      "Token: ' to' (ID: 281)\n",
      "  expert 0: logit = -1.8775, post-softmax = 0.0032\n",
      "  expert 1: logit = -0.6073, post-softmax = 0.0114\n",
      "  expert 2: logit = -0.7444, post-softmax = 0.0099\n",
      "  expert 3: logit = -0.2176, post-softmax = 0.0168\n",
      "  expert 4: logit = -0.8478, post-softmax = 0.0089\n",
      "  expert 5: logit = -0.3996, post-softmax = 0.0140\n",
      "  expert 6: logit = 1.1167, post-softmax = 0.0636\n",
      "  expert 7: logit = -0.6699, post-softmax = 0.0107\n",
      "  expert 8: logit = 1.8468, post-softmax = 0.1321\n",
      "  expert 9: logit = -2.8630, post-softmax = 0.0012\n",
      "  expert 10: logit = -0.5751, post-softmax = 0.0117\n",
      "  expert 11: logit = -0.7879, post-softmax = 0.0095\n",
      "  expert 12: logit = -2.8293, post-softmax = 0.0012\n",
      "  expert 13: logit = -0.9097, post-softmax = 0.0084\n",
      "  expert 14: logit = -0.6510, post-softmax = 0.0109\n",
      "  expert 15: logit = -0.6181, post-softmax = 0.0112\n",
      "  expert 16: logit = -0.9470, post-softmax = 0.0081\n",
      "  expert 17: logit = -0.8770, post-softmax = 0.0087\n",
      "  expert 18: logit = -1.0125, post-softmax = 0.0076\n",
      "  expert 19: logit = -0.6251, post-softmax = 0.0111\n",
      "  expert 20: logit = -2.9808, post-softmax = 0.0011\n",
      "  expert 21: logit = -0.8586, post-softmax = 0.0088\n",
      "  expert 22: logit = -0.1917, post-softmax = 0.0172\n",
      "  expert 23: logit = -0.3110, post-softmax = 0.0153\n",
      "  expert 24: logit = -0.7404, post-softmax = 0.0099\n",
      "  expert 25: logit = 0.0809, post-softmax = 0.0226\n",
      "  expert 26: logit = -0.4438, post-softmax = 0.0134\n",
      "  expert 27: logit = -0.3149, post-softmax = 0.0152\n",
      "  expert 28: logit = -1.6234, post-softmax = 0.0041\n",
      "  expert 29: logit = -0.2220, post-softmax = 0.0167\n",
      "  expert 30: logit = -1.3974, post-softmax = 0.0052\n",
      "  expert 31: logit = 0.1417, post-softmax = 0.0240\n",
      "  expert 32: logit = -0.5442, post-softmax = 0.0121\n",
      "  expert 33: logit = -2.1030, post-softmax = 0.0025\n",
      "  expert 34: logit = -1.6122, post-softmax = 0.0042\n",
      "  expert 35: logit = -1.3289, post-softmax = 0.0055\n",
      "  expert 36: logit = -0.4979, post-softmax = 0.0127\n",
      "  expert 37: logit = -0.2763, post-softmax = 0.0158\n",
      "  expert 38: logit = -0.6602, post-softmax = 0.0108\n",
      "  expert 39: logit = 0.1004, post-softmax = 0.0230\n",
      "  expert 40: logit = -0.3719, post-softmax = 0.0144\n",
      "  expert 41: logit = -2.5254, post-softmax = 0.0017\n",
      "  expert 42: logit = -0.4768, post-softmax = 0.0129\n",
      "  expert 43: logit = -0.5127, post-softmax = 0.0125\n",
      "  expert 44: logit = 0.0136, post-softmax = 0.0211\n",
      "  expert 45: logit = -0.7070, post-softmax = 0.0103\n",
      "  expert 46: logit = 0.3266, post-softmax = 0.0289\n",
      "  expert 47: logit = -0.5349, post-softmax = 0.0122\n",
      "  expert 48: logit = -1.0431, post-softmax = 0.0073\n",
      "  expert 49: logit = 0.1221, post-softmax = 0.0235\n",
      "  expert 50: logit = 1.4377, post-softmax = 0.0877\n",
      "  expert 51: logit = -1.7880, post-softmax = 0.0035\n",
      "  expert 52: logit = -0.1107, post-softmax = 0.0186\n",
      "  expert 53: logit = -0.1464, post-softmax = 0.0180\n",
      "  expert 54: logit = -0.4001, post-softmax = 0.0140\n",
      "  expert 55: logit = -0.7064, post-softmax = 0.0103\n",
      "  expert 56: logit = 0.0132, post-softmax = 0.0211\n",
      "  expert 57: logit = -0.3851, post-softmax = 0.0142\n",
      "  expert 58: logit = -2.7837, post-softmax = 0.0013\n",
      "  expert 59: logit = 0.2172, post-softmax = 0.0259\n",
      "  expert 60: logit = -1.4359, post-softmax = 0.0050\n",
      "  expert 61: logit = -0.9883, post-softmax = 0.0078\n",
      "  expert 62: logit = -0.3932, post-softmax = 0.0141\n",
      "  expert 63: logit = -0.4035, post-softmax = 0.0139\n",
      "\n",
      "Token: ' watch' (ID: 3698)\n",
      "  expert 0: logit = -2.2297, post-softmax = 0.0029\n",
      "  expert 1: logit = -0.9474, post-softmax = 0.0103\n",
      "  expert 2: logit = -0.8022, post-softmax = 0.0120\n",
      "  expert 3: logit = -0.4225, post-softmax = 0.0175\n",
      "  expert 4: logit = -0.4235, post-softmax = 0.0175\n",
      "  expert 5: logit = -1.2109, post-softmax = 0.0079\n",
      "  expert 6: logit = 0.6920, post-softmax = 0.0533\n",
      "  expert 7: logit = -2.0136, post-softmax = 0.0036\n",
      "  expert 8: logit = -0.2238, post-softmax = 0.0213\n",
      "  expert 9: logit = -1.2800, post-softmax = 0.0074\n",
      "  expert 10: logit = -0.6049, post-softmax = 0.0146\n",
      "  expert 11: logit = -0.8362, post-softmax = 0.0116\n",
      "  expert 12: logit = -1.2636, post-softmax = 0.0075\n",
      "  expert 13: logit = -0.1993, post-softmax = 0.0218\n",
      "  expert 14: logit = -0.5964, post-softmax = 0.0147\n",
      "  expert 15: logit = -0.5579, post-softmax = 0.0153\n",
      "  expert 16: logit = -0.5774, post-softmax = 0.0150\n",
      "  expert 17: logit = -1.1913, post-softmax = 0.0081\n",
      "  expert 18: logit = -0.3902, post-softmax = 0.0180\n",
      "  expert 19: logit = -0.5940, post-softmax = 0.0147\n",
      "  expert 20: logit = -1.8288, post-softmax = 0.0043\n",
      "  expert 21: logit = -1.8207, post-softmax = 0.0043\n",
      "  expert 22: logit = -2.7296, post-softmax = 0.0017\n",
      "  expert 23: logit = -0.7792, post-softmax = 0.0122\n",
      "  expert 24: logit = -1.1410, post-softmax = 0.0085\n",
      "  expert 25: logit = -0.3130, post-softmax = 0.0195\n",
      "  expert 26: logit = 1.1167, post-softmax = 0.0814\n",
      "  expert 27: logit = -0.4125, post-softmax = 0.0176\n",
      "  expert 28: logit = -2.3906, post-softmax = 0.0024\n",
      "  expert 29: logit = -0.2979, post-softmax = 0.0198\n",
      "  expert 30: logit = -1.3717, post-softmax = 0.0068\n",
      "  expert 31: logit = -0.5233, post-softmax = 0.0158\n",
      "  expert 32: logit = -0.4880, post-softmax = 0.0164\n",
      "  expert 33: logit = -0.1601, post-softmax = 0.0227\n",
      "  expert 34: logit = -2.4483, post-softmax = 0.0023\n",
      "  expert 35: logit = 0.4515, post-softmax = 0.0419\n",
      "  expert 36: logit = -0.6366, post-softmax = 0.0141\n",
      "  expert 37: logit = -0.5419, post-softmax = 0.0155\n",
      "  expert 38: logit = -0.6100, post-softmax = 0.0145\n",
      "  expert 39: logit = -1.5497, post-softmax = 0.0057\n",
      "  expert 40: logit = -0.5442, post-softmax = 0.0155\n",
      "  expert 41: logit = 0.7161, post-softmax = 0.0546\n",
      "  expert 42: logit = -1.0853, post-softmax = 0.0090\n",
      "  expert 43: logit = -1.6319, post-softmax = 0.0052\n",
      "  expert 44: logit = -0.5092, post-softmax = 0.0160\n",
      "  expert 45: logit = -1.0361, post-softmax = 0.0095\n",
      "  expert 46: logit = -0.7661, post-softmax = 0.0124\n",
      "  expert 47: logit = -0.3095, post-softmax = 0.0196\n",
      "  expert 48: logit = -0.8907, post-softmax = 0.0109\n",
      "  expert 49: logit = -0.3396, post-softmax = 0.0190\n",
      "  expert 50: logit = -0.0464, post-softmax = 0.0255\n",
      "  expert 51: logit = -0.5380, post-softmax = 0.0156\n",
      "  expert 52: logit = -0.4680, post-softmax = 0.0167\n",
      "  expert 53: logit = -0.6784, post-softmax = 0.0135\n",
      "  expert 54: logit = -0.7145, post-softmax = 0.0130\n",
      "  expert 55: logit = -0.4358, post-softmax = 0.0172\n",
      "  expert 56: logit = -0.6306, post-softmax = 0.0142\n",
      "  expert 57: logit = -1.7842, post-softmax = 0.0045\n",
      "  expert 58: logit = -0.8184, post-softmax = 0.0118\n",
      "  expert 59: logit = -0.0955, post-softmax = 0.0242\n",
      "  expert 60: logit = -1.0999, post-softmax = 0.0089\n",
      "  expert 61: logit = -0.7560, post-softmax = 0.0125\n",
      "  expert 62: logit = -0.4351, post-softmax = 0.0173\n",
      "  expert 63: logit = -0.8601, post-softmax = 0.0113\n",
      "\n",
      "Token: '.' (ID: 15)\n",
      "  expert 0: logit = -2.5039, post-softmax = 0.0021\n",
      "  expert 1: logit = -0.8554, post-softmax = 0.0107\n",
      "  expert 2: logit = -0.4411, post-softmax = 0.0162\n",
      "  expert 3: logit = -1.0861, post-softmax = 0.0085\n",
      "  expert 4: logit = -1.4106, post-softmax = 0.0061\n",
      "  expert 5: logit = -0.7835, post-softmax = 0.0115\n",
      "  expert 6: logit = 1.2040, post-softmax = 0.0839\n",
      "  expert 7: logit = -0.6645, post-softmax = 0.0129\n",
      "  expert 8: logit = -0.1032, post-softmax = 0.0227\n",
      "  expert 9: logit = -3.0557, post-softmax = 0.0012\n",
      "  expert 10: logit = -0.5188, post-softmax = 0.0150\n",
      "  expert 11: logit = -2.0787, post-softmax = 0.0031\n",
      "  expert 12: logit = -2.2712, post-softmax = 0.0026\n",
      "  expert 13: logit = -0.4519, post-softmax = 0.0160\n",
      "  expert 14: logit = -0.0976, post-softmax = 0.0228\n",
      "  expert 15: logit = -0.0377, post-softmax = 0.0242\n",
      "  expert 16: logit = -0.5575, post-softmax = 0.0144\n",
      "  expert 17: logit = -0.6338, post-softmax = 0.0133\n",
      "  expert 18: logit = -0.4662, post-softmax = 0.0158\n",
      "  expert 19: logit = 0.2530, post-softmax = 0.0324\n",
      "  expert 20: logit = -1.7479, post-softmax = 0.0044\n",
      "  expert 21: logit = -1.5967, post-softmax = 0.0051\n",
      "  expert 22: logit = -1.2843, post-softmax = 0.0070\n",
      "  expert 23: logit = -0.1031, post-softmax = 0.0227\n",
      "  expert 24: logit = -0.6283, post-softmax = 0.0134\n",
      "  expert 25: logit = -0.5012, post-softmax = 0.0152\n",
      "  expert 26: logit = -0.1378, post-softmax = 0.0219\n",
      "  expert 27: logit = -0.1269, post-softmax = 0.0222\n",
      "  expert 28: logit = -2.3006, post-softmax = 0.0025\n",
      "  expert 29: logit = -0.1888, post-softmax = 0.0208\n",
      "  expert 30: logit = -0.5025, post-softmax = 0.0152\n",
      "  expert 31: logit = -0.6012, post-softmax = 0.0138\n",
      "  expert 32: logit = -0.5334, post-softmax = 0.0148\n",
      "  expert 33: logit = 0.3120, post-softmax = 0.0344\n",
      "  expert 34: logit = -2.1901, post-softmax = 0.0028\n",
      "  expert 35: logit = -0.5880, post-softmax = 0.0140\n",
      "  expert 36: logit = 0.6491, post-softmax = 0.0481\n",
      "  expert 37: logit = -0.3828, post-softmax = 0.0172\n",
      "  expert 38: logit = -0.8649, post-softmax = 0.0106\n",
      "  expert 39: logit = -0.5213, post-softmax = 0.0149\n",
      "  expert 40: logit = -0.6043, post-softmax = 0.0137\n",
      "  expert 41: logit = -2.5936, post-softmax = 0.0019\n",
      "  expert 42: logit = -0.6200, post-softmax = 0.0135\n",
      "  expert 43: logit = -0.8563, post-softmax = 0.0107\n",
      "  expert 44: logit = -0.7344, post-softmax = 0.0121\n",
      "  expert 45: logit = -0.9234, post-softmax = 0.0100\n",
      "  expert 46: logit = -0.9849, post-softmax = 0.0094\n",
      "  expert 47: logit = -0.3084, post-softmax = 0.0185\n",
      "  expert 48: logit = -0.7326, post-softmax = 0.0121\n",
      "  expert 49: logit = -0.3649, post-softmax = 0.0175\n",
      "  expert 50: logit = -0.9665, post-softmax = 0.0096\n",
      "  expert 51: logit = -2.1044, post-softmax = 0.0031\n",
      "  expert 52: logit = -0.4608, post-softmax = 0.0159\n",
      "  expert 53: logit = -0.2230, post-softmax = 0.0201\n",
      "  expert 54: logit = -0.4109, post-softmax = 0.0167\n",
      "  expert 55: logit = -0.6358, post-softmax = 0.0133\n",
      "  expert 56: logit = -0.6928, post-softmax = 0.0126\n",
      "  expert 57: logit = -1.4000, post-softmax = 0.0062\n",
      "  expert 58: logit = -2.3785, post-softmax = 0.0023\n",
      "  expert 59: logit = 0.3921, post-softmax = 0.0372\n",
      "  expert 60: logit = -0.2898, post-softmax = 0.0188\n",
      "  expert 61: logit = -0.4019, post-softmax = 0.0168\n",
      "  expert 62: logit = -0.3766, post-softmax = 0.0173\n",
      "  expert 63: logit = 0.3102, post-softmax = 0.0343\n",
      "\n",
      "\n",
      "Top 8 experts for each token:\n",
      "Token: 'The' (ID: 510)\n",
      "  1. expert 5: probability = 0.1689\n",
      "  2. expert 14: probability = 0.0967\n",
      "  3. expert 61: probability = 0.0716\n",
      "  4. expert 18: probability = 0.0513\n",
      "  5. expert 6: probability = 0.0401\n",
      "  6. expert 38: probability = 0.0393\n",
      "  7. expert 19: probability = 0.0273\n",
      "  8. expert 41: probability = 0.0239\n",
      "\n",
      "Token: ' tennis' (ID: 23354)\n",
      "  1. expert 5: probability = 0.1510\n",
      "  2. expert 18: probability = 0.0870\n",
      "  3. expert 14: probability = 0.0564\n",
      "  4. expert 41: probability = 0.0518\n",
      "  5. expert 9: probability = 0.0413\n",
      "  6. expert 6: probability = 0.0332\n",
      "  7. expert 61: probability = 0.0296\n",
      "  8. expert 17: probability = 0.0207\n",
      "\n",
      "Token: ' match' (ID: 3761)\n",
      "  1. expert 44: probability = 0.1720\n",
      "  2. expert 42: probability = 0.0619\n",
      "  3. expert 60: probability = 0.0579\n",
      "  4. expert 41: probability = 0.0470\n",
      "  5. expert 10: probability = 0.0435\n",
      "  6. expert 46: probability = 0.0407\n",
      "  7. expert 17: probability = 0.0308\n",
      "  8. expert 6: probability = 0.0283\n",
      "\n",
      "Token: ' was' (ID: 369)\n",
      "  1. expert 22: probability = 0.0725\n",
      "  2. expert 6: probability = 0.0603\n",
      "  3. expert 60: probability = 0.0527\n",
      "  4. expert 49: probability = 0.0466\n",
      "  5. expert 16: probability = 0.0407\n",
      "  6. expert 59: probability = 0.0361\n",
      "  7. expert 27: probability = 0.0274\n",
      "  8. expert 24: probability = 0.0265\n",
      "\n",
      "Token: ' thrilling' (ID: 47330)\n",
      "  1. expert 31: probability = 0.1028\n",
      "  2. expert 41: probability = 0.0630\n",
      "  3. expert 56: probability = 0.0528\n",
      "  4. expert 33: probability = 0.0437\n",
      "  5. expert 6: probability = 0.0412\n",
      "  6. expert 48: probability = 0.0394\n",
      "  7. expert 37: probability = 0.0233\n",
      "  8. expert 58: probability = 0.0228\n",
      "\n",
      "Token: ' to' (ID: 281)\n",
      "  1. expert 8: probability = 0.1321\n",
      "  2. expert 50: probability = 0.0877\n",
      "  3. expert 6: probability = 0.0636\n",
      "  4. expert 46: probability = 0.0289\n",
      "  5. expert 59: probability = 0.0259\n",
      "  6. expert 31: probability = 0.0240\n",
      "  7. expert 49: probability = 0.0235\n",
      "  8. expert 39: probability = 0.0230\n",
      "\n",
      "Token: ' watch' (ID: 3698)\n",
      "  1. expert 26: probability = 0.0814\n",
      "  2. expert 41: probability = 0.0546\n",
      "  3. expert 6: probability = 0.0533\n",
      "  4. expert 35: probability = 0.0419\n",
      "  5. expert 50: probability = 0.0255\n",
      "  6. expert 59: probability = 0.0242\n",
      "  7. expert 33: probability = 0.0227\n",
      "  8. expert 13: probability = 0.0218\n",
      "\n",
      "Token: '.' (ID: 15)\n",
      "  1. expert 6: probability = 0.0839\n",
      "  2. expert 36: probability = 0.0481\n",
      "  3. expert 59: probability = 0.0372\n",
      "  4. expert 33: probability = 0.0344\n",
      "  5. expert 63: probability = 0.0343\n",
      "  6. expert 19: probability = 0.0324\n",
      "  7. expert 15: probability = 0.0242\n",
      "  8. expert 14: probability = 0.0228\n",
      "\n",
      "Analysis results saved to expert_routing_analysis_1.json\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The tennis match was thrilling to watch.\"\n",
    "router_logits, analysis_results = analyze_expert_routing(input_text, model, tokenizer,layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyze_expert_routing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe online app found her a good match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m router_logits_2, analysis_results_2 \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_expert_routing\u001b[49m(input_text_2, model, tokenizer, layer_num)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_expert_routing' is not defined"
     ]
    }
   ],
   "source": [
    "input_text_2 = \"The online app found her a good match.\"\n",
    "router_logits_2, analysis_results_2 = analyze_expert_routing(input_text_2, model, tokenizer, layer_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_token_kl_divergence(json_file1, json_file2, target_token):\n",
    "    # Load JSON data from files\n",
    "    with open(json_file1, 'r') as f1, open(json_file2, 'r') as f2:\n",
    "        data1 = json.load(f1)\n",
    "        data2 = json.load(f2)\n",
    "    \n",
    "    # Find the target token in both inputs\n",
    "    token1 = next((t for t in data1['tokens'] if t['token'] == target_token or t['id'] == target_token), None)\n",
    "    token2 = next((t for t in data2['tokens'] if t['token'] == target_token or t['id'] == target_token), None)\n",
    "    \n",
    "    if not token1 or not token2:\n",
    "        raise ValueError(f\"Token '{target_token}' not found in one or both inputs\")\n",
    "    \n",
    "    # Get router probabilities for the target token\n",
    "    probs1 = np.array(token1['router_probability'])\n",
    "    probs2 = np.array(token2['router_probability'])\n",
    "    \n",
    "    # Ensure probabilities sum to 1\n",
    "    probs1 = probs1 / np.sum(probs1)\n",
    "    probs2 = probs2 / np.sum(probs2)\n",
    "    \n",
    "    # Calculate KL divergence for each expert\n",
    "    kl_divergences = kl_div(probs1, probs2)\n",
    "    \n",
    "    # Create a dictionary of expert-wise KL divergences\n",
    "    expert_kl = {f\"Expert_{i}\": kl for i, kl in enumerate(kl_divergences)}\n",
    "    \n",
    "    # Calculate the total KL divergence\n",
    "    total_kl = np.sum(kl_divergences)\n",
    "    \n",
    "    result = {\n",
    "        \"token\": target_token,\n",
    "        \"expert_kl_divergences\": expert_kl,\n",
    "        \"total_kl_divergence\": total_kl\n",
    "    }\n",
    "    \n",
    "    # Save the result as a JSON file\n",
    "    base_filename = \"kl_divergence_analysis\"\n",
    "    counter = 1\n",
    "    filename = f\"{base_filename}_{counter}.json\"\n",
    "    while os.path.exists(filename):\n",
    "        counter += 1\n",
    "        filename = f\"{base_filename}_{target_token}_{counter}.json\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    print(f\"KL divergence analysis results saved to {filename}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence analysis results saved to kl_divergence_analysis_1.json\n",
      "{\n",
      "  \"token\": 3761,\n",
      "  \"expert_kl_divergences\": {\n",
      "    \"Expert_0\": 0.0007812551380871591,\n",
      "    \"Expert_1\": 0.00010657651477648418,\n",
      "    \"Expert_2\": 0.0018868115937772031,\n",
      "    \"Expert_3\": 0.003657222177318448,\n",
      "    \"Expert_4\": 0.02627870109798979,\n",
      "    \"Expert_5\": 0.00020753548841244591,\n",
      "    \"Expert_6\": 0.006246837614801284,\n",
      "    \"Expert_7\": 0.00032112401034644494,\n",
      "    \"Expert_8\": 0.0062337563288345,\n",
      "    \"Expert_9\": 0.009274871867047087,\n",
      "    \"Expert_10\": 0.05506577895205873,\n",
      "    \"Expert_11\": 0.0037240712460497646,\n",
      "    \"Expert_12\": 0.0006189850341964769,\n",
      "    \"Expert_13\": 0.0002855744818948073,\n",
      "    \"Expert_14\": 0.004536782226628571,\n",
      "    \"Expert_15\": 0.00014146661485302942,\n",
      "    \"Expert_16\": 0.0026401448071286925,\n",
      "    \"Expert_17\": 0.006159151167502408,\n",
      "    \"Expert_18\": 1.6855209224618181e-06,\n",
      "    \"Expert_19\": 0.001297812824272446,\n",
      "    \"Expert_20\": 0.003679239357394368,\n",
      "    \"Expert_21\": 0.003841600094424787,\n",
      "    \"Expert_22\": 0.017021835965967777,\n",
      "    \"Expert_23\": 6.325050314589034e-06,\n",
      "    \"Expert_24\": 0.0006430402807279783,\n",
      "    \"Expert_25\": 0.008024622670379872,\n",
      "    \"Expert_26\": 0.0004566971711456539,\n",
      "    \"Expert_27\": 0.00010634480887789996,\n",
      "    \"Expert_28\": 0.003088508850769698,\n",
      "    \"Expert_29\": 0.0010269137529428703,\n",
      "    \"Expert_30\": 0.0012957583863349286,\n",
      "    \"Expert_31\": 0.001413963212234453,\n",
      "    \"Expert_32\": 0.002846117670938288,\n",
      "    \"Expert_33\": 0.0017026479899575004,\n",
      "    \"Expert_34\": 0.0017768531303663654,\n",
      "    \"Expert_35\": 0.0005512957420744239,\n",
      "    \"Expert_36\": 0.00034927933602759385,\n",
      "    \"Expert_37\": 0.0030611727288245797,\n",
      "    \"Expert_38\": 2.5462430141230327e-05,\n",
      "    \"Expert_39\": 3.356868700135889e-05,\n",
      "    \"Expert_40\": 0.00015081499516341086,\n",
      "    \"Expert_41\": 0.0013386019256780637,\n",
      "    \"Expert_42\": 0.04805936793091522,\n",
      "    \"Expert_43\": 0.0007834439332473605,\n",
      "    \"Expert_44\": 0.2234872891279102,\n",
      "    \"Expert_45\": 0.0003786565198821822,\n",
      "    \"Expert_46\": 0.018234596451509495,\n",
      "    \"Expert_47\": 0.001602327901944067,\n",
      "    \"Expert_48\": 0.00037572898390829737,\n",
      "    \"Expert_49\": 0.0004348561729447791,\n",
      "    \"Expert_50\": 0.09277071782639329,\n",
      "    \"Expert_51\": 0.0030215097019857503,\n",
      "    \"Expert_52\": 0.0008722285225697644,\n",
      "    \"Expert_53\": 8.095045549002394e-05,\n",
      "    \"Expert_54\": 0.00012088460213088674,\n",
      "    \"Expert_55\": 0.00014232308238848468,\n",
      "    \"Expert_56\": 0.0001005273109431961,\n",
      "    \"Expert_57\": 8.825091901162093e-05,\n",
      "    \"Expert_58\": 0.004360584351001711,\n",
      "    \"Expert_59\": 2.405233535866455e-06,\n",
      "    \"Expert_60\": 0.055746917097350586,\n",
      "    \"Expert_61\": 0.031605516405201275,\n",
      "    \"Expert_62\": 0.002369888975139898,\n",
      "    \"Expert_63\": 8.955350363760821e-05\n",
      "  },\n",
      "  \"total_kl_divergence\": 0.6666353339536275\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_file1 = \"expert_routing_analysis_1.json\"\n",
    "json_file2 = \"expert_routing_analysis_2.json\"\n",
    "target_token =  3761 # use token id\n",
    "\n",
    "result = calculate_token_kl_divergence(json_file1, json_file2, target_token)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def generate_heatmap(input_text, model, tokenizer, output_file, num_layers=16):\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=num_layers, cols=1, \n",
    "        subplot_titles=[f\"Layer {i}\" for i in range(num_layers)],\n",
    "        vertical_spacing=0.02\n",
    "    )\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        # Call analyze_expert_routing without specifying output file\n",
    "        router_logits, analysis_results = analyze_expert_routing(input_text, model, tokenizer, layer)\n",
    "        \n",
    "        # Find the JSON file created by analyze_expert_routing\n",
    "        default_json_file = f\"json/expert_routing_analysis{layer}.json\"\n",
    "        \n",
    "        # Extract data from analysis_results\n",
    "        tokens = [token['token'] for token in analysis_results['tokens']]\n",
    "        probabilities = [token['router_probability'] for token in analysis_results['tokens']]\n",
    "        \n",
    "        heatmap = go.Heatmap(\n",
    "            z=probabilities,\n",
    "            x=[f\"E{i}\" for i in range(64)],\n",
    "            y=tokens,\n",
    "            colorscale='Viridis',\n",
    "            zmax = 1,\n",
    "            zmin = 0\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(heatmap, row=layer + 1, col=1)\n",
    "        \n",
    "        fig.update_xaxes(\n",
    "            title_text=\"Experts\" if layer == num_layers - 1 else None,\n",
    "            row=layer + 1, col=1,\n",
    "            tickangle=45,\n",
    "            tickmode='array',\n",
    "            tickvals=[f\"E{i}\" for i in range(0, 64, 8)],\n",
    "            ticktext=[f\"E{i}\" for i in range(0, 64, 8)]\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            title_text=\"Tokens\" if layer == 0 else None,\n",
    "            row=layer + 1, col=1,\n",
    "            tickmode='array',\n",
    "            tickvals=tokens,\n",
    "            ticktext=tokens,\n",
    "            side='left'\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Expert Routing Heatmaps for '{input_text}'\",\n",
    "        height=300 * num_layers,\n",
    "        width=1000,\n",
    "        font=dict(size=10),\n",
    "        coloraxis_colorbar=dict(\n",
    "            title='Probability',\n",
    "            thickness=10,\n",
    "            len=0.5,\n",
    "            tickvals=[0, 0.25, 0.5, 0.75, 1],\n",
    "            ticktext=['0', '0.25', '0.5', '0.75', '1'],\n",
    "            orientation='h'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in fig['layout']['annotations']:\n",
    "        i['font'] = dict(size=12)\n",
    "\n",
    "    # fig.show()\n",
    "\n",
    "    fig.write_html(output_file)\n",
    "    print(f\"Heatmaps saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('sentences.json', 'r') as f:\n",
    "    sentences = json.load(f)\n",
    "\n",
    "for dictionary_name, sentence_list in sentences.items():\n",
    "    for i, sentence in enumerate(sentence_list):\n",
    "        input_text = sentence\n",
    "        output_file = f\"plots/{dictionary_name}_{i}.html\"\n",
    "        generate_heatmap(input_text, model, tokenizer, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot of KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0.0034882319244850125,
           0.0004693411663262177,
           0.008435111892799342,
           0.016356922313548475,
           0.11757811309974568,
           0.0009210882677346104,
           0.027944314949527052,
           0.0014293470553286025,
           0.02788578193551784,
           0.041493439382485044,
           0.2463876533540373,
           0.0166560425595616,
           0.002762144421434732,
           0.0012702785163360257,
           0.02029256754131393,
           0.000625459052728876,
           0.0118059474240055,
           0.02755195657885956,
           0,
           0.005799600880015962,
           0.01645543953219956,
           0.017181932578776017,
           0.07615770398784268,
           0.00002075985798300501,
           0.0028697810930737883,
           0.03589912289637325,
           0.00203597745393639,
           0.000468304384113652,
           0.01381217975577341,
           0.004587446419248246,
           0.005790408171830918,
           0.006319322893816297,
           0.012727585598837606,
           0.0076110605854786025,
           0.007943096024053693,
           0.002459264544478142,
           0.001555329781852052,
           0.013689862606463021,
           0.0001063912343122628,
           0.00014266317634922677,
           0.0006672889521027127,
           0.005982114208603103,
           0.21503703878172378,
           0.0034980258222792097,
           1,
           0.0016867797874920195,
           0.0815842749435022,
           0.007162172216857535,
           0.0016736803487512892,
           0.00193824857185912,
           0.41510070809129396,
           0.013512387967387032,
           0.0038952978965848856,
           0.0003546757969562732,
           0.0005333635781660611,
           0.0006292913690912374,
           0.0004422736338513846,
           0.000387342167423855,
           0.019504159371914746,
           0.000003220398100766674,
           0.24943544763831557,
           0.14141327393891542,
           0.01059667117700369,
           0.0003931706619888917
          ],
          "colorbar": {
           "title": {
            "text": "KL Divergence"
           }
          },
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ]
         },
         "type": "bar",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63
         ],
         "y": [
          0.0007812551380871591,
          0.00010657651477648418,
          0.0018868115937772031,
          0.003657222177318448,
          0.02627870109798979,
          0.00020753548841244591,
          0.006246837614801284,
          0.00032112401034644494,
          0.0062337563288345,
          0.009274871867047087,
          0.05506577895205873,
          0.003724071246049763,
          0.0006189850341964767,
          0.0002855744818948073,
          0.004536782226628571,
          0.00014146661485302942,
          0.0026401448071286925,
          0.006159151167502406,
          0.0000016855209224618181,
          0.001297812824272446,
          0.003679239357394368,
          0.0038416000944247863,
          0.017021835965967777,
          0.000006325050314589034,
          0.0006430402807279783,
          0.008024622670379872,
          0.0004566971711456539,
          0.00010634480887789996,
          0.003088508850769698,
          0.0010269137529428703,
          0.0012957583863349286,
          0.001413963212234453,
          0.002846117670938288,
          0.0017026479899575004,
          0.0017768531303663654,
          0.000551295742074423,
          0.0003492793360275921,
          0.0030611727288245797,
          0.000025462430141230327,
          0.00003356868700135889,
          0.00015081499516341086,
          0.0013386019256780637,
          0.04805936793091522,
          0.0007834439332473605,
          0.22348728912791016,
          0.0003786565198821822,
          0.018234596451509495,
          0.0016023279019440652,
          0.00037572898390829737,
          0.0004348561729447791,
          0.09277071782639329,
          0.0030215097019857503,
          0.0008722285225697644,
          0.00008095045549002394,
          0.00012088460213088674,
          0.00014232308238848468,
          0.00010052731094319436,
          0.00008825091901162093,
          0.0043605843510017145,
          0.000002405233535866455,
          0.055746917097350586,
          0.031605516405201275,
          0.0023698889751399,
          0.00008955350363760821
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "color": "red",
           "size": 12
          },
          "showarrow": false,
          "text": "Mean: 0.0104",
          "x": 65.28,
          "y": 0.01041617709302543
         },
         {
          "font": {
           "color": "darkblue",
           "size": 16
          },
          "showarrow": false,
          "text": "Total KL Divergence: 0.6666",
          "x": 32,
          "y": 0.2458360180407012
         },
         {
          "arrowcolor": "#636363",
          "arrowhead": 2,
          "arrowsize": 1,
          "arrowwidth": 2,
          "bgcolor": "white",
          "bordercolor": "black",
          "borderwidth": 1,
          "font": {
           "color": "black",
           "size": 10
          },
          "showarrow": true,
          "text": "Expert 44",
          "x": 44,
          "y": 0.22348728912791016
         },
         {
          "arrowcolor": "#636363",
          "arrowhead": 2,
          "arrowsize": 1,
          "arrowwidth": 2,
          "bgcolor": "white",
          "bordercolor": "black",
          "borderwidth": 1,
          "font": {
           "color": "black",
           "size": 10
          },
          "showarrow": true,
          "text": "Expert 50",
          "x": 50,
          "y": 0.09277071782639329
         },
         {
          "arrowcolor": "#636363",
          "arrowhead": 2,
          "arrowsize": 1,
          "arrowwidth": 2,
          "bgcolor": "white",
          "bordercolor": "black",
          "borderwidth": 1,
          "font": {
           "color": "black",
           "size": 10
          },
          "showarrow": true,
          "text": "Expert 60",
          "x": 60,
          "y": 0.055746917097350586
         },
         {
          "arrowcolor": "#636363",
          "arrowhead": 2,
          "arrowsize": 1,
          "arrowwidth": 2,
          "bgcolor": "white",
          "bordercolor": "black",
          "borderwidth": 1,
          "font": {
           "color": "black",
           "size": 10
          },
          "showarrow": true,
          "text": "Expert 10",
          "x": 10,
          "y": 0.05506577895205873
         },
         {
          "arrowcolor": "#636363",
          "arrowhead": 2,
          "arrowsize": 1,
          "arrowwidth": 2,
          "bgcolor": "white",
          "bordercolor": "black",
          "borderwidth": 1,
          "font": {
           "color": "black",
           "size": 10
          },
          "showarrow": true,
          "text": "Expert 42",
          "x": 42,
          "y": 0.04805936793091522
         }
        ],
        "bargap": 0.2,
        "bargroupgap": 0.1,
        "font": {
         "family": "Arial",
         "size": 14
        },
        "height": 600,
        "modebar": {
         "add": [
          "v1hovermode",
          "toggleSpikelines"
         ]
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": -1,
          "x1": 64,
          "y0": 0.01041617709302543,
          "y1": 0.01041617709302543
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 24
         },
         "text": "KL Divergence by Expert",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "gridcolor": "lightgrey",
         "showgrid": true,
         "title": {
          "text": "Expert Number"
         }
        },
        "yaxis": {
         "gridcolor": "lightgrey",
         "showgrid": true,
         "title": {
          "text": "KL Divergence"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Load the KL divergence results\n",
    "with open('kl_divergence_analysis_1.json', 'r') as f:\n",
    "    kl_data = json.load(f)\n",
    "\n",
    "# Extract expert numbers and KL divergence values\n",
    "experts = [int(expert.split('_')[1]) for expert in kl_data['expert_kl_divergences'].keys()]\n",
    "kl_values = list(kl_data['expert_kl_divergences'].values())\n",
    "\n",
    "# Create a color scale based on KL divergence values\n",
    "colors = px.colors.sequential.Viridis\n",
    "\n",
    "# Normalize KL values for color mapping\n",
    "norm_kl_values = (np.array(kl_values) - min(kl_values)) / (max(kl_values) - min(kl_values))\n",
    "\n",
    "# Create the bar chart\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=experts,\n",
    "    y=kl_values,\n",
    "    marker=dict(\n",
    "        color=norm_kl_values,\n",
    "        colorscale=colors,\n",
    "        colorbar=dict(title=\"KL Divergence\")\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'KL Divergence by Expert',\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top',\n",
    "        'font': dict(size=24)\n",
    "    },\n",
    "    xaxis_title='Expert Number',\n",
    "    yaxis_title='KL Divergence',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.1,\n",
    "    plot_bgcolor='white',  # Set plot background to white\n",
    "    paper_bgcolor='white',  # Set paper background to white\n",
    "    font=dict(family=\"Arial\", size=14),\n",
    "    xaxis=dict(showgrid=True, gridcolor='lightgrey'),  # Add light grid to x-axis\n",
    "    yaxis=dict(showgrid=True, gridcolor='lightgrey'),  # Add light grid to y-axis\n",
    ")\n",
    "\n",
    "# Add a horizontal line for the mean KL divergence\n",
    "mean_kl = sum(kl_values) / len(kl_values)\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-1,\n",
    "    y0=mean_kl,\n",
    "    x1=len(experts),\n",
    "    y1=mean_kl,\n",
    "    line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for mean KL divergence\n",
    "fig.add_annotation(\n",
    "    x=len(experts) * 1.02,\n",
    "    y=mean_kl,\n",
    "    text=f\"Mean: {mean_kl:.4f}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"red\")\n",
    ")\n",
    "\n",
    "# Add annotation for total KL divergence\n",
    "fig.add_annotation(\n",
    "    x=len(experts) * 0.5,\n",
    "    y=max(kl_values) * 1.1,\n",
    "    text=f\"Total KL Divergence: {kl_data['total_kl_divergence']:.4f}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=16, color=\"darkblue\")\n",
    ")\n",
    "\n",
    "# Highlight top 5 experts with highest KL divergence\n",
    "top_5_indices = sorted(range(len(kl_values)), key=lambda i: kl_values[i], reverse=True)[:5]\n",
    "for idx in top_5_indices:\n",
    "    fig.add_annotation(\n",
    "        x=experts[idx],\n",
    "        y=kl_values[idx],\n",
    "        text=f\"Expert {experts[idx]}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        arrowsize=1,\n",
    "        arrowwidth=2,\n",
    "        arrowcolor=\"#636363\",\n",
    "        font=dict(size=10, color=\"black\"),\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "    )\n",
    "\n",
    "# Update size and add modebar\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    modebar_add=[\"v1hovermode\", \"toggleSpikelines\"]\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
