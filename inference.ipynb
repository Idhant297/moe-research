{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OlmoeForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import kl_div\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference for olmoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3a3af464ba46fab96f4f361b2eb596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OlmoeForCausalLM(\n",
       "  (model): OlmoeModel(\n",
       "    (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x OlmoeDecoderLayer(\n",
       "        (self_attn): OlmoeSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "          (k_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "        (mlp): OlmoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x OlmoeMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (down_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): OlmoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n",
    "\n",
    "# Set the model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to get the router logits and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expert_routing(input_text, model, tokenizer, layer_num):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Print the tokenized input\n",
    "    print(\"Tokenized input:\")\n",
    "    for token_id in inputs.input_ids[0]:\n",
    "        token = tokenizer.decode([token_id])\n",
    "        print(f\"Token: '{token}', ID: {token_id.item()}\")\n",
    "\n",
    "    print(f\"\\nInput shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "    # Forward pass with output_router_logits=True\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_router_logits=True)\n",
    "\n",
    "    # Get the router logits from the specified layer\n",
    "    router_logits = outputs.router_logits[layer_num]\n",
    "\n",
    "    # Print which layer the logits are from\n",
    "    print(f\"\\nRouter logits are from layer {layer_num} of the model\")\n",
    "\n",
    "    # Initialize a dictionary to store the analysis results\n",
    "    analysis_results = {\n",
    "        \"input_text\": input_text,\n",
    "        \"tokens\": []\n",
    "    }\n",
    "\n",
    "    # Print router logits and probabilities for each token\n",
    "    print(\"\\nRouter logits and probabilities for each token:\")\n",
    "    for token_idx, token_id in enumerate(inputs.input_ids[0]):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        logits = router_logits[token_idx]\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        print(f\"Token: '{token}' (ID: {token_id.item()})\")\n",
    "        for expert_idx, (logit, prob) in enumerate(zip(logits, probabilities)):\n",
    "            print(f\"  expert {expert_idx}: logit = {logit.item():.4f}, post-softmax = {prob.item():.4f}\")\n",
    "        print()\n",
    "        \n",
    "        token_data = {\n",
    "            \"token\": token,\n",
    "            \"id\": token_id.item(),\n",
    "            \"router_probability\": probabilities.tolist()\n",
    "        }\n",
    "        analysis_results[\"tokens\"].append(token_data)\n",
    "\n",
    "    # Print the top-k experts for each token\n",
    "    k = 8\n",
    "    print(f\"\\nTop {k} experts for each token:\")\n",
    "    for token_idx, token_id in enumerate(inputs.input_ids[0]):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        probabilities = F.softmax(router_logits[token_idx], dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
    "        \n",
    "        print(f\"Token: '{token}' (ID: {token_id.item()})\")\n",
    "        for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):\n",
    "            print(f\"  {i+1}. expert {idx.item()}: probability = {prob.item():.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Save the analysis results as a JSON file with a unique name\n",
    "    base_filename = \"expert_routing_analysis\"\n",
    "    counter = 1\n",
    "    filename = f\"{base_filename}_{counter}.json\"\n",
    "    while os.path.exists(filename):\n",
    "        counter += 1\n",
    "        filename = f\"{base_filename}_{counter}.json\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "    print(f\"Analysis results saved to {filename}\")\n",
    "\n",
    "    return router_logits, analysis_results\n",
    "\n",
    "layer_num = 0 # layer to analyze (0-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input:\n",
      "Token: 'How', ID: 2347\n",
      "Token: ' will', ID: 588\n",
      "Token: ' you', ID: 368\n",
      "Token: ' pitch', ID: 11288\n",
      "Token: ' your', ID: 634\n",
      "Token: ' idea', ID: 2934\n",
      "Token: ' to', ID: 281\n",
      "Token: ' the', ID: 253\n",
      "Token: ' investors', ID: 12946\n",
      "Token: '?', ID: 32\n",
      "\n",
      "Input shape: torch.Size([1, 10])\n",
      "\n",
      "Router logits are from layer 0 of the model\n",
      "\n",
      "Router logits and probabilities for each token:\n",
      "Token: 'How' (ID: 2347)\n",
      "  expert 0: logit = -3.0995, post-softmax = 0.0008\n",
      "  expert 1: logit = -0.1792, post-softmax = 0.0145\n",
      "  expert 2: logit = -0.1681, post-softmax = 0.0147\n",
      "  expert 3: logit = -0.3178, post-softmax = 0.0126\n",
      "  expert 4: logit = 0.1484, post-softmax = 0.0202\n",
      "  expert 5: logit = -0.4750, post-softmax = 0.0108\n",
      "  expert 6: logit = 0.0466, post-softmax = 0.0182\n",
      "  expert 7: logit = 0.7465, post-softmax = 0.0367\n",
      "  expert 8: logit = -1.4290, post-softmax = 0.0042\n",
      "  expert 9: logit = -0.9157, post-softmax = 0.0070\n",
      "  expert 10: logit = -0.5113, post-softmax = 0.0104\n",
      "  expert 11: logit = -0.1142, post-softmax = 0.0155\n",
      "  expert 12: logit = -4.5195, post-softmax = 0.0002\n",
      "  expert 13: logit = -2.7419, post-softmax = 0.0011\n",
      "  expert 14: logit = 1.8359, post-softmax = 0.1090\n",
      "  expert 15: logit = -0.5961, post-softmax = 0.0096\n",
      "  expert 16: logit = -0.3109, post-softmax = 0.0127\n",
      "  expert 17: logit = 0.5035, post-softmax = 0.0288\n",
      "  expert 18: logit = 0.7489, post-softmax = 0.0368\n",
      "  expert 19: logit = -0.5600, post-softmax = 0.0099\n",
      "  expert 20: logit = -2.4236, post-softmax = 0.0015\n",
      "  expert 21: logit = -1.6136, post-softmax = 0.0035\n",
      "  expert 22: logit = -1.0930, post-softmax = 0.0058\n",
      "  expert 23: logit = -0.0544, post-softmax = 0.0165\n",
      "  expert 24: logit = -0.5513, post-softmax = 0.0100\n",
      "  expert 25: logit = -0.1023, post-softmax = 0.0157\n",
      "  expert 26: logit = -0.7925, post-softmax = 0.0079\n",
      "  expert 27: logit = -0.0133, post-softmax = 0.0172\n",
      "  expert 28: logit = -2.1940, post-softmax = 0.0019\n",
      "  expert 29: logit = -0.6294, post-softmax = 0.0093\n",
      "  expert 30: logit = 0.0837, post-softmax = 0.0189\n",
      "  expert 31: logit = -1.0442, post-softmax = 0.0061\n",
      "  expert 32: logit = -0.3303, post-softmax = 0.0125\n",
      "  expert 33: logit = -0.4124, post-softmax = 0.0115\n",
      "  expert 34: logit = -2.2241, post-softmax = 0.0019\n",
      "  expert 35: logit = 0.4233, post-softmax = 0.0265\n",
      "  expert 36: logit = -0.6259, post-softmax = 0.0093\n",
      "  expert 37: logit = -0.2097, post-softmax = 0.0141\n",
      "  expert 38: logit = -0.7124, post-softmax = 0.0085\n",
      "  expert 39: logit = 0.4768, post-softmax = 0.0280\n",
      "  expert 40: logit = -1.2331, post-softmax = 0.0051\n",
      "  expert 41: logit = -1.6138, post-softmax = 0.0035\n",
      "  expert 42: logit = -0.5582, post-softmax = 0.0099\n",
      "  expert 43: logit = 0.5817, post-softmax = 0.0311\n",
      "  expert 44: logit = -0.1488, post-softmax = 0.0150\n",
      "  expert 45: logit = 0.2209, post-softmax = 0.0217\n",
      "  expert 46: logit = -0.5650, post-softmax = 0.0099\n",
      "  expert 47: logit = -0.3971, post-softmax = 0.0117\n",
      "  expert 48: logit = -1.0323, post-softmax = 0.0062\n",
      "  expert 49: logit = -0.7636, post-softmax = 0.0081\n",
      "  expert 50: logit = -0.4477, post-softmax = 0.0111\n",
      "  expert 51: logit = -1.5522, post-softmax = 0.0037\n",
      "  expert 52: logit = -0.5973, post-softmax = 0.0096\n",
      "  expert 53: logit = -2.7773, post-softmax = 0.0011\n",
      "  expert 54: logit = -0.1416, post-softmax = 0.0151\n",
      "  expert 55: logit = -0.0258, post-softmax = 0.0169\n",
      "  expert 56: logit = -1.0143, post-softmax = 0.0063\n",
      "  expert 57: logit = 1.8911, post-softmax = 0.1152\n",
      "  expert 58: logit = -3.0006, post-softmax = 0.0009\n",
      "  expert 59: logit = -0.6850, post-softmax = 0.0088\n",
      "  expert 60: logit = -0.2627, post-softmax = 0.0134\n",
      "  expert 61: logit = 1.0703, post-softmax = 0.0507\n",
      "  expert 62: logit = -0.2344, post-softmax = 0.0138\n",
      "  expert 63: logit = -0.4277, post-softmax = 0.0113\n",
      "\n",
      "Token: ' will' (ID: 588)\n",
      "  expert 0: logit = -2.3664, post-softmax = 0.0017\n",
      "  expert 1: logit = -0.7166, post-softmax = 0.0090\n",
      "  expert 2: logit = -0.4003, post-softmax = 0.0123\n",
      "  expert 3: logit = -0.7795, post-softmax = 0.0084\n",
      "  expert 4: logit = -0.5592, post-softmax = 0.0105\n",
      "  expert 5: logit = -0.2075, post-softmax = 0.0149\n",
      "  expert 6: logit = 1.4444, post-softmax = 0.0779\n",
      "  expert 7: logit = 0.1080, post-softmax = 0.0205\n",
      "  expert 8: logit = 0.4586, post-softmax = 0.0291\n",
      "  expert 9: logit = -1.1798, post-softmax = 0.0056\n",
      "  expert 10: logit = -0.6730, post-softmax = 0.0094\n",
      "  expert 11: logit = -1.0028, post-softmax = 0.0067\n",
      "  expert 12: logit = -4.4525, post-softmax = 0.0002\n",
      "  expert 13: logit = -1.1059, post-softmax = 0.0061\n",
      "  expert 14: logit = 1.9246, post-softmax = 0.1259\n",
      "  expert 15: logit = 0.0694, post-softmax = 0.0197\n",
      "  expert 16: logit = -0.2844, post-softmax = 0.0138\n",
      "  expert 17: logit = 0.0936, post-softmax = 0.0202\n",
      "  expert 18: logit = 1.4157, post-softmax = 0.0757\n",
      "  expert 19: logit = -3.1688, post-softmax = 0.0008\n",
      "  expert 20: logit = -1.6554, post-softmax = 0.0035\n",
      "  expert 21: logit = -0.7525, post-softmax = 0.0087\n",
      "  expert 22: logit = -2.2276, post-softmax = 0.0020\n",
      "  expert 23: logit = 0.3758, post-softmax = 0.0267\n",
      "  expert 24: logit = -0.7041, post-softmax = 0.0091\n",
      "  expert 25: logit = 0.0677, post-softmax = 0.0197\n",
      "  expert 26: logit = -0.5219, post-softmax = 0.0109\n",
      "  expert 27: logit = -0.3865, post-softmax = 0.0125\n",
      "  expert 28: logit = -1.4435, post-softmax = 0.0043\n",
      "  expert 29: logit = -0.6013, post-softmax = 0.0101\n",
      "  expert 30: logit = -0.4016, post-softmax = 0.0123\n",
      "  expert 31: logit = -1.2497, post-softmax = 0.0053\n",
      "  expert 32: logit = -0.1379, post-softmax = 0.0160\n",
      "  expert 33: logit = -0.7748, post-softmax = 0.0085\n",
      "  expert 34: logit = -2.7258, post-softmax = 0.0012\n",
      "  expert 35: logit = -0.2232, post-softmax = 0.0147\n",
      "  expert 36: logit = -0.6413, post-softmax = 0.0097\n",
      "  expert 37: logit = -0.5842, post-softmax = 0.0102\n",
      "  expert 38: logit = -0.6459, post-softmax = 0.0096\n",
      "  expert 39: logit = 1.0297, post-softmax = 0.0514\n",
      "  expert 40: logit = -0.5436, post-softmax = 0.0107\n",
      "  expert 41: logit = -1.9305, post-softmax = 0.0027\n",
      "  expert 42: logit = -0.7445, post-softmax = 0.0087\n",
      "  expert 43: logit = -0.1867, post-softmax = 0.0152\n",
      "  expert 44: logit = -0.7126, post-softmax = 0.0090\n",
      "  expert 45: logit = -0.5353, post-softmax = 0.0108\n",
      "  expert 46: logit = -1.0989, post-softmax = 0.0061\n",
      "  expert 47: logit = -0.4581, post-softmax = 0.0116\n",
      "  expert 48: logit = -1.3474, post-softmax = 0.0048\n",
      "  expert 49: logit = -0.5749, post-softmax = 0.0103\n",
      "  expert 50: logit = -0.2596, post-softmax = 0.0142\n",
      "  expert 51: logit = -2.0828, post-softmax = 0.0023\n",
      "  expert 52: logit = 0.3978, post-softmax = 0.0273\n",
      "  expert 53: logit = -0.3114, post-softmax = 0.0135\n",
      "  expert 54: logit = 0.3377, post-softmax = 0.0257\n",
      "  expert 55: logit = -0.0829, post-softmax = 0.0169\n",
      "  expert 56: logit = -1.3161, post-softmax = 0.0049\n",
      "  expert 57: logit = -0.3430, post-softmax = 0.0130\n",
      "  expert 58: logit = -3.1422, post-softmax = 0.0008\n",
      "  expert 59: logit = -0.8663, post-softmax = 0.0077\n",
      "  expert 60: logit = -1.1374, post-softmax = 0.0059\n",
      "  expert 61: logit = 0.0138, post-softmax = 0.0186\n",
      "  expert 62: logit = -0.1995, post-softmax = 0.0150\n",
      "  expert 63: logit = 0.4766, post-softmax = 0.0296\n",
      "\n",
      "Token: ' you' (ID: 368)\n",
      "  expert 0: logit = -2.8561, post-softmax = 0.0013\n",
      "  expert 1: logit = -0.9498, post-softmax = 0.0087\n",
      "  expert 2: logit = -0.4505, post-softmax = 0.0143\n",
      "  expert 3: logit = -0.4452, post-softmax = 0.0143\n",
      "  expert 4: logit = -0.6640, post-softmax = 0.0115\n",
      "  expert 5: logit = -0.5290, post-softmax = 0.0132\n",
      "  expert 6: logit = 1.3923, post-softmax = 0.0901\n",
      "  expert 7: logit = 0.4053, post-softmax = 0.0336\n",
      "  expert 8: logit = 0.1208, post-softmax = 0.0253\n",
      "  expert 9: logit = -1.4489, post-softmax = 0.0053\n",
      "  expert 10: logit = -0.0983, post-softmax = 0.0203\n",
      "  expert 11: logit = -1.0729, post-softmax = 0.0077\n",
      "  expert 12: logit = -4.4448, post-softmax = 0.0003\n",
      "  expert 13: logit = -1.6234, post-softmax = 0.0044\n",
      "  expert 14: logit = -0.5380, post-softmax = 0.0131\n",
      "  expert 15: logit = -0.5247, post-softmax = 0.0132\n",
      "  expert 16: logit = -0.9118, post-softmax = 0.0090\n",
      "  expert 17: logit = -0.7343, post-softmax = 0.0107\n",
      "  expert 18: logit = -0.1357, post-softmax = 0.0195\n",
      "  expert 19: logit = -2.0414, post-softmax = 0.0029\n",
      "  expert 20: logit = -2.1363, post-softmax = 0.0026\n",
      "  expert 21: logit = -1.1692, post-softmax = 0.0070\n",
      "  expert 22: logit = -1.4661, post-softmax = 0.0052\n",
      "  expert 23: logit = 0.3420, post-softmax = 0.0315\n",
      "  expert 24: logit = -0.8790, post-softmax = 0.0093\n",
      "  expert 25: logit = -0.3092, post-softmax = 0.0164\n",
      "  expert 26: logit = -0.4212, post-softmax = 0.0147\n",
      "  expert 27: logit = 0.1015, post-softmax = 0.0248\n",
      "  expert 28: logit = -1.9638, post-softmax = 0.0031\n",
      "  expert 29: logit = -0.4544, post-softmax = 0.0142\n",
      "  expert 30: logit = -0.3449, post-softmax = 0.0159\n",
      "  expert 31: logit = -0.7113, post-softmax = 0.0110\n",
      "  expert 32: logit = -0.2578, post-softmax = 0.0173\n",
      "  expert 33: logit = -1.4347, post-softmax = 0.0053\n",
      "  expert 34: logit = -2.6582, post-softmax = 0.0016\n",
      "  expert 35: logit = 0.9398, post-softmax = 0.0573\n",
      "  expert 36: logit = -0.9995, post-softmax = 0.0082\n",
      "  expert 37: logit = -0.2385, post-softmax = 0.0176\n",
      "  expert 38: logit = 0.3560, post-softmax = 0.0320\n",
      "  expert 39: logit = 0.2952, post-softmax = 0.0301\n",
      "  expert 40: logit = -0.7110, post-softmax = 0.0110\n",
      "  expert 41: logit = -2.3084, post-softmax = 0.0022\n",
      "  expert 42: logit = -0.4952, post-softmax = 0.0136\n",
      "  expert 43: logit = 0.3219, post-softmax = 0.0309\n",
      "  expert 44: logit = -0.6013, post-softmax = 0.0123\n",
      "  expert 45: logit = -0.8392, post-softmax = 0.0097\n",
      "  expert 46: logit = -1.0380, post-softmax = 0.0079\n",
      "  expert 47: logit = -0.2876, post-softmax = 0.0168\n",
      "  expert 48: logit = 0.8163, post-softmax = 0.0506\n",
      "  expert 49: logit = -0.3771, post-softmax = 0.0154\n",
      "  expert 50: logit = -0.4542, post-softmax = 0.0142\n",
      "  expert 51: logit = -1.2554, post-softmax = 0.0064\n",
      "  expert 52: logit = -0.4161, post-softmax = 0.0148\n",
      "  expert 53: logit = -0.4363, post-softmax = 0.0145\n",
      "  expert 54: logit = -0.3652, post-softmax = 0.0155\n",
      "  expert 55: logit = -0.1568, post-softmax = 0.0191\n",
      "  expert 56: logit = -0.7914, post-softmax = 0.0101\n",
      "  expert 57: logit = -0.1443, post-softmax = 0.0194\n",
      "  expert 58: logit = -3.2226, post-softmax = 0.0009\n",
      "  expert 59: logit = -0.8035, post-softmax = 0.0100\n",
      "  expert 60: logit = -1.3601, post-softmax = 0.0057\n",
      "  expert 61: logit = -0.7133, post-softmax = 0.0110\n",
      "  expert 62: logit = -0.4403, post-softmax = 0.0144\n",
      "  expert 63: logit = 0.2941, post-softmax = 0.0300\n",
      "\n",
      "Token: ' pitch' (ID: 11288)\n",
      "  expert 0: logit = -1.4624, post-softmax = 0.0050\n",
      "  expert 1: logit = 1.4839, post-softmax = 0.0952\n",
      "  expert 2: logit = -0.2250, post-softmax = 0.0172\n",
      "  expert 3: logit = -0.3928, post-softmax = 0.0146\n",
      "  expert 4: logit = -0.4417, post-softmax = 0.0139\n",
      "  expert 5: logit = -1.2481, post-softmax = 0.0062\n",
      "  expert 6: logit = 0.8913, post-softmax = 0.0526\n",
      "  expert 7: logit = -1.6522, post-softmax = 0.0041\n",
      "  expert 8: logit = 0.1231, post-softmax = 0.0244\n",
      "  expert 9: logit = -0.4536, post-softmax = 0.0137\n",
      "  expert 10: logit = -0.4243, post-softmax = 0.0141\n",
      "  expert 11: logit = -1.1417, post-softmax = 0.0069\n",
      "  expert 12: logit = -1.3943, post-softmax = 0.0054\n",
      "  expert 13: logit = -0.9003, post-softmax = 0.0088\n",
      "  expert 14: logit = -1.5323, post-softmax = 0.0047\n",
      "  expert 15: logit = -0.5070, post-softmax = 0.0130\n",
      "  expert 16: logit = -0.4590, post-softmax = 0.0136\n",
      "  expert 17: logit = -0.6301, post-softmax = 0.0115\n",
      "  expert 18: logit = -0.4421, post-softmax = 0.0139\n",
      "  expert 19: logit = -1.4752, post-softmax = 0.0049\n",
      "  expert 20: logit = -0.1442, post-softmax = 0.0187\n",
      "  expert 21: logit = -1.0174, post-softmax = 0.0078\n",
      "  expert 22: logit = -2.6436, post-softmax = 0.0015\n",
      "  expert 23: logit = 0.1811, post-softmax = 0.0259\n",
      "  expert 24: logit = -0.9120, post-softmax = 0.0087\n",
      "  expert 25: logit = -0.5423, post-softmax = 0.0126\n",
      "  expert 26: logit = -0.2894, post-softmax = 0.0162\n",
      "  expert 27: logit = -0.5557, post-softmax = 0.0124\n",
      "  expert 28: logit = -1.7241, post-softmax = 0.0038\n",
      "  expert 29: logit = -0.3698, post-softmax = 0.0149\n",
      "  expert 30: logit = -0.3375, post-softmax = 0.0154\n",
      "  expert 31: logit = -0.1137, post-softmax = 0.0193\n",
      "  expert 32: logit = -0.6226, post-softmax = 0.0116\n",
      "  expert 33: logit = -0.4611, post-softmax = 0.0136\n",
      "  expert 34: logit = -2.5414, post-softmax = 0.0017\n",
      "  expert 35: logit = 1.6461, post-softmax = 0.1120\n",
      "  expert 36: logit = -0.5688, post-softmax = 0.0122\n",
      "  expert 37: logit = -0.3840, post-softmax = 0.0147\n",
      "  expert 38: logit = -0.7343, post-softmax = 0.0104\n",
      "  expert 39: logit = -1.9497, post-softmax = 0.0031\n",
      "  expert 40: logit = -0.5290, post-softmax = 0.0127\n",
      "  expert 41: logit = -0.5367, post-softmax = 0.0126\n",
      "  expert 42: logit = -0.7047, post-softmax = 0.0107\n",
      "  expert 43: logit = -2.0441, post-softmax = 0.0028\n",
      "  expert 44: logit = -0.6594, post-softmax = 0.0112\n",
      "  expert 45: logit = -0.3618, post-softmax = 0.0150\n",
      "  expert 46: logit = -1.1081, post-softmax = 0.0071\n",
      "  expert 47: logit = -0.4183, post-softmax = 0.0142\n",
      "  expert 48: logit = -0.0403, post-softmax = 0.0207\n",
      "  expert 49: logit = 0.0967, post-softmax = 0.0238\n",
      "  expert 50: logit = -0.7450, post-softmax = 0.0102\n",
      "  expert 51: logit = -0.7414, post-softmax = 0.0103\n",
      "  expert 52: logit = -0.4779, post-softmax = 0.0134\n",
      "  expert 53: logit = -0.7055, post-softmax = 0.0107\n",
      "  expert 54: logit = -0.5189, post-softmax = 0.0128\n",
      "  expert 55: logit = -0.5822, post-softmax = 0.0121\n",
      "  expert 56: logit = -0.0802, post-softmax = 0.0199\n",
      "  expert 57: logit = -1.1445, post-softmax = 0.0069\n",
      "  expert 58: logit = 0.1468, post-softmax = 0.0250\n",
      "  expert 59: logit = -0.2897, post-softmax = 0.0162\n",
      "  expert 60: logit = -1.1551, post-softmax = 0.0068\n",
      "  expert 61: logit = -0.0852, post-softmax = 0.0198\n",
      "  expert 62: logit = -0.4700, post-softmax = 0.0135\n",
      "  expert 63: logit = -0.6222, post-softmax = 0.0116\n",
      "\n",
      "Token: ' your' (ID: 634)\n",
      "  expert 0: logit = -3.1072, post-softmax = 0.0012\n",
      "  expert 1: logit = -0.1909, post-softmax = 0.0217\n",
      "  expert 2: logit = -0.5334, post-softmax = 0.0154\n",
      "  expert 3: logit = 0.0960, post-softmax = 0.0289\n",
      "  expert 4: logit = -0.9919, post-softmax = 0.0097\n",
      "  expert 5: logit = -0.7762, post-softmax = 0.0121\n",
      "  expert 6: logit = 0.9269, post-softmax = 0.0664\n",
      "  expert 7: logit = 0.0535, post-softmax = 0.0277\n",
      "  expert 8: logit = -0.9221, post-softmax = 0.0105\n",
      "  expert 9: logit = -1.4983, post-softmax = 0.0059\n",
      "  expert 10: logit = -0.4368, post-softmax = 0.0170\n",
      "  expert 11: logit = -0.5761, post-softmax = 0.0148\n",
      "  expert 12: logit = -4.7576, post-softmax = 0.0002\n",
      "  expert 13: logit = -2.1436, post-softmax = 0.0031\n",
      "  expert 14: logit = -1.7330, post-softmax = 0.0046\n",
      "  expert 15: logit = -0.6507, post-softmax = 0.0137\n",
      "  expert 16: logit = -1.4207, post-softmax = 0.0063\n",
      "  expert 17: logit = -0.7126, post-softmax = 0.0129\n",
      "  expert 18: logit = -1.4638, post-softmax = 0.0061\n",
      "  expert 19: logit = -1.4817, post-softmax = 0.0060\n",
      "  expert 20: logit = -2.3548, post-softmax = 0.0025\n",
      "  expert 21: logit = -1.5379, post-softmax = 0.0056\n",
      "  expert 22: logit = -0.1345, post-softmax = 0.0230\n",
      "  expert 23: logit = -0.2288, post-softmax = 0.0209\n",
      "  expert 24: logit = 0.5717, post-softmax = 0.0465\n",
      "  expert 25: logit = -0.6634, post-softmax = 0.0135\n",
      "  expert 26: logit = -1.8804, post-softmax = 0.0040\n",
      "  expert 27: logit = 0.0113, post-softmax = 0.0266\n",
      "  expert 28: logit = -2.3869, post-softmax = 0.0024\n",
      "  expert 29: logit = -0.5868, post-softmax = 0.0146\n",
      "  expert 30: logit = -0.3441, post-softmax = 0.0186\n",
      "  expert 31: logit = -1.5180, post-softmax = 0.0058\n",
      "  expert 32: logit = -0.6487, post-softmax = 0.0137\n",
      "  expert 33: logit = -1.8967, post-softmax = 0.0039\n",
      "  expert 34: logit = -2.8061, post-softmax = 0.0016\n",
      "  expert 35: logit = -1.1547, post-softmax = 0.0083\n",
      "  expert 36: logit = -1.2571, post-softmax = 0.0075\n",
      "  expert 37: logit = -0.7909, post-softmax = 0.0119\n",
      "  expert 38: logit = 0.2642, post-softmax = 0.0342\n",
      "  expert 39: logit = -0.4498, post-softmax = 0.0168\n",
      "  expert 40: logit = -0.8484, post-softmax = 0.0112\n",
      "  expert 41: logit = -2.3387, post-softmax = 0.0025\n",
      "  expert 42: logit = 0.2772, post-softmax = 0.0347\n",
      "  expert 43: logit = -0.2230, post-softmax = 0.0210\n",
      "  expert 44: logit = -0.2643, post-softmax = 0.0202\n",
      "  expert 45: logit = -0.7936, post-softmax = 0.0119\n",
      "  expert 46: logit = 0.1291, post-softmax = 0.0299\n",
      "  expert 47: logit = -0.3828, post-softmax = 0.0179\n",
      "  expert 48: logit = -0.8427, post-softmax = 0.0113\n",
      "  expert 49: logit = -0.5204, post-softmax = 0.0156\n",
      "  expert 50: logit = -0.4105, post-softmax = 0.0174\n",
      "  expert 51: logit = -2.5693, post-softmax = 0.0020\n",
      "  expert 52: logit = -0.6764, post-softmax = 0.0134\n",
      "  expert 53: logit = -0.6391, post-softmax = 0.0139\n",
      "  expert 54: logit = -0.3763, post-softmax = 0.0180\n",
      "  expert 55: logit = 0.3108, post-softmax = 0.0359\n",
      "  expert 56: logit = -0.7919, post-softmax = 0.0119\n",
      "  expert 57: logit = 0.3809, post-softmax = 0.0385\n",
      "  expert 58: logit = -2.6029, post-softmax = 0.0019\n",
      "  expert 59: logit = -0.3093, post-softmax = 0.0193\n",
      "  expert 60: logit = 0.6211, post-softmax = 0.0489\n",
      "  expert 61: logit = -1.3363, post-softmax = 0.0069\n",
      "  expert 62: logit = -1.0531, post-softmax = 0.0092\n",
      "  expert 63: logit = -0.2538, post-softmax = 0.0204\n",
      "\n",
      "Token: ' idea' (ID: 2934)\n",
      "  expert 0: logit = -2.7322, post-softmax = 0.0015\n",
      "  expert 1: logit = 1.3597, post-softmax = 0.0915\n",
      "  expert 2: logit = -0.3376, post-softmax = 0.0168\n",
      "  expert 3: logit = -0.7636, post-softmax = 0.0109\n",
      "  expert 4: logit = -0.7233, post-softmax = 0.0114\n",
      "  expert 5: logit = 0.6472, post-softmax = 0.0449\n",
      "  expert 6: logit = 0.7754, post-softmax = 0.0510\n",
      "  expert 7: logit = -0.9639, post-softmax = 0.0090\n",
      "  expert 8: logit = -0.1312, post-softmax = 0.0206\n",
      "  expert 9: logit = -0.4924, post-softmax = 0.0144\n",
      "  expert 10: logit = -0.8947, post-softmax = 0.0096\n",
      "  expert 11: logit = -1.0112, post-softmax = 0.0085\n",
      "  expert 12: logit = -3.2038, post-softmax = 0.0010\n",
      "  expert 13: logit = -0.7646, post-softmax = 0.0109\n",
      "  expert 14: logit = -0.7904, post-softmax = 0.0107\n",
      "  expert 15: logit = -0.5405, post-softmax = 0.0137\n",
      "  expert 16: logit = -0.7776, post-softmax = 0.0108\n",
      "  expert 17: logit = -1.1057, post-softmax = 0.0078\n",
      "  expert 18: logit = -0.4807, post-softmax = 0.0145\n",
      "  expert 19: logit = -1.1291, post-softmax = 0.0076\n",
      "  expert 20: logit = -2.1720, post-softmax = 0.0027\n",
      "  expert 21: logit = -1.3890, post-softmax = 0.0059\n",
      "  expert 22: logit = -1.7173, post-softmax = 0.0042\n",
      "  expert 23: logit = -0.0609, post-softmax = 0.0221\n",
      "  expert 24: logit = -0.4283, post-softmax = 0.0153\n",
      "  expert 25: logit = -0.7212, post-softmax = 0.0114\n",
      "  expert 26: logit = -0.6342, post-softmax = 0.0125\n",
      "  expert 27: logit = -0.7329, post-softmax = 0.0113\n",
      "  expert 28: logit = -2.4998, post-softmax = 0.0019\n",
      "  expert 29: logit = -0.5741, post-softmax = 0.0132\n",
      "  expert 30: logit = -0.6031, post-softmax = 0.0129\n",
      "  expert 31: logit = -0.3279, post-softmax = 0.0169\n",
      "  expert 32: logit = -0.3399, post-softmax = 0.0167\n",
      "  expert 33: logit = -1.3199, post-softmax = 0.0063\n",
      "  expert 34: logit = -3.3144, post-softmax = 0.0009\n",
      "  expert 35: logit = -0.9819, post-softmax = 0.0088\n",
      "  expert 36: logit = -0.6888, post-softmax = 0.0118\n",
      "  expert 37: logit = -1.3507, post-softmax = 0.0061\n",
      "  expert 38: logit = -0.4788, post-softmax = 0.0146\n",
      "  expert 39: logit = -0.9129, post-softmax = 0.0094\n",
      "  expert 40: logit = -0.7344, post-softmax = 0.0113\n",
      "  expert 41: logit = -1.5214, post-softmax = 0.0051\n",
      "  expert 42: logit = -0.8103, post-softmax = 0.0104\n",
      "  expert 43: logit = -0.6191, post-softmax = 0.0127\n",
      "  expert 44: logit = -0.6736, post-softmax = 0.0120\n",
      "  expert 45: logit = -0.2768, post-softmax = 0.0178\n",
      "  expert 46: logit = -1.2644, post-softmax = 0.0066\n",
      "  expert 47: logit = -0.6159, post-softmax = 0.0127\n",
      "  expert 48: logit = -0.2820, post-softmax = 0.0177\n",
      "  expert 49: logit = -0.6472, post-softmax = 0.0123\n",
      "  expert 50: logit = -0.8514, post-softmax = 0.0100\n",
      "  expert 51: logit = -2.0146, post-softmax = 0.0031\n",
      "  expert 52: logit = -0.5693, post-softmax = 0.0133\n",
      "  expert 53: logit = -0.3766, post-softmax = 0.0161\n",
      "  expert 54: logit = -0.2129, post-softmax = 0.0190\n",
      "  expert 55: logit = -0.2304, post-softmax = 0.0187\n",
      "  expert 56: logit = 1.1293, post-softmax = 0.0727\n",
      "  expert 57: logit = -1.0710, post-softmax = 0.0081\n",
      "  expert 58: logit = -0.4488, post-softmax = 0.0150\n",
      "  expert 59: logit = -0.5474, post-softmax = 0.0136\n",
      "  expert 60: logit = -0.1915, post-softmax = 0.0194\n",
      "  expert 61: logit = 1.2178, post-softmax = 0.0794\n",
      "  expert 62: logit = -0.8550, post-softmax = 0.0100\n",
      "  expert 63: logit = -0.7459, post-softmax = 0.0111\n",
      "\n",
      "Token: ' to' (ID: 281)\n",
      "  expert 0: logit = -2.1932, post-softmax = 0.0025\n",
      "  expert 1: logit = 0.9941, post-softmax = 0.0617\n",
      "  expert 2: logit = -0.4375, post-softmax = 0.0147\n",
      "  expert 3: logit = -0.1362, post-softmax = 0.0199\n",
      "  expert 4: logit = -0.0293, post-softmax = 0.0222\n",
      "  expert 5: logit = 0.4686, post-softmax = 0.0365\n",
      "  expert 6: logit = 1.0281, post-softmax = 0.0638\n",
      "  expert 7: logit = -0.6324, post-softmax = 0.0121\n",
      "  expert 8: logit = 1.4944, post-softmax = 0.1017\n",
      "  expert 9: logit = -1.9951, post-softmax = 0.0031\n",
      "  expert 10: logit = -0.8858, post-softmax = 0.0094\n",
      "  expert 11: logit = -0.5925, post-softmax = 0.0126\n",
      "  expert 12: logit = -4.1261, post-softmax = 0.0004\n",
      "  expert 13: logit = -0.9016, post-softmax = 0.0093\n",
      "  expert 14: logit = -0.8545, post-softmax = 0.0097\n",
      "  expert 15: logit = -0.4035, post-softmax = 0.0152\n",
      "  expert 16: logit = -1.4071, post-softmax = 0.0056\n",
      "  expert 17: logit = -0.9717, post-softmax = 0.0086\n",
      "  expert 18: logit = -0.5824, post-softmax = 0.0127\n",
      "  expert 19: logit = -1.2328, post-softmax = 0.0067\n",
      "  expert 20: logit = -2.0773, post-softmax = 0.0029\n",
      "  expert 21: logit = -1.0160, post-softmax = 0.0083\n",
      "  expert 22: logit = 0.7257, post-softmax = 0.0471\n",
      "  expert 23: logit = 0.0011, post-softmax = 0.0228\n",
      "  expert 24: logit = -0.3303, post-softmax = 0.0164\n",
      "  expert 25: logit = -0.9765, post-softmax = 0.0086\n",
      "  expert 26: logit = -0.5457, post-softmax = 0.0132\n",
      "  expert 27: logit = -0.7151, post-softmax = 0.0112\n",
      "  expert 28: logit = -1.7969, post-softmax = 0.0038\n",
      "  expert 29: logit = -0.5674, post-softmax = 0.0129\n",
      "  expert 30: logit = -0.8089, post-softmax = 0.0102\n",
      "  expert 31: logit = -0.4666, post-softmax = 0.0143\n",
      "  expert 32: logit = -0.3529, post-softmax = 0.0160\n",
      "  expert 33: logit = -1.5241, post-softmax = 0.0050\n",
      "  expert 34: logit = -2.2859, post-softmax = 0.0023\n",
      "  expert 35: logit = -0.6563, post-softmax = 0.0118\n",
      "  expert 36: logit = -0.6800, post-softmax = 0.0116\n",
      "  expert 37: logit = -0.5725, post-softmax = 0.0129\n",
      "  expert 38: logit = -0.6809, post-softmax = 0.0115\n",
      "  expert 39: logit = -0.0270, post-softmax = 0.0222\n",
      "  expert 40: logit = -0.6652, post-softmax = 0.0117\n",
      "  expert 41: logit = -2.4916, post-softmax = 0.0019\n",
      "  expert 42: logit = -0.7520, post-softmax = 0.0108\n",
      "  expert 43: logit = -0.4152, post-softmax = 0.0151\n",
      "  expert 44: logit = -1.0355, post-softmax = 0.0081\n",
      "  expert 45: logit = -0.4626, post-softmax = 0.0144\n",
      "  expert 46: logit = -1.1159, post-softmax = 0.0075\n",
      "  expert 47: logit = -0.7037, post-softmax = 0.0113\n",
      "  expert 48: logit = -0.6161, post-softmax = 0.0123\n",
      "  expert 49: logit = 0.3352, post-softmax = 0.0319\n",
      "  expert 50: logit = -0.8135, post-softmax = 0.0101\n",
      "  expert 51: logit = -2.1598, post-softmax = 0.0026\n",
      "  expert 52: logit = -0.4058, post-softmax = 0.0152\n",
      "  expert 53: logit = -0.1996, post-softmax = 0.0187\n",
      "  expert 54: logit = -0.3878, post-softmax = 0.0155\n",
      "  expert 55: logit = -1.3618, post-softmax = 0.0058\n",
      "  expert 56: logit = 0.0691, post-softmax = 0.0244\n",
      "  expert 57: logit = -0.3797, post-softmax = 0.0156\n",
      "  expert 58: logit = -2.4955, post-softmax = 0.0019\n",
      "  expert 59: logit = -0.7767, post-softmax = 0.0105\n",
      "  expert 60: logit = -0.4174, post-softmax = 0.0150\n",
      "  expert 61: logit = -0.3395, post-softmax = 0.0162\n",
      "  expert 62: logit = -0.3859, post-softmax = 0.0155\n",
      "  expert 63: logit = -0.4522, post-softmax = 0.0145\n",
      "\n",
      "Token: ' the' (ID: 253)\n",
      "  expert 0: logit = -1.8001, post-softmax = 0.0046\n",
      "  expert 1: logit = -0.8550, post-softmax = 0.0119\n",
      "  expert 2: logit = -0.4025, post-softmax = 0.0187\n",
      "  expert 3: logit = -0.0746, post-softmax = 0.0259\n",
      "  expert 4: logit = -0.5371, post-softmax = 0.0163\n",
      "  expert 5: logit = -0.5788, post-softmax = 0.0156\n",
      "  expert 6: logit = 1.0839, post-softmax = 0.0825\n",
      "  expert 7: logit = -0.5750, post-softmax = 0.0157\n",
      "  expert 8: logit = -0.3809, post-softmax = 0.0191\n",
      "  expert 9: logit = -1.9658, post-softmax = 0.0039\n",
      "  expert 10: logit = -0.8162, post-softmax = 0.0123\n",
      "  expert 11: logit = -0.6829, post-softmax = 0.0141\n",
      "  expert 12: logit = -3.4429, post-softmax = 0.0009\n",
      "  expert 13: logit = -1.0918, post-softmax = 0.0094\n",
      "  expert 14: logit = -0.9934, post-softmax = 0.0103\n",
      "  expert 15: logit = -0.5418, post-softmax = 0.0162\n",
      "  expert 16: logit = -1.6407, post-softmax = 0.0054\n",
      "  expert 17: logit = -1.2364, post-softmax = 0.0081\n",
      "  expert 18: logit = -0.3398, post-softmax = 0.0199\n",
      "  expert 19: logit = -1.1712, post-softmax = 0.0087\n",
      "  expert 20: logit = -2.0357, post-softmax = 0.0036\n",
      "  expert 21: logit = -0.8343, post-softmax = 0.0121\n",
      "  expert 22: logit = -1.4800, post-softmax = 0.0064\n",
      "  expert 23: logit = -0.3743, post-softmax = 0.0192\n",
      "  expert 24: logit = -1.3913, post-softmax = 0.0069\n",
      "  expert 25: logit = -0.6769, post-softmax = 0.0142\n",
      "  expert 26: logit = 0.0147, post-softmax = 0.0283\n",
      "  expert 27: logit = -0.4342, post-softmax = 0.0181\n",
      "  expert 28: logit = -1.6272, post-softmax = 0.0055\n",
      "  expert 29: logit = -0.5652, post-softmax = 0.0159\n",
      "  expert 30: logit = -0.9827, post-softmax = 0.0104\n",
      "  expert 31: logit = -0.9758, post-softmax = 0.0105\n",
      "  expert 32: logit = -0.4524, post-softmax = 0.0178\n",
      "  expert 33: logit = -1.2617, post-softmax = 0.0079\n",
      "  expert 34: logit = -1.6854, post-softmax = 0.0052\n",
      "  expert 35: logit = 1.0818, post-softmax = 0.0823\n",
      "  expert 36: logit = -0.8685, post-softmax = 0.0117\n",
      "  expert 37: logit = -0.7306, post-softmax = 0.0134\n",
      "  expert 38: logit = 1.2459, post-softmax = 0.0970\n",
      "  expert 39: logit = -0.5218, post-softmax = 0.0166\n",
      "  expert 40: logit = -1.0510, post-softmax = 0.0098\n",
      "  expert 41: logit = -2.5431, post-softmax = 0.0022\n",
      "  expert 42: logit = -1.1050, post-softmax = 0.0092\n",
      "  expert 43: logit = -0.5481, post-softmax = 0.0161\n",
      "  expert 44: logit = -0.9487, post-softmax = 0.0108\n",
      "  expert 45: logit = -0.6442, post-softmax = 0.0147\n",
      "  expert 46: logit = -1.4169, post-softmax = 0.0068\n",
      "  expert 47: logit = -0.4948, post-softmax = 0.0170\n",
      "  expert 48: logit = -0.8276, post-softmax = 0.0122\n",
      "  expert 49: logit = -0.4958, post-softmax = 0.0170\n",
      "  expert 50: logit = -0.9432, post-softmax = 0.0109\n",
      "  expert 51: logit = -1.9349, post-softmax = 0.0040\n",
      "  expert 52: logit = -0.5353, post-softmax = 0.0163\n",
      "  expert 53: logit = -0.5004, post-softmax = 0.0169\n",
      "  expert 54: logit = -0.5465, post-softmax = 0.0162\n",
      "  expert 55: logit = -0.9876, post-softmax = 0.0104\n",
      "  expert 56: logit = -0.8491, post-softmax = 0.0119\n",
      "  expert 57: logit = -0.8977, post-softmax = 0.0114\n",
      "  expert 58: logit = -2.3586, post-softmax = 0.0026\n",
      "  expert 59: logit = -1.1726, post-softmax = 0.0086\n",
      "  expert 60: logit = -1.5320, post-softmax = 0.0060\n",
      "  expert 61: logit = -1.0912, post-softmax = 0.0094\n",
      "  expert 62: logit = -0.5227, post-softmax = 0.0165\n",
      "  expert 63: logit = -0.3105, post-softmax = 0.0205\n",
      "\n",
      "Token: ' investors' (ID: 12946)\n",
      "  expert 0: logit = -1.3185, post-softmax = 0.0065\n",
      "  expert 1: logit = -0.8376, post-softmax = 0.0105\n",
      "  expert 2: logit = -0.3483, post-softmax = 0.0172\n",
      "  expert 3: logit = -1.0684, post-softmax = 0.0084\n",
      "  expert 4: logit = -0.6729, post-softmax = 0.0124\n",
      "  expert 5: logit = 1.2558, post-softmax = 0.0856\n",
      "  expert 6: logit = 1.0755, post-softmax = 0.0715\n",
      "  expert 7: logit = -3.1236, post-softmax = 0.0011\n",
      "  expert 8: logit = -0.3329, post-softmax = 0.0175\n",
      "  expert 9: logit = -0.2563, post-softmax = 0.0189\n",
      "  expert 10: logit = -0.7585, post-softmax = 0.0114\n",
      "  expert 11: logit = -1.4109, post-softmax = 0.0059\n",
      "  expert 12: logit = -1.3552, post-softmax = 0.0063\n",
      "  expert 13: logit = -0.4646, post-softmax = 0.0153\n",
      "  expert 14: logit = -0.7643, post-softmax = 0.0113\n",
      "  expert 15: logit = -0.4863, post-softmax = 0.0150\n",
      "  expert 16: logit = -0.5946, post-softmax = 0.0134\n",
      "  expert 17: logit = -1.1720, post-softmax = 0.0075\n",
      "  expert 18: logit = 1.0628, post-softmax = 0.0705\n",
      "  expert 19: logit = -1.3048, post-softmax = 0.0066\n",
      "  expert 20: logit = -2.0153, post-softmax = 0.0032\n",
      "  expert 21: logit = -1.3417, post-softmax = 0.0064\n",
      "  expert 22: logit = -2.3709, post-softmax = 0.0023\n",
      "  expert 23: logit = -0.2452, post-softmax = 0.0191\n",
      "  expert 24: logit = -1.4042, post-softmax = 0.0060\n",
      "  expert 25: logit = -0.7821, post-softmax = 0.0111\n",
      "  expert 26: logit = -0.0686, post-softmax = 0.0228\n",
      "  expert 27: logit = -0.7283, post-softmax = 0.0118\n",
      "  expert 28: logit = -1.4304, post-softmax = 0.0058\n",
      "  expert 29: logit = -0.7828, post-softmax = 0.0111\n",
      "  expert 30: logit = -1.0641, post-softmax = 0.0084\n",
      "  expert 31: logit = -0.4290, post-softmax = 0.0159\n",
      "  expert 32: logit = -0.4241, post-softmax = 0.0159\n",
      "  expert 33: logit = -1.2252, post-softmax = 0.0072\n",
      "  expert 34: logit = -2.3063, post-softmax = 0.0024\n",
      "  expert 35: logit = -0.1385, post-softmax = 0.0212\n",
      "  expert 36: logit = -0.7371, post-softmax = 0.0117\n",
      "  expert 37: logit = -0.7373, post-softmax = 0.0117\n",
      "  expert 38: logit = 0.7050, post-softmax = 0.0493\n",
      "  expert 39: logit = -1.2539, post-softmax = 0.0070\n",
      "  expert 40: logit = -0.8571, post-softmax = 0.0103\n",
      "  expert 41: logit = 0.8731, post-softmax = 0.0584\n",
      "  expert 42: logit = -0.7726, post-softmax = 0.0113\n",
      "  expert 43: logit = -2.2927, post-softmax = 0.0025\n",
      "  expert 44: logit = -0.8681, post-softmax = 0.0102\n",
      "  expert 45: logit = -0.8253, post-softmax = 0.0107\n",
      "  expert 46: logit = -1.3276, post-softmax = 0.0065\n",
      "  expert 47: logit = -0.7452, post-softmax = 0.0116\n",
      "  expert 48: logit = -0.4932, post-softmax = 0.0149\n",
      "  expert 49: logit = -0.5254, post-softmax = 0.0144\n",
      "  expert 50: logit = -0.9794, post-softmax = 0.0092\n",
      "  expert 51: logit = -1.2679, post-softmax = 0.0069\n",
      "  expert 52: logit = -0.7692, post-softmax = 0.0113\n",
      "  expert 53: logit = -0.6089, post-softmax = 0.0133\n",
      "  expert 54: logit = -0.4254, post-softmax = 0.0159\n",
      "  expert 55: logit = -0.5015, post-softmax = 0.0148\n",
      "  expert 56: logit = -0.4154, post-softmax = 0.0161\n",
      "  expert 57: logit = -2.0276, post-softmax = 0.0032\n",
      "  expert 58: logit = 0.3760, post-softmax = 0.0355\n",
      "  expert 59: logit = -0.9760, post-softmax = 0.0092\n",
      "  expert 60: logit = -1.0447, post-softmax = 0.0086\n",
      "  expert 61: logit = -0.3593, post-softmax = 0.0170\n",
      "  expert 62: logit = -0.3320, post-softmax = 0.0175\n",
      "  expert 63: logit = -1.1358, post-softmax = 0.0078\n",
      "\n",
      "Token: '?' (ID: 32)\n",
      "  expert 0: logit = -2.3862, post-softmax = 0.0026\n",
      "  expert 1: logit = -0.8838, post-softmax = 0.0117\n",
      "  expert 2: logit = -0.3756, post-softmax = 0.0194\n",
      "  expert 3: logit = -0.6548, post-softmax = 0.0147\n",
      "  expert 4: logit = -1.1419, post-softmax = 0.0090\n",
      "  expert 5: logit = 0.1304, post-softmax = 0.0322\n",
      "  expert 6: logit = 1.0694, post-softmax = 0.0824\n",
      "  expert 7: logit = -0.3071, post-softmax = 0.0208\n",
      "  expert 8: logit = -0.6523, post-softmax = 0.0147\n",
      "  expert 9: logit = -2.2648, post-softmax = 0.0029\n",
      "  expert 10: logit = 0.6953, post-softmax = 0.0567\n",
      "  expert 11: logit = -1.6887, post-softmax = 0.0052\n",
      "  expert 12: logit = -3.5257, post-softmax = 0.0008\n",
      "  expert 13: logit = -1.3389, post-softmax = 0.0074\n",
      "  expert 14: logit = -0.6969, post-softmax = 0.0141\n",
      "  expert 15: logit = -0.4781, post-softmax = 0.0175\n",
      "  expert 16: logit = -0.6151, post-softmax = 0.0153\n",
      "  expert 17: logit = -0.5077, post-softmax = 0.0170\n",
      "  expert 18: logit = -0.6961, post-softmax = 0.0141\n",
      "  expert 19: logit = -0.0657, post-softmax = 0.0265\n",
      "  expert 20: logit = -2.4379, post-softmax = 0.0025\n",
      "  expert 21: logit = -1.5842, post-softmax = 0.0058\n",
      "  expert 22: logit = -0.7737, post-softmax = 0.0131\n",
      "  expert 23: logit = 0.0961, post-softmax = 0.0311\n",
      "  expert 24: logit = -1.0505, post-softmax = 0.0099\n",
      "  expert 25: logit = -0.6296, post-softmax = 0.0151\n",
      "  expert 26: logit = -0.6468, post-softmax = 0.0148\n",
      "  expert 27: logit = -0.5810, post-softmax = 0.0158\n",
      "  expert 28: logit = -2.1085, post-softmax = 0.0034\n",
      "  expert 29: logit = -0.8629, post-softmax = 0.0119\n",
      "  expert 30: logit = -0.0171, post-softmax = 0.0278\n",
      "  expert 31: logit = -0.6743, post-softmax = 0.0144\n",
      "  expert 32: logit = -0.6887, post-softmax = 0.0142\n",
      "  expert 33: logit = -1.3460, post-softmax = 0.0074\n",
      "  expert 34: logit = -2.4705, post-softmax = 0.0024\n",
      "  expert 35: logit = -0.7328, post-softmax = 0.0136\n",
      "  expert 36: logit = -0.5652, post-softmax = 0.0161\n",
      "  expert 37: logit = -0.3936, post-softmax = 0.0191\n",
      "  expert 38: logit = -0.4065, post-softmax = 0.0188\n",
      "  expert 39: logit = -0.2312, post-softmax = 0.0225\n",
      "  expert 40: logit = -0.8301, post-softmax = 0.0123\n",
      "  expert 41: logit = -2.4739, post-softmax = 0.0024\n",
      "  expert 42: logit = -0.5671, post-softmax = 0.0160\n",
      "  expert 43: logit = -0.4624, post-softmax = 0.0178\n",
      "  expert 44: logit = -0.3863, post-softmax = 0.0192\n",
      "  expert 45: logit = -0.6802, post-softmax = 0.0143\n",
      "  expert 46: logit = -1.5678, post-softmax = 0.0059\n",
      "  expert 47: logit = -0.6199, post-softmax = 0.0152\n",
      "  expert 48: logit = -0.8721, post-softmax = 0.0118\n",
      "  expert 49: logit = -0.8574, post-softmax = 0.0120\n",
      "  expert 50: logit = -1.0525, post-softmax = 0.0099\n",
      "  expert 51: logit = -1.1855, post-softmax = 0.0086\n",
      "  expert 52: logit = -0.8104, post-softmax = 0.0126\n",
      "  expert 53: logit = -0.3868, post-softmax = 0.0192\n",
      "  expert 54: logit = -0.5030, post-softmax = 0.0171\n",
      "  expert 55: logit = -0.9213, post-softmax = 0.0113\n",
      "  expert 56: logit = 0.0659, post-softmax = 0.0302\n",
      "  expert 57: logit = -1.0213, post-softmax = 0.0102\n",
      "  expert 58: logit = -2.4408, post-softmax = 0.0025\n",
      "  expert 59: logit = -0.8362, post-softmax = 0.0123\n",
      "  expert 60: logit = -0.8669, post-softmax = 0.0119\n",
      "  expert 61: logit = -0.5759, post-softmax = 0.0159\n",
      "  expert 62: logit = -0.4251, post-softmax = 0.0185\n",
      "  expert 63: logit = -0.0240, post-softmax = 0.0276\n",
      "\n",
      "\n",
      "Top 8 experts for each token:\n",
      "Token: 'How' (ID: 2347)\n",
      "  1. expert 57: probability = 0.1152\n",
      "  2. expert 14: probability = 0.1090\n",
      "  3. expert 61: probability = 0.0507\n",
      "  4. expert 18: probability = 0.0368\n",
      "  5. expert 7: probability = 0.0367\n",
      "  6. expert 43: probability = 0.0311\n",
      "  7. expert 17: probability = 0.0288\n",
      "  8. expert 39: probability = 0.0280\n",
      "\n",
      "Token: ' will' (ID: 588)\n",
      "  1. expert 14: probability = 0.1259\n",
      "  2. expert 6: probability = 0.0779\n",
      "  3. expert 18: probability = 0.0757\n",
      "  4. expert 39: probability = 0.0514\n",
      "  5. expert 63: probability = 0.0296\n",
      "  6. expert 8: probability = 0.0291\n",
      "  7. expert 52: probability = 0.0273\n",
      "  8. expert 23: probability = 0.0267\n",
      "\n",
      "Token: ' you' (ID: 368)\n",
      "  1. expert 6: probability = 0.0901\n",
      "  2. expert 35: probability = 0.0573\n",
      "  3. expert 48: probability = 0.0506\n",
      "  4. expert 7: probability = 0.0336\n",
      "  5. expert 38: probability = 0.0320\n",
      "  6. expert 23: probability = 0.0315\n",
      "  7. expert 43: probability = 0.0309\n",
      "  8. expert 39: probability = 0.0301\n",
      "\n",
      "Token: ' pitch' (ID: 11288)\n",
      "  1. expert 35: probability = 0.1120\n",
      "  2. expert 1: probability = 0.0952\n",
      "  3. expert 6: probability = 0.0526\n",
      "  4. expert 23: probability = 0.0259\n",
      "  5. expert 58: probability = 0.0250\n",
      "  6. expert 8: probability = 0.0244\n",
      "  7. expert 49: probability = 0.0238\n",
      "  8. expert 48: probability = 0.0207\n",
      "\n",
      "Token: ' your' (ID: 634)\n",
      "  1. expert 6: probability = 0.0664\n",
      "  2. expert 60: probability = 0.0489\n",
      "  3. expert 24: probability = 0.0465\n",
      "  4. expert 57: probability = 0.0385\n",
      "  5. expert 55: probability = 0.0359\n",
      "  6. expert 42: probability = 0.0347\n",
      "  7. expert 38: probability = 0.0342\n",
      "  8. expert 46: probability = 0.0299\n",
      "\n",
      "Token: ' idea' (ID: 2934)\n",
      "  1. expert 1: probability = 0.0915\n",
      "  2. expert 61: probability = 0.0794\n",
      "  3. expert 56: probability = 0.0727\n",
      "  4. expert 6: probability = 0.0510\n",
      "  5. expert 5: probability = 0.0449\n",
      "  6. expert 23: probability = 0.0221\n",
      "  7. expert 8: probability = 0.0206\n",
      "  8. expert 60: probability = 0.0194\n",
      "\n",
      "Token: ' to' (ID: 281)\n",
      "  1. expert 8: probability = 0.1017\n",
      "  2. expert 6: probability = 0.0638\n",
      "  3. expert 1: probability = 0.0617\n",
      "  4. expert 22: probability = 0.0471\n",
      "  5. expert 5: probability = 0.0365\n",
      "  6. expert 49: probability = 0.0319\n",
      "  7. expert 56: probability = 0.0244\n",
      "  8. expert 23: probability = 0.0228\n",
      "\n",
      "Token: ' the' (ID: 253)\n",
      "  1. expert 38: probability = 0.0970\n",
      "  2. expert 6: probability = 0.0825\n",
      "  3. expert 35: probability = 0.0823\n",
      "  4. expert 26: probability = 0.0283\n",
      "  5. expert 3: probability = 0.0259\n",
      "  6. expert 63: probability = 0.0205\n",
      "  7. expert 18: probability = 0.0199\n",
      "  8. expert 23: probability = 0.0192\n",
      "\n",
      "Token: ' investors' (ID: 12946)\n",
      "  1. expert 5: probability = 0.0856\n",
      "  2. expert 6: probability = 0.0715\n",
      "  3. expert 18: probability = 0.0705\n",
      "  4. expert 41: probability = 0.0584\n",
      "  5. expert 38: probability = 0.0493\n",
      "  6. expert 58: probability = 0.0355\n",
      "  7. expert 26: probability = 0.0228\n",
      "  8. expert 35: probability = 0.0212\n",
      "\n",
      "Token: '?' (ID: 32)\n",
      "  1. expert 6: probability = 0.0824\n",
      "  2. expert 10: probability = 0.0567\n",
      "  3. expert 5: probability = 0.0322\n",
      "  4. expert 23: probability = 0.0311\n",
      "  5. expert 56: probability = 0.0302\n",
      "  6. expert 30: probability = 0.0278\n",
      "  7. expert 63: probability = 0.0276\n",
      "  8. expert 19: probability = 0.0265\n",
      "\n",
      "Analysis results saved to expert_routing_analysis_1.json\n"
     ]
    }
   ],
   "source": [
    "input_text = \"How will you pitch your idea to the investors?\"\n",
    "last_layer_router_logits, analysis_results = analyze_expert_routing(input_text, model, tokenizer,layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input:\n",
      "Token: 'What', ID: 1276\n",
      "Token: ''s', ID: 434\n",
      "Token: ' the', ID: 253\n",
      "Token: ' musical', ID: 12256\n",
      "Token: ' pitch', ID: 11288\n",
      "Token: ' of', ID: 273\n",
      "Token: ' that', ID: 326\n",
      "Token: ' note', ID: 3877\n",
      "Token: '?', ID: 32\n",
      "\n",
      "Input shape: torch.Size([1, 9])\n",
      "\n",
      "Router logits are from layer 0 of the model\n",
      "\n",
      "Router logits and probabilities for each token:\n",
      "Token: 'What' (ID: 1276)\n",
      "  expert 0: logit = -3.1594, post-softmax = 0.0007\n",
      "  expert 1: logit = -0.1499, post-softmax = 0.0145\n",
      "  expert 2: logit = -0.1624, post-softmax = 0.0143\n",
      "  expert 3: logit = -0.5962, post-softmax = 0.0093\n",
      "  expert 4: logit = 0.0160, post-softmax = 0.0171\n",
      "  expert 5: logit = -0.7897, post-softmax = 0.0076\n",
      "  expert 6: logit = -0.2797, post-softmax = 0.0127\n",
      "  expert 7: logit = 0.4158, post-softmax = 0.0255\n",
      "  expert 8: logit = -0.4473, post-softmax = 0.0108\n",
      "  expert 9: logit = -1.2607, post-softmax = 0.0048\n",
      "  expert 10: logit = -0.7840, post-softmax = 0.0077\n",
      "  expert 11: logit = -0.9305, post-softmax = 0.0066\n",
      "  expert 12: logit = -3.3033, post-softmax = 0.0006\n",
      "  expert 13: logit = -2.5774, post-softmax = 0.0013\n",
      "  expert 14: logit = 1.7173, post-softmax = 0.0938\n",
      "  expert 15: logit = -0.9227, post-softmax = 0.0067\n",
      "  expert 16: logit = -0.5682, post-softmax = 0.0095\n",
      "  expert 17: logit = 0.1749, post-softmax = 0.0201\n",
      "  expert 18: logit = -1.0046, post-softmax = 0.0062\n",
      "  expert 19: logit = -0.6748, post-softmax = 0.0086\n",
      "  expert 20: logit = -2.2250, post-softmax = 0.0018\n",
      "  expert 21: logit = -2.0674, post-softmax = 0.0021\n",
      "  expert 22: logit = -0.3692, post-softmax = 0.0116\n",
      "  expert 23: logit = -0.1851, post-softmax = 0.0140\n",
      "  expert 24: logit = -0.1753, post-softmax = 0.0141\n",
      "  expert 25: logit = -0.3484, post-softmax = 0.0119\n",
      "  expert 26: logit = -0.6463, post-softmax = 0.0088\n",
      "  expert 27: logit = -0.0697, post-softmax = 0.0157\n",
      "  expert 28: logit = -2.8167, post-softmax = 0.0010\n",
      "  expert 29: logit = -0.5153, post-softmax = 0.0101\n",
      "  expert 30: logit = 0.3207, post-softmax = 0.0232\n",
      "  expert 31: logit = -0.8042, post-softmax = 0.0075\n",
      "  expert 32: logit = -0.7553, post-softmax = 0.0079\n",
      "  expert 33: logit = -1.0572, post-softmax = 0.0059\n",
      "  expert 34: logit = -2.4620, post-softmax = 0.0014\n",
      "  expert 35: logit = -0.1865, post-softmax = 0.0140\n",
      "  expert 36: logit = -0.5052, post-softmax = 0.0102\n",
      "  expert 37: logit = -0.3584, post-softmax = 0.0118\n",
      "  expert 38: logit = -0.6568, post-softmax = 0.0087\n",
      "  expert 39: logit = 0.3217, post-softmax = 0.0232\n",
      "  expert 40: logit = -1.5198, post-softmax = 0.0037\n",
      "  expert 41: logit = -1.8413, post-softmax = 0.0027\n",
      "  expert 42: logit = -0.1408, post-softmax = 0.0146\n",
      "  expert 43: logit = 0.4102, post-softmax = 0.0254\n",
      "  expert 44: logit = -0.6493, post-softmax = 0.0088\n",
      "  expert 45: logit = 0.0022, post-softmax = 0.0169\n",
      "  expert 46: logit = -0.9157, post-softmax = 0.0067\n",
      "  expert 47: logit = -0.1120, post-softmax = 0.0151\n",
      "  expert 48: logit = -0.1945, post-softmax = 0.0139\n",
      "  expert 49: logit = -0.6204, post-softmax = 0.0091\n",
      "  expert 50: logit = -0.3106, post-softmax = 0.0123\n",
      "  expert 51: logit = -1.0021, post-softmax = 0.0062\n",
      "  expert 52: logit = -0.3019, post-softmax = 0.0125\n",
      "  expert 53: logit = -2.9042, post-softmax = 0.0009\n",
      "  expert 54: logit = -0.5925, post-softmax = 0.0093\n",
      "  expert 55: logit = -0.4861, post-softmax = 0.0104\n",
      "  expert 56: logit = -0.4230, post-softmax = 0.0110\n",
      "  expert 57: logit = 1.9739, post-softmax = 0.1213\n",
      "  expert 58: logit = -3.2299, post-softmax = 0.0007\n",
      "  expert 59: logit = -0.4027, post-softmax = 0.0113\n",
      "  expert 60: logit = -0.2822, post-softmax = 0.0127\n",
      "  expert 61: logit = 1.7979, post-softmax = 0.1017\n",
      "  expert 62: logit = 1.4966, post-softmax = 0.0752\n",
      "  expert 63: logit = -0.1646, post-softmax = 0.0143\n",
      "\n",
      "Token: ''s' (ID: 434)\n",
      "  expert 0: logit = -2.4319, post-softmax = 0.0013\n",
      "  expert 1: logit = -0.3704, post-softmax = 0.0105\n",
      "  expert 2: logit = 0.0170, post-softmax = 0.0154\n",
      "  expert 3: logit = -0.7175, post-softmax = 0.0074\n",
      "  expert 4: logit = -0.2747, post-softmax = 0.0115\n",
      "  expert 5: logit = -0.0352, post-softmax = 0.0146\n",
      "  expert 6: logit = 1.4998, post-softmax = 0.0679\n",
      "  expert 7: logit = -0.4073, post-softmax = 0.0101\n",
      "  expert 8: logit = 1.4206, post-softmax = 0.0627\n",
      "  expert 9: logit = -2.6434, post-softmax = 0.0011\n",
      "  expert 10: logit = -0.2054, post-softmax = 0.0123\n",
      "  expert 11: logit = -2.7131, post-softmax = 0.0010\n",
      "  expert 12: logit = -2.6080, post-softmax = 0.0011\n",
      "  expert 13: logit = -0.4864, post-softmax = 0.0093\n",
      "  expert 14: logit = 2.0716, post-softmax = 0.1203\n",
      "  expert 15: logit = 0.2566, post-softmax = 0.0196\n",
      "  expert 16: logit = -0.6229, post-softmax = 0.0081\n",
      "  expert 17: logit = 0.0364, post-softmax = 0.0157\n",
      "  expert 18: logit = 0.0576, post-softmax = 0.0161\n",
      "  expert 19: logit = -2.6626, post-softmax = 0.0011\n",
      "  expert 20: logit = -0.5620, post-softmax = 0.0086\n",
      "  expert 21: logit = -1.4335, post-softmax = 0.0036\n",
      "  expert 22: logit = -1.9193, post-softmax = 0.0022\n",
      "  expert 23: logit = 0.5093, post-softmax = 0.0252\n",
      "  expert 24: logit = -0.4294, post-softmax = 0.0099\n",
      "  expert 25: logit = -0.0539, post-softmax = 0.0144\n",
      "  expert 26: logit = -0.4368, post-softmax = 0.0098\n",
      "  expert 27: logit = -0.0726, post-softmax = 0.0141\n",
      "  expert 28: logit = -3.5826, post-softmax = 0.0004\n",
      "  expert 29: logit = -0.5702, post-softmax = 0.0086\n",
      "  expert 30: logit = -0.3774, post-softmax = 0.0104\n",
      "  expert 31: logit = -1.0643, post-softmax = 0.0052\n",
      "  expert 32: logit = -0.1536, post-softmax = 0.0130\n",
      "  expert 33: logit = -0.8272, post-softmax = 0.0066\n",
      "  expert 34: logit = -2.8214, post-softmax = 0.0009\n",
      "  expert 35: logit = -0.5667, post-softmax = 0.0086\n",
      "  expert 36: logit = -0.2709, post-softmax = 0.0116\n",
      "  expert 37: logit = -0.5265, post-softmax = 0.0089\n",
      "  expert 38: logit = -0.5343, post-softmax = 0.0089\n",
      "  expert 39: logit = 0.8674, post-softmax = 0.0361\n",
      "  expert 40: logit = -0.8599, post-softmax = 0.0064\n",
      "  expert 41: logit = -2.4376, post-softmax = 0.0013\n",
      "  expert 42: logit = -0.4664, post-softmax = 0.0095\n",
      "  expert 43: logit = -0.2550, post-softmax = 0.0117\n",
      "  expert 44: logit = -1.0180, post-softmax = 0.0055\n",
      "  expert 45: logit = -0.1081, post-softmax = 0.0136\n",
      "  expert 46: logit = -1.1283, post-softmax = 0.0049\n",
      "  expert 47: logit = 0.0239, post-softmax = 0.0155\n",
      "  expert 48: logit = 0.0458, post-softmax = 0.0159\n",
      "  expert 49: logit = -0.4980, post-softmax = 0.0092\n",
      "  expert 50: logit = -0.2711, post-softmax = 0.0116\n",
      "  expert 51: logit = -1.9708, post-softmax = 0.0021\n",
      "  expert 52: logit = 0.7771, post-softmax = 0.0330\n",
      "  expert 53: logit = -0.2065, post-softmax = 0.0123\n",
      "  expert 54: logit = 0.0319, post-softmax = 0.0156\n",
      "  expert 55: logit = 0.0394, post-softmax = 0.0158\n",
      "  expert 56: logit = -0.2436, post-softmax = 0.0119\n",
      "  expert 57: logit = -0.6024, post-softmax = 0.0083\n",
      "  expert 58: logit = -2.4872, post-softmax = 0.0013\n",
      "  expert 59: logit = -0.3649, post-softmax = 0.0105\n",
      "  expert 60: logit = -1.0349, post-softmax = 0.0054\n",
      "  expert 61: logit = 1.1231, post-softmax = 0.0466\n",
      "  expert 62: logit = 1.7328, post-softmax = 0.0857\n",
      "  expert 63: logit = 0.7593, post-softmax = 0.0324\n",
      "\n",
      "Token: ' the' (ID: 253)\n",
      "  expert 0: logit = -0.8816, post-softmax = 0.0076\n",
      "  expert 1: logit = -0.4596, post-softmax = 0.0116\n",
      "  expert 2: logit = -0.2511, post-softmax = 0.0143\n",
      "  expert 3: logit = 0.1426, post-softmax = 0.0213\n",
      "  expert 4: logit = -0.5461, post-softmax = 0.0107\n",
      "  expert 5: logit = -0.6018, post-softmax = 0.0101\n",
      "  expert 6: logit = 1.5200, post-softmax = 0.0843\n",
      "  expert 7: logit = -0.4305, post-softmax = 0.0120\n",
      "  expert 8: logit = 0.0580, post-softmax = 0.0195\n",
      "  expert 9: logit = -2.6323, post-softmax = 0.0013\n",
      "  expert 10: logit = 0.8800, post-softmax = 0.0444\n",
      "  expert 11: logit = -1.0805, post-softmax = 0.0063\n",
      "  expert 12: logit = -1.8509, post-softmax = 0.0029\n",
      "  expert 13: logit = -1.5420, post-softmax = 0.0039\n",
      "  expert 14: logit = -0.3865, post-softmax = 0.0125\n",
      "  expert 15: logit = -0.2522, post-softmax = 0.0143\n",
      "  expert 16: logit = -1.4222, post-softmax = 0.0044\n",
      "  expert 17: logit = -0.4124, post-softmax = 0.0122\n",
      "  expert 18: logit = -0.3748, post-softmax = 0.0127\n",
      "  expert 19: logit = -2.0945, post-softmax = 0.0023\n",
      "  expert 20: logit = -1.8735, post-softmax = 0.0028\n",
      "  expert 21: logit = -0.6905, post-softmax = 0.0092\n",
      "  expert 22: logit = -0.8035, post-softmax = 0.0083\n",
      "  expert 23: logit = 0.5117, post-softmax = 0.0307\n",
      "  expert 24: logit = -0.8237, post-softmax = 0.0081\n",
      "  expert 25: logit = -0.2727, post-softmax = 0.0140\n",
      "  expert 26: logit = -1.5272, post-softmax = 0.0040\n",
      "  expert 27: logit = -0.2470, post-softmax = 0.0144\n",
      "  expert 28: logit = -0.4801, post-softmax = 0.0114\n",
      "  expert 29: logit = -0.5673, post-softmax = 0.0105\n",
      "  expert 30: logit = -0.4354, post-softmax = 0.0119\n",
      "  expert 31: logit = -0.5012, post-softmax = 0.0112\n",
      "  expert 32: logit = -0.2100, post-softmax = 0.0149\n",
      "  expert 33: logit = -2.3161, post-softmax = 0.0018\n",
      "  expert 34: logit = -0.7912, post-softmax = 0.0084\n",
      "  expert 35: logit = -0.9782, post-softmax = 0.0069\n",
      "  expert 36: logit = -0.5756, post-softmax = 0.0104\n",
      "  expert 37: logit = -0.4349, post-softmax = 0.0119\n",
      "  expert 38: logit = 1.1075, post-softmax = 0.0558\n",
      "  expert 39: logit = -0.0421, post-softmax = 0.0177\n",
      "  expert 40: logit = -0.6502, post-softmax = 0.0096\n",
      "  expert 41: logit = -2.3356, post-softmax = 0.0018\n",
      "  expert 42: logit = -0.5840, post-softmax = 0.0103\n",
      "  expert 43: logit = -0.3445, post-softmax = 0.0131\n",
      "  expert 44: logit = -0.6716, post-softmax = 0.0094\n",
      "  expert 45: logit = -0.2082, post-softmax = 0.0150\n",
      "  expert 46: logit = -0.8585, post-softmax = 0.0078\n",
      "  expert 47: logit = -0.5585, post-softmax = 0.0105\n",
      "  expert 48: logit = 0.9913, post-softmax = 0.0497\n",
      "  expert 49: logit = -0.3057, post-softmax = 0.0136\n",
      "  expert 50: logit = -0.3447, post-softmax = 0.0131\n",
      "  expert 51: logit = -1.9756, post-softmax = 0.0026\n",
      "  expert 52: logit = -0.0129, post-softmax = 0.0182\n",
      "  expert 53: logit = -0.0901, post-softmax = 0.0168\n",
      "  expert 54: logit = -0.2997, post-softmax = 0.0137\n",
      "  expert 55: logit = 0.3491, post-softmax = 0.0261\n",
      "  expert 56: logit = 1.9260, post-softmax = 0.1265\n",
      "  expert 57: logit = -1.2570, post-softmax = 0.0052\n",
      "  expert 58: logit = -2.0671, post-softmax = 0.0023\n",
      "  expert 59: logit = -1.2023, post-softmax = 0.0055\n",
      "  expert 60: logit = -1.2565, post-softmax = 0.0052\n",
      "  expert 61: logit = -0.7093, post-softmax = 0.0091\n",
      "  expert 62: logit = 0.0179, post-softmax = 0.0188\n",
      "  expert 63: logit = 0.2329, post-softmax = 0.0233\n",
      "\n",
      "Token: ' musical' (ID: 12256)\n",
      "  expert 0: logit = -1.2927, post-softmax = 0.0052\n",
      "  expert 1: logit = -0.4604, post-softmax = 0.0119\n",
      "  expert 2: logit = -0.0772, post-softmax = 0.0174\n",
      "  expert 3: logit = -0.5234, post-softmax = 0.0112\n",
      "  expert 4: logit = -0.4011, post-softmax = 0.0126\n",
      "  expert 5: logit = 1.4915, post-softmax = 0.0837\n",
      "  expert 6: logit = 1.5852, post-softmax = 0.0919\n",
      "  expert 7: logit = -2.1388, post-softmax = 0.0022\n",
      "  expert 8: logit = -0.3701, post-softmax = 0.0130\n",
      "  expert 9: logit = -1.9696, post-softmax = 0.0026\n",
      "  expert 10: logit = -0.1944, post-softmax = 0.0155\n",
      "  expert 11: logit = -1.9169, post-softmax = 0.0028\n",
      "  expert 12: logit = -0.0270, post-softmax = 0.0183\n",
      "  expert 13: logit = -0.3839, post-softmax = 0.0128\n",
      "  expert 14: logit = -0.6699, post-softmax = 0.0096\n",
      "  expert 15: logit = -0.0305, post-softmax = 0.0183\n",
      "  expert 16: logit = -0.2730, post-softmax = 0.0143\n",
      "  expert 17: logit = -0.4077, post-softmax = 0.0125\n",
      "  expert 18: logit = 1.5185, post-softmax = 0.0860\n",
      "  expert 19: logit = -1.7712, post-softmax = 0.0032\n",
      "  expert 20: logit = -1.9686, post-softmax = 0.0026\n",
      "  expert 21: logit = -1.7012, post-softmax = 0.0034\n",
      "  expert 22: logit = -1.6637, post-softmax = 0.0036\n",
      "  expert 23: logit = 0.1739, post-softmax = 0.0224\n",
      "  expert 24: logit = -0.8601, post-softmax = 0.0080\n",
      "  expert 25: logit = -0.5629, post-softmax = 0.0107\n",
      "  expert 26: logit = -0.4110, post-softmax = 0.0125\n",
      "  expert 27: logit = -0.6828, post-softmax = 0.0095\n",
      "  expert 28: logit = -1.7476, post-softmax = 0.0033\n",
      "  expert 29: logit = -0.6023, post-softmax = 0.0103\n",
      "  expert 30: logit = -0.4795, post-softmax = 0.0117\n",
      "  expert 31: logit = 0.0314, post-softmax = 0.0194\n",
      "  expert 32: logit = -0.2820, post-softmax = 0.0142\n",
      "  expert 33: logit = -1.5141, post-softmax = 0.0041\n",
      "  expert 34: logit = -2.6679, post-softmax = 0.0013\n",
      "  expert 35: logit = -0.6249, post-softmax = 0.0101\n",
      "  expert 36: logit = -0.3433, post-softmax = 0.0134\n",
      "  expert 37: logit = -0.5413, post-softmax = 0.0110\n",
      "  expert 38: logit = 0.7492, post-softmax = 0.0399\n",
      "  expert 39: logit = -1.5867, post-softmax = 0.0039\n",
      "  expert 40: logit = -0.7091, post-softmax = 0.0093\n",
      "  expert 41: logit = 0.9629, post-softmax = 0.0494\n",
      "  expert 42: logit = -0.4623, post-softmax = 0.0119\n",
      "  expert 43: logit = -1.2100, post-softmax = 0.0056\n",
      "  expert 44: logit = -0.3015, post-softmax = 0.0139\n",
      "  expert 45: logit = -0.4376, post-softmax = 0.0122\n",
      "  expert 46: logit = -0.8480, post-softmax = 0.0081\n",
      "  expert 47: logit = -0.2651, post-softmax = 0.0145\n",
      "  expert 48: logit = 0.1718, post-softmax = 0.0224\n",
      "  expert 49: logit = -0.2163, post-softmax = 0.0152\n",
      "  expert 50: logit = -0.4672, post-softmax = 0.0118\n",
      "  expert 51: logit = -0.7959, post-softmax = 0.0085\n",
      "  expert 52: logit = -0.7129, post-softmax = 0.0092\n",
      "  expert 53: logit = -0.2940, post-softmax = 0.0140\n",
      "  expert 54: logit = -0.3328, post-softmax = 0.0135\n",
      "  expert 55: logit = -0.1765, post-softmax = 0.0158\n",
      "  expert 56: logit = 0.1465, post-softmax = 0.0218\n",
      "  expert 57: logit = -2.0095, post-softmax = 0.0025\n",
      "  expert 58: logit = -0.0689, post-softmax = 0.0176\n",
      "  expert 59: logit = -0.8047, post-softmax = 0.0084\n",
      "  expert 60: logit = -0.2999, post-softmax = 0.0140\n",
      "  expert 61: logit = -0.1360, post-softmax = 0.0164\n",
      "  expert 62: logit = -0.2833, post-softmax = 0.0142\n",
      "  expert 63: logit = -0.6988, post-softmax = 0.0094\n",
      "\n",
      "Token: ' pitch' (ID: 11288)\n",
      "  expert 0: logit = -1.4346, post-softmax = 0.0041\n",
      "  expert 1: logit = -1.2424, post-softmax = 0.0050\n",
      "  expert 2: logit = -0.1638, post-softmax = 0.0147\n",
      "  expert 3: logit = -0.0599, post-softmax = 0.0164\n",
      "  expert 4: logit = -0.1064, post-softmax = 0.0156\n",
      "  expert 5: logit = -0.2760, post-softmax = 0.0132\n",
      "  expert 6: logit = 1.0300, post-softmax = 0.0486\n",
      "  expert 7: logit = -1.4496, post-softmax = 0.0041\n",
      "  expert 8: logit = -0.7013, post-softmax = 0.0086\n",
      "  expert 9: logit = -2.3128, post-softmax = 0.0017\n",
      "  expert 10: logit = -0.7332, post-softmax = 0.0083\n",
      "  expert 11: logit = -1.8077, post-softmax = 0.0028\n",
      "  expert 12: logit = 0.4425, post-softmax = 0.0270\n",
      "  expert 13: logit = -0.3980, post-softmax = 0.0117\n",
      "  expert 14: logit = -0.8575, post-softmax = 0.0074\n",
      "  expert 15: logit = -0.7612, post-softmax = 0.0081\n",
      "  expert 16: logit = -0.8315, post-softmax = 0.0076\n",
      "  expert 17: logit = -0.3009, post-softmax = 0.0129\n",
      "  expert 18: logit = -0.1062, post-softmax = 0.0156\n",
      "  expert 19: logit = -0.7448, post-softmax = 0.0082\n",
      "  expert 20: logit = -2.0471, post-softmax = 0.0022\n",
      "  expert 21: logit = -2.0354, post-softmax = 0.0023\n",
      "  expert 22: logit = -0.2903, post-softmax = 0.0130\n",
      "  expert 23: logit = -0.0823, post-softmax = 0.0160\n",
      "  expert 24: logit = 0.2646, post-softmax = 0.0226\n",
      "  expert 25: logit = -0.0673, post-softmax = 0.0162\n",
      "  expert 26: logit = -0.6030, post-softmax = 0.0095\n",
      "  expert 27: logit = -0.4760, post-softmax = 0.0108\n",
      "  expert 28: logit = -1.8738, post-softmax = 0.0027\n",
      "  expert 29: logit = -0.9497, post-softmax = 0.0067\n",
      "  expert 30: logit = 0.5254, post-softmax = 0.0294\n",
      "  expert 31: logit = -0.6414, post-softmax = 0.0091\n",
      "  expert 32: logit = -1.2223, post-softmax = 0.0051\n",
      "  expert 33: logit = -1.7031, post-softmax = 0.0032\n",
      "  expert 34: logit = -2.5914, post-softmax = 0.0013\n",
      "  expert 35: logit = -0.9541, post-softmax = 0.0067\n",
      "  expert 36: logit = -0.7404, post-softmax = 0.0083\n",
      "  expert 37: logit = -0.7937, post-softmax = 0.0079\n",
      "  expert 38: logit = -0.5958, post-softmax = 0.0096\n",
      "  expert 39: logit = -1.7893, post-softmax = 0.0029\n",
      "  expert 40: logit = -0.3592, post-softmax = 0.0121\n",
      "  expert 41: logit = 1.2001, post-softmax = 0.0577\n",
      "  expert 42: logit = -0.5144, post-softmax = 0.0104\n",
      "  expert 43: logit = -2.1219, post-softmax = 0.0021\n",
      "  expert 44: logit = 1.9568, post-softmax = 0.1229\n",
      "  expert 45: logit = -0.3866, post-softmax = 0.0118\n",
      "  expert 46: logit = 0.3189, post-softmax = 0.0239\n",
      "  expert 47: logit = -0.2946, post-softmax = 0.0129\n",
      "  expert 48: logit = -0.7853, post-softmax = 0.0079\n",
      "  expert 49: logit = 0.4739, post-softmax = 0.0279\n",
      "  expert 50: logit = 1.3735, post-softmax = 0.0686\n",
      "  expert 51: logit = -0.0655, post-softmax = 0.0163\n",
      "  expert 52: logit = -0.5198, post-softmax = 0.0103\n",
      "  expert 53: logit = -0.8301, post-softmax = 0.0076\n",
      "  expert 54: logit = -0.4867, post-softmax = 0.0107\n",
      "  expert 55: logit = -0.0377, post-softmax = 0.0167\n",
      "  expert 56: logit = -0.3325, post-softmax = 0.0125\n",
      "  expert 57: logit = -1.5703, post-softmax = 0.0036\n",
      "  expert 58: logit = -0.0391, post-softmax = 0.0167\n",
      "  expert 59: logit = -0.0738, post-softmax = 0.0161\n",
      "  expert 60: logit = 1.3085, post-softmax = 0.0643\n",
      "  expert 61: logit = 0.1615, post-softmax = 0.0204\n",
      "  expert 62: logit = -0.4929, post-softmax = 0.0106\n",
      "  expert 63: logit = -0.6445, post-softmax = 0.0091\n",
      "\n",
      "Token: ' of' (ID: 273)\n",
      "  expert 0: logit = -1.4409, post-softmax = 0.0039\n",
      "  expert 1: logit = -0.3240, post-softmax = 0.0119\n",
      "  expert 2: logit = -0.1092, post-softmax = 0.0148\n",
      "  expert 3: logit = 0.3254, post-softmax = 0.0228\n",
      "  expert 4: logit = -0.9766, post-softmax = 0.0062\n",
      "  expert 5: logit = -0.2035, post-softmax = 0.0134\n",
      "  expert 6: logit = 1.1505, post-softmax = 0.0520\n",
      "  expert 7: logit = -0.0790, post-softmax = 0.0152\n",
      "  expert 8: logit = 1.0393, post-softmax = 0.0465\n",
      "  expert 9: logit = -2.6261, post-softmax = 0.0012\n",
      "  expert 10: logit = -0.8893, post-softmax = 0.0068\n",
      "  expert 11: logit = -1.2099, post-softmax = 0.0049\n",
      "  expert 12: logit = -2.6581, post-softmax = 0.0012\n",
      "  expert 13: logit = -1.3836, post-softmax = 0.0041\n",
      "  expert 14: logit = -0.6265, post-softmax = 0.0088\n",
      "  expert 15: logit = -0.5356, post-softmax = 0.0096\n",
      "  expert 16: logit = -0.8764, post-softmax = 0.0069\n",
      "  expert 17: logit = -0.2541, post-softmax = 0.0128\n",
      "  expert 18: logit = -1.0223, post-softmax = 0.0059\n",
      "  expert 19: logit = -0.4563, post-softmax = 0.0104\n",
      "  expert 20: logit = -2.3991, post-softmax = 0.0015\n",
      "  expert 21: logit = -1.0956, post-softmax = 0.0055\n",
      "  expert 22: logit = 0.0628, post-softmax = 0.0175\n",
      "  expert 23: logit = -0.0856, post-softmax = 0.0151\n",
      "  expert 24: logit = 0.3414, post-softmax = 0.0232\n",
      "  expert 25: logit = -0.6894, post-softmax = 0.0083\n",
      "  expert 26: logit = -0.9447, post-softmax = 0.0064\n",
      "  expert 27: logit = -0.1445, post-softmax = 0.0142\n",
      "  expert 28: logit = -1.4767, post-softmax = 0.0038\n",
      "  expert 29: logit = -0.7204, post-softmax = 0.0080\n",
      "  expert 30: logit = -0.6423, post-softmax = 0.0087\n",
      "  expert 31: logit = -1.1472, post-softmax = 0.0052\n",
      "  expert 32: logit = -0.4690, post-softmax = 0.0103\n",
      "  expert 33: logit = -0.9624, post-softmax = 0.0063\n",
      "  expert 34: logit = -2.0535, post-softmax = 0.0021\n",
      "  expert 35: logit = -0.8414, post-softmax = 0.0071\n",
      "  expert 36: logit = -0.6893, post-softmax = 0.0083\n",
      "  expert 37: logit = -0.6071, post-softmax = 0.0090\n",
      "  expert 38: logit = -0.6807, post-softmax = 0.0083\n",
      "  expert 39: logit = -0.2070, post-softmax = 0.0134\n",
      "  expert 40: logit = -0.4263, post-softmax = 0.0107\n",
      "  expert 41: logit = -2.2668, post-softmax = 0.0017\n",
      "  expert 42: logit = 0.5962, post-softmax = 0.0299\n",
      "  expert 43: logit = -0.1746, post-softmax = 0.0138\n",
      "  expert 44: logit = 0.0832, post-softmax = 0.0179\n",
      "  expert 45: logit = -0.4951, post-softmax = 0.0100\n",
      "  expert 46: logit = 2.7548, post-softmax = 0.2586\n",
      "  expert 47: logit = -0.3749, post-softmax = 0.0113\n",
      "  expert 48: logit = -0.5724, post-softmax = 0.0093\n",
      "  expert 49: logit = -0.5054, post-softmax = 0.0099\n",
      "  expert 50: logit = 0.0290, post-softmax = 0.0169\n",
      "  expert 51: logit = -1.5550, post-softmax = 0.0035\n",
      "  expert 52: logit = -0.3968, post-softmax = 0.0111\n",
      "  expert 53: logit = -0.2110, post-softmax = 0.0133\n",
      "  expert 54: logit = -0.0893, post-softmax = 0.0150\n",
      "  expert 55: logit = 0.2702, post-softmax = 0.0216\n",
      "  expert 56: logit = -0.3488, post-softmax = 0.0116\n",
      "  expert 57: logit = -0.4010, post-softmax = 0.0110\n",
      "  expert 58: logit = -2.6368, post-softmax = 0.0012\n",
      "  expert 59: logit = 0.1305, post-softmax = 0.0187\n",
      "  expert 60: logit = 0.7785, post-softmax = 0.0358\n",
      "  expert 61: logit = -1.2488, post-softmax = 0.0047\n",
      "  expert 62: logit = -0.7939, post-softmax = 0.0074\n",
      "  expert 63: logit = -0.1928, post-softmax = 0.0136\n",
      "\n",
      "Token: ' that' (ID: 326)\n",
      "  expert 0: logit = -1.2382, post-softmax = 0.0058\n",
      "  expert 1: logit = -1.1829, post-softmax = 0.0062\n",
      "  expert 2: logit = -0.1981, post-softmax = 0.0165\n",
      "  expert 3: logit = -0.4038, post-softmax = 0.0135\n",
      "  expert 4: logit = -0.2947, post-softmax = 0.0150\n",
      "  expert 5: logit = -0.9922, post-softmax = 0.0075\n",
      "  expert 6: logit = 1.2222, post-softmax = 0.0685\n",
      "  expert 7: logit = -0.6345, post-softmax = 0.0107\n",
      "  expert 8: logit = -0.0039, post-softmax = 0.0201\n",
      "  expert 9: logit = -2.4213, post-softmax = 0.0018\n",
      "  expert 10: logit = -0.4038, post-softmax = 0.0135\n",
      "  expert 11: logit = -1.6682, post-softmax = 0.0038\n",
      "  expert 12: logit = -2.7465, post-softmax = 0.0013\n",
      "  expert 13: logit = 0.5804, post-softmax = 0.0360\n",
      "  expert 14: logit = -0.2684, post-softmax = 0.0154\n",
      "  expert 15: logit = -0.8065, post-softmax = 0.0090\n",
      "  expert 16: logit = -0.0283, post-softmax = 0.0196\n",
      "  expert 17: logit = -0.6531, post-softmax = 0.0105\n",
      "  expert 18: logit = 1.5473, post-softmax = 0.0948\n",
      "  expert 19: logit = -0.8665, post-softmax = 0.0085\n",
      "  expert 20: logit = -2.6576, post-softmax = 0.0014\n",
      "  expert 21: logit = -1.2039, post-softmax = 0.0061\n",
      "  expert 22: logit = -0.1925, post-softmax = 0.0166\n",
      "  expert 23: logit = -0.0985, post-softmax = 0.0183\n",
      "  expert 24: logit = -1.4896, post-softmax = 0.0045\n",
      "  expert 25: logit = -0.0736, post-softmax = 0.0187\n",
      "  expert 26: logit = 0.7991, post-softmax = 0.0449\n",
      "  expert 27: logit = -0.2294, post-softmax = 0.0160\n",
      "  expert 28: logit = -1.4098, post-softmax = 0.0049\n",
      "  expert 29: logit = -0.5540, post-softmax = 0.0116\n",
      "  expert 30: logit = -0.6817, post-softmax = 0.0102\n",
      "  expert 31: logit = -0.8347, post-softmax = 0.0088\n",
      "  expert 32: logit = -0.2784, post-softmax = 0.0153\n",
      "  expert 33: logit = -0.7390, post-softmax = 0.0096\n",
      "  expert 34: logit = -1.4866, post-softmax = 0.0046\n",
      "  expert 35: logit = -0.6765, post-softmax = 0.0103\n",
      "  expert 36: logit = -0.5073, post-softmax = 0.0121\n",
      "  expert 37: logit = 0.0051, post-softmax = 0.0203\n",
      "  expert 38: logit = -0.6298, post-softmax = 0.0107\n",
      "  expert 39: logit = -0.8622, post-softmax = 0.0085\n",
      "  expert 40: logit = -0.6058, post-softmax = 0.0110\n",
      "  expert 41: logit = -2.6641, post-softmax = 0.0014\n",
      "  expert 42: logit = -0.2007, post-softmax = 0.0165\n",
      "  expert 43: logit = -0.7956, post-softmax = 0.0091\n",
      "  expert 44: logit = -0.4933, post-softmax = 0.0123\n",
      "  expert 45: logit = -0.4203, post-softmax = 0.0133\n",
      "  expert 46: logit = -0.8713, post-softmax = 0.0084\n",
      "  expert 47: logit = -0.4216, post-softmax = 0.0132\n",
      "  expert 48: logit = 0.6668, post-softmax = 0.0393\n",
      "  expert 49: logit = -0.5274, post-softmax = 0.0119\n",
      "  expert 50: logit = -0.4105, post-softmax = 0.0134\n",
      "  expert 51: logit = -1.2156, post-softmax = 0.0060\n",
      "  expert 52: logit = -0.5433, post-softmax = 0.0117\n",
      "  expert 53: logit = -0.1100, post-softmax = 0.0181\n",
      "  expert 54: logit = -0.4601, post-softmax = 0.0127\n",
      "  expert 55: logit = 1.5564, post-softmax = 0.0957\n",
      "  expert 56: logit = -0.5183, post-softmax = 0.0120\n",
      "  expert 57: logit = -0.7806, post-softmax = 0.0092\n",
      "  expert 58: logit = -3.6998, post-softmax = 0.0005\n",
      "  expert 59: logit = -0.6291, post-softmax = 0.0108\n",
      "  expert 60: logit = -1.2488, post-softmax = 0.0058\n",
      "  expert 61: logit = -0.9646, post-softmax = 0.0077\n",
      "  expert 62: logit = -0.5123, post-softmax = 0.0121\n",
      "  expert 63: logit = -0.2056, post-softmax = 0.0164\n",
      "\n",
      "Token: ' note' (ID: 3877)\n",
      "  expert 0: logit = -1.5468, post-softmax = 0.0046\n",
      "  expert 1: logit = -0.5166, post-softmax = 0.0129\n",
      "  expert 2: logit = -0.2193, post-softmax = 0.0173\n",
      "  expert 3: logit = -0.8990, post-softmax = 0.0088\n",
      "  expert 4: logit = -0.7980, post-softmax = 0.0097\n",
      "  expert 5: logit = -1.4324, post-softmax = 0.0051\n",
      "  expert 6: logit = 0.9567, post-softmax = 0.0561\n",
      "  expert 7: logit = -0.9022, post-softmax = 0.0087\n",
      "  expert 8: logit = -0.9318, post-softmax = 0.0085\n",
      "  expert 9: logit = -1.1368, post-softmax = 0.0069\n",
      "  expert 10: logit = -0.4390, post-softmax = 0.0139\n",
      "  expert 11: logit = -1.7509, post-softmax = 0.0037\n",
      "  expert 12: logit = -0.6363, post-softmax = 0.0114\n",
      "  expert 13: logit = -0.0795, post-softmax = 0.0199\n",
      "  expert 14: logit = -0.1273, post-softmax = 0.0190\n",
      "  expert 15: logit = -0.2069, post-softmax = 0.0175\n",
      "  expert 16: logit = -0.2257, post-softmax = 0.0172\n",
      "  expert 17: logit = -1.1210, post-softmax = 0.0070\n",
      "  expert 18: logit = 0.2842, post-softmax = 0.0286\n",
      "  expert 19: logit = -0.2373, post-softmax = 0.0170\n",
      "  expert 20: logit = -0.3897, post-softmax = 0.0146\n",
      "  expert 21: logit = -2.0478, post-softmax = 0.0028\n",
      "  expert 22: logit = -2.8071, post-softmax = 0.0013\n",
      "  expert 23: logit = -0.4492, post-softmax = 0.0138\n",
      "  expert 24: logit = -1.1520, post-softmax = 0.0068\n",
      "  expert 25: logit = -0.6238, post-softmax = 0.0116\n",
      "  expert 26: logit = 0.1500, post-softmax = 0.0250\n",
      "  expert 27: logit = -0.2819, post-softmax = 0.0163\n",
      "  expert 28: logit = -2.0542, post-softmax = 0.0028\n",
      "  expert 29: logit = -0.3386, post-softmax = 0.0154\n",
      "  expert 30: logit = -0.8383, post-softmax = 0.0093\n",
      "  expert 31: logit = -0.6463, post-softmax = 0.0113\n",
      "  expert 32: logit = -0.4375, post-softmax = 0.0139\n",
      "  expert 33: logit = -0.7402, post-softmax = 0.0103\n",
      "  expert 34: logit = -2.5525, post-softmax = 0.0017\n",
      "  expert 35: logit = -0.7075, post-softmax = 0.0106\n",
      "  expert 36: logit = -0.2352, post-softmax = 0.0170\n",
      "  expert 37: logit = 1.9581, post-softmax = 0.1527\n",
      "  expert 38: logit = -0.3378, post-softmax = 0.0154\n",
      "  expert 39: logit = -1.0121, post-softmax = 0.0078\n",
      "  expert 40: logit = -0.3306, post-softmax = 0.0155\n",
      "  expert 41: logit = 0.8981, post-softmax = 0.0529\n",
      "  expert 42: logit = -0.4374, post-softmax = 0.0139\n",
      "  expert 43: logit = -0.9286, post-softmax = 0.0085\n",
      "  expert 44: logit = -0.3738, post-softmax = 0.0148\n",
      "  expert 45: logit = -1.0098, post-softmax = 0.0079\n",
      "  expert 46: logit = -0.8283, post-softmax = 0.0094\n",
      "  expert 47: logit = -0.3693, post-softmax = 0.0149\n",
      "  expert 48: logit = -0.3147, post-softmax = 0.0157\n",
      "  expert 49: logit = -0.4280, post-softmax = 0.0140\n",
      "  expert 50: logit = -0.5427, post-softmax = 0.0125\n",
      "  expert 51: logit = -0.0480, post-softmax = 0.0205\n",
      "  expert 52: logit = -0.6645, post-softmax = 0.0111\n",
      "  expert 53: logit = -0.3306, post-softmax = 0.0155\n",
      "  expert 54: logit = -0.6375, post-softmax = 0.0114\n",
      "  expert 55: logit = 0.3645, post-softmax = 0.0310\n",
      "  expert 56: logit = -1.0287, post-softmax = 0.0077\n",
      "  expert 57: logit = -2.2646, post-softmax = 0.0022\n",
      "  expert 58: logit = -0.8473, post-softmax = 0.0092\n",
      "  expert 59: logit = -0.6635, post-softmax = 0.0111\n",
      "  expert 60: logit = -0.8630, post-softmax = 0.0091\n",
      "  expert 61: logit = -0.3361, post-softmax = 0.0154\n",
      "  expert 62: logit = -0.3429, post-softmax = 0.0153\n",
      "  expert 63: logit = -1.2790, post-softmax = 0.0060\n",
      "\n",
      "Token: '?' (ID: 32)\n",
      "  expert 0: logit = -1.3771, post-softmax = 0.0064\n",
      "  expert 1: logit = -0.0656, post-softmax = 0.0238\n",
      "  expert 2: logit = -0.4418, post-softmax = 0.0164\n",
      "  expert 3: logit = -0.5673, post-softmax = 0.0144\n",
      "  expert 4: logit = -0.8018, post-softmax = 0.0114\n",
      "  expert 5: logit = -0.8257, post-softmax = 0.0111\n",
      "  expert 6: logit = 1.0003, post-softmax = 0.0692\n",
      "  expert 7: logit = -0.4214, post-softmax = 0.0167\n",
      "  expert 8: logit = -0.3757, post-softmax = 0.0175\n",
      "  expert 9: logit = -2.8604, post-softmax = 0.0015\n",
      "  expert 10: logit = -0.3799, post-softmax = 0.0174\n",
      "  expert 11: logit = -2.1651, post-softmax = 0.0029\n",
      "  expert 12: logit = -2.5906, post-softmax = 0.0019\n",
      "  expert 13: logit = -0.9678, post-softmax = 0.0097\n",
      "  expert 14: logit = -0.6484, post-softmax = 0.0133\n",
      "  expert 15: logit = -0.3911, post-softmax = 0.0172\n",
      "  expert 16: logit = -0.4893, post-softmax = 0.0156\n",
      "  expert 17: logit = -0.3247, post-softmax = 0.0184\n",
      "  expert 18: logit = -1.2284, post-softmax = 0.0074\n",
      "  expert 19: logit = 0.0367, post-softmax = 0.0264\n",
      "  expert 20: logit = -1.1122, post-softmax = 0.0084\n",
      "  expert 21: logit = -1.5471, post-softmax = 0.0054\n",
      "  expert 22: logit = -0.2925, post-softmax = 0.0190\n",
      "  expert 23: logit = -0.0895, post-softmax = 0.0233\n",
      "  expert 24: logit = 0.5171, post-softmax = 0.0427\n",
      "  expert 25: logit = -0.3080, post-softmax = 0.0187\n",
      "  expert 26: logit = -0.5219, post-softmax = 0.0151\n",
      "  expert 27: logit = -0.1985, post-softmax = 0.0209\n",
      "  expert 28: logit = -1.8464, post-softmax = 0.0040\n",
      "  expert 29: logit = -0.3019, post-softmax = 0.0188\n",
      "  expert 30: logit = -0.5176, post-softmax = 0.0152\n",
      "  expert 31: logit = -0.6026, post-softmax = 0.0139\n",
      "  expert 32: logit = -0.6665, post-softmax = 0.0131\n",
      "  expert 33: logit = -1.0165, post-softmax = 0.0092\n",
      "  expert 34: logit = -1.9692, post-softmax = 0.0036\n",
      "  expert 35: logit = -0.9008, post-softmax = 0.0103\n",
      "  expert 36: logit = -0.2672, post-softmax = 0.0195\n",
      "  expert 37: logit = -0.5757, post-softmax = 0.0143\n",
      "  expert 38: logit = -0.8332, post-softmax = 0.0111\n",
      "  expert 39: logit = -0.3039, post-softmax = 0.0188\n",
      "  expert 40: logit = -0.6419, post-softmax = 0.0134\n",
      "  expert 41: logit = -2.6840, post-softmax = 0.0017\n",
      "  expert 42: logit = -0.8788, post-softmax = 0.0106\n",
      "  expert 43: logit = -0.7318, post-softmax = 0.0122\n",
      "  expert 44: logit = -1.3493, post-softmax = 0.0066\n",
      "  expert 45: logit = -0.3829, post-softmax = 0.0173\n",
      "  expert 46: logit = -1.4178, post-softmax = 0.0062\n",
      "  expert 47: logit = -0.1528, post-softmax = 0.0218\n",
      "  expert 48: logit = -0.6170, post-softmax = 0.0137\n",
      "  expert 49: logit = 0.3568, post-softmax = 0.0364\n",
      "  expert 50: logit = -0.6339, post-softmax = 0.0135\n",
      "  expert 51: logit = -1.0345, post-softmax = 0.0090\n",
      "  expert 52: logit = -0.4416, post-softmax = 0.0164\n",
      "  expert 53: logit = -0.3886, post-softmax = 0.0173\n",
      "  expert 54: logit = -0.5599, post-softmax = 0.0145\n",
      "  expert 55: logit = -1.3782, post-softmax = 0.0064\n",
      "  expert 56: logit = -0.5833, post-softmax = 0.0142\n",
      "  expert 57: logit = -1.1210, post-softmax = 0.0083\n",
      "  expert 58: logit = -3.0203, post-softmax = 0.0012\n",
      "  expert 59: logit = 0.2214, post-softmax = 0.0317\n",
      "  expert 60: logit = 0.3568, post-softmax = 0.0363\n",
      "  expert 61: logit = -0.2190, post-softmax = 0.0204\n",
      "  expert 62: logit = -0.4189, post-softmax = 0.0167\n",
      "  expert 63: logit = 0.1737, post-softmax = 0.0303\n",
      "\n",
      "\n",
      "Top 8 experts for each token:\n",
      "Token: 'What' (ID: 1276)\n",
      "  1. expert 57: probability = 0.1213\n",
      "  2. expert 61: probability = 0.1017\n",
      "  3. expert 14: probability = 0.0938\n",
      "  4. expert 62: probability = 0.0752\n",
      "  5. expert 7: probability = 0.0255\n",
      "  6. expert 43: probability = 0.0254\n",
      "  7. expert 39: probability = 0.0232\n",
      "  8. expert 30: probability = 0.0232\n",
      "\n",
      "Token: ''s' (ID: 434)\n",
      "  1. expert 14: probability = 0.1203\n",
      "  2. expert 62: probability = 0.0857\n",
      "  3. expert 6: probability = 0.0679\n",
      "  4. expert 8: probability = 0.0627\n",
      "  5. expert 61: probability = 0.0466\n",
      "  6. expert 39: probability = 0.0361\n",
      "  7. expert 52: probability = 0.0330\n",
      "  8. expert 63: probability = 0.0324\n",
      "\n",
      "Token: ' the' (ID: 253)\n",
      "  1. expert 56: probability = 0.1265\n",
      "  2. expert 6: probability = 0.0843\n",
      "  3. expert 38: probability = 0.0558\n",
      "  4. expert 48: probability = 0.0497\n",
      "  5. expert 10: probability = 0.0444\n",
      "  6. expert 23: probability = 0.0307\n",
      "  7. expert 55: probability = 0.0261\n",
      "  8. expert 63: probability = 0.0233\n",
      "\n",
      "Token: ' musical' (ID: 12256)\n",
      "  1. expert 6: probability = 0.0919\n",
      "  2. expert 18: probability = 0.0860\n",
      "  3. expert 5: probability = 0.0837\n",
      "  4. expert 41: probability = 0.0494\n",
      "  5. expert 38: probability = 0.0399\n",
      "  6. expert 23: probability = 0.0224\n",
      "  7. expert 48: probability = 0.0224\n",
      "  8. expert 56: probability = 0.0218\n",
      "\n",
      "Token: ' pitch' (ID: 11288)\n",
      "  1. expert 44: probability = 0.1229\n",
      "  2. expert 50: probability = 0.0686\n",
      "  3. expert 60: probability = 0.0643\n",
      "  4. expert 41: probability = 0.0577\n",
      "  5. expert 6: probability = 0.0486\n",
      "  6. expert 30: probability = 0.0294\n",
      "  7. expert 49: probability = 0.0279\n",
      "  8. expert 12: probability = 0.0270\n",
      "\n",
      "Token: ' of' (ID: 273)\n",
      "  1. expert 46: probability = 0.2586\n",
      "  2. expert 6: probability = 0.0520\n",
      "  3. expert 8: probability = 0.0465\n",
      "  4. expert 60: probability = 0.0358\n",
      "  5. expert 42: probability = 0.0299\n",
      "  6. expert 24: probability = 0.0232\n",
      "  7. expert 3: probability = 0.0228\n",
      "  8. expert 55: probability = 0.0216\n",
      "\n",
      "Token: ' that' (ID: 326)\n",
      "  1. expert 55: probability = 0.0957\n",
      "  2. expert 18: probability = 0.0948\n",
      "  3. expert 6: probability = 0.0685\n",
      "  4. expert 26: probability = 0.0449\n",
      "  5. expert 48: probability = 0.0393\n",
      "  6. expert 13: probability = 0.0360\n",
      "  7. expert 37: probability = 0.0203\n",
      "  8. expert 8: probability = 0.0201\n",
      "\n",
      "Token: ' note' (ID: 3877)\n",
      "  1. expert 37: probability = 0.1527\n",
      "  2. expert 6: probability = 0.0561\n",
      "  3. expert 41: probability = 0.0529\n",
      "  4. expert 55: probability = 0.0310\n",
      "  5. expert 18: probability = 0.0286\n",
      "  6. expert 26: probability = 0.0250\n",
      "  7. expert 51: probability = 0.0205\n",
      "  8. expert 13: probability = 0.0199\n",
      "\n",
      "Token: '?' (ID: 32)\n",
      "  1. expert 6: probability = 0.0692\n",
      "  2. expert 24: probability = 0.0427\n",
      "  3. expert 49: probability = 0.0364\n",
      "  4. expert 60: probability = 0.0363\n",
      "  5. expert 59: probability = 0.0317\n",
      "  6. expert 63: probability = 0.0303\n",
      "  7. expert 19: probability = 0.0264\n",
      "  8. expert 1: probability = 0.0238\n",
      "\n",
      "Analysis results saved to expert_routing_analysis_2.json\n"
     ]
    }
   ],
   "source": [
    "input_text_2 = \"What's the musical pitch of that note?\"\n",
    "last_layer_router_logits_2, analysis_results_2 = analyze_expert_routing(input_text_2, model, tokenizer, layer_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_token_kl_divergence(json_file1, json_file2, target_token):\n",
    "    # Load JSON data from files\n",
    "    with open(json_file1, 'r') as f1, open(json_file2, 'r') as f2:\n",
    "        data1 = json.load(f1)\n",
    "        data2 = json.load(f2)\n",
    "    \n",
    "    # Find the target token in both inputs\n",
    "    token1 = next((t for t in data1['tokens'] if t['token'] == target_token or t['id'] == target_token), None)\n",
    "    token2 = next((t for t in data2['tokens'] if t['token'] == target_token or t['id'] == target_token), None)\n",
    "    \n",
    "    if not token1 or not token2:\n",
    "        raise ValueError(f\"Token '{target_token}' not found in one or both inputs\")\n",
    "    \n",
    "    # Get router probabilities for the target token\n",
    "    probs1 = np.array(token1['router_probability'])\n",
    "    probs2 = np.array(token2['router_probability'])\n",
    "    \n",
    "    # Ensure probabilities sum to 1\n",
    "    probs1 = probs1 / np.sum(probs1)\n",
    "    probs2 = probs2 / np.sum(probs2)\n",
    "    \n",
    "    # Calculate KL divergence for each expert\n",
    "    kl_divergences = kl_div(probs1, probs2)\n",
    "    \n",
    "    # Create a dictionary of expert-wise KL divergences\n",
    "    expert_kl = {f\"Expert_{i}\": kl for i, kl in enumerate(kl_divergences)}\n",
    "    \n",
    "    # Calculate the total KL divergence\n",
    "    total_kl = np.sum(kl_divergences)\n",
    "    \n",
    "    result = {\n",
    "        \"token\": target_token,\n",
    "        \"expert_kl_divergences\": expert_kl,\n",
    "        \"total_kl_divergence\": total_kl\n",
    "    }\n",
    "    \n",
    "    # Save the result as a JSON file\n",
    "    base_filename = \"kl_divergence_analysis\"\n",
    "    counter = 1\n",
    "    filename = f\"{base_filename}_{counter}.json\"\n",
    "    while os.path.exists(filename):\n",
    "        counter += 1\n",
    "        filename = f\"{base_filename}_{counter}.json\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    print(f\"KL divergence analysis results saved to {filename}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence analysis results saved to kl_divergence_analysis_1.json\n",
      "{\n",
      "  \"token\": 11288,\n",
      "  \"expert_kl_divergences\": {\n",
      "    \"Expert_0\": 0.009453152321398161,\n",
      "    \"Expert_1\": 0.0010285437104599988,\n",
      "    \"Expert_2\": 0.019584900520401495,\n",
      "    \"Expert_3\": 4.717867089935879e-06,\n",
      "    \"Expert_4\": 0.0011487151426666509,\n",
      "    \"Expert_5\": 0.001311021631701912,\n",
      "    \"Expert_6\": 0.00022197336573581824,\n",
      "    \"Expert_7\": 0.039499575999606906,\n",
      "    \"Expert_8\": 0.0001660485313056282,\n",
      "    \"Expert_9\": 8.936434448928235e-05,\n",
      "    \"Expert_10\": 0.0015351983129790721,\n",
      "    \"Expert_11\": 5.294601192123516e-05,\n",
      "    \"Expert_12\": 5.617166773176294e-06,\n",
      "    \"Expert_13\": 0.0025187000490325545,\n",
      "    \"Expert_14\": 0.0005910418768493853,\n",
      "    \"Expert_15\": 0.017322743208425296,\n",
      "    \"Expert_16\": 0.0025415154277968455,\n",
      "    \"Expert_17\": 0.010948062989407141,\n",
      "    \"Expert_18\": 0.024757676308353425,\n",
      "    \"Expert_19\": 0.007494485665606062,\n",
      "    \"Expert_20\": 0.00031472358405389665,\n",
      "    \"Expert_21\": 0.011316649822444161,\n",
      "    \"Expert_22\": 0.07835270653967723,\n",
      "    \"Expert_23\": 0.09321935747960607,\n",
      "    \"Expert_24\": 0.015701430149906687,\n",
      "    \"Expert_25\": 0.020602156639143665,\n",
      "    \"Expert_26\": 0.006064220730911743,\n",
      "    \"Expert_27\": 0.016441127176231966,\n",
      "    \"Expert_28\": 0.0048595578386885635,\n",
      "    \"Expert_29\": 0.001402839609569292,\n",
      "    \"Expert_30\": 0.0035555194108879157,\n",
      "    \"Expert_31\": 0.002382737880591814,\n",
      "    \"Expert_32\": 0.013335462617916394,\n",
      "    \"Expert_33\": 0.00034754765027922253,\n",
      "    \"Expert_34\": 0.009891338857553004,\n",
      "    \"Expert_35\": 5.117969614477219e-05,\n",
      "    \"Expert_36\": 0.000620581869318463,\n",
      "    \"Expert_37\": 0.0013117257924501512,\n",
      "    \"Expert_38\": 0.00026157149098147287,\n",
      "    \"Expert_39\": 0.0020486646811386997,\n",
      "    \"Expert_40\": 0.0006558526270959118,\n",
      "    \"Expert_41\": 0.00032910905912099213,\n",
      "    \"Expert_42\": 4.5087010477548595e-07,\n",
      "    \"Expert_43\": 0.0030803803117042863,\n",
      "    \"Expert_44\": 0.03379181081642356,\n",
      "    \"Expert_45\": 0.0075431085569666705,\n",
      "    \"Expert_46\": 0.0002929948593938213,\n",
      "    \"Expert_47\": 0.0017111498387582594,\n",
      "    \"Expert_48\": 0.001184903696533705,\n",
      "    \"Expert_49\": 0.016686672542135173,\n",
      "    \"Expert_50\": 0.00037879401215020793,\n",
      "    \"Expert_51\": 0.027247515854777464,\n",
      "    \"Expert_52\": 3.541442891226605e-05,\n",
      "    \"Expert_53\": 0.00038343677089589315,\n",
      "    \"Expert_54\": 0.041663726808104755,\n",
      "    \"Expert_55\": 2.180783130819111e-05,\n",
      "    \"Expert_56\": 0.10416141030729134,\n",
      "    \"Expert_57\": 0.0007433434999750991,\n",
      "    \"Expert_58\": 0.0005747280910692043,\n",
      "    \"Expert_59\": 0.01383466885451879,\n",
      "    \"Expert_60\": 0.0009808327146492717,\n",
      "    \"Expert_61\": 0.07657686840049456,\n",
      "    \"Expert_62\": 0.008034431762744074,\n",
      "    \"Expert_63\": 0.0013282547259072693\n",
      "  },\n",
      "  \"total_kl_divergence\": 0.7635987672105308\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_file1 = \"expert_routing_analysis_1.json\"\n",
    "json_file2 = \"expert_routing_analysis_2.json\"\n",
    "target_token =  11288 # use token id\n",
    "\n",
    "result = calculate_token_kl_divergence(json_file1, json_file2, target_token)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "title": {
           "text": "Probability"
          }
         },
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "hovertemplate": "Token: %{y}<br>Expert: %{x}<br>Probability: %{z:.4f}<extra></extra>",
         "type": "heatmap",
         "x": [
          "Expert 0",
          "Expert 1",
          "Expert 2",
          "Expert 3",
          "Expert 4",
          "Expert 5",
          "Expert 6",
          "Expert 7",
          "Expert 8",
          "Expert 9",
          "Expert 10",
          "Expert 11",
          "Expert 12",
          "Expert 13",
          "Expert 14",
          "Expert 15",
          "Expert 16",
          "Expert 17",
          "Expert 18",
          "Expert 19",
          "Expert 20",
          "Expert 21",
          "Expert 22",
          "Expert 23",
          "Expert 24",
          "Expert 25",
          "Expert 26",
          "Expert 27",
          "Expert 28",
          "Expert 29",
          "Expert 30",
          "Expert 31",
          "Expert 32",
          "Expert 33",
          "Expert 34",
          "Expert 35",
          "Expert 36",
          "Expert 37",
          "Expert 38",
          "Expert 39",
          "Expert 40",
          "Expert 41",
          "Expert 42",
          "Expert 43",
          "Expert 44",
          "Expert 45",
          "Expert 46",
          "Expert 47",
          "Expert 48",
          "Expert 49",
          "Expert 50",
          "Expert 51",
          "Expert 52",
          "Expert 53",
          "Expert 54",
          "Expert 55",
          "Expert 56",
          "Expert 57",
          "Expert 58",
          "Expert 59",
          "Expert 60",
          "Expert 61",
          "Expert 62",
          "Expert 63"
         ],
         "xaxis": "x",
         "y": [
          "How",
          " will",
          " you",
          " pitch",
          " your",
          " idea",
          " to",
          " the",
          " investors",
          "?"
         ],
         "yaxis": "y",
         "z": [
          [
           0.0007834091666154563,
           0.01453004963696003,
           0.014692817814648151,
           0.012649590149521828,
           0.020162563771009445,
           0.010809337720274925,
           0.01821129210293293,
           0.03666841611266136,
           0.0041639357805252075,
           0.006957123521715403,
           0.010424365289509296,
           0.015506052412092686,
           0.00018936817650683224,
           0.0011202640598639846,
           0.10899905115365982,
           0.009576713666319847,
           0.012736928649246693,
           0.028758501634001732,
           0.036757197231054306,
           0.009928720071911812,
           0.0015400135889649391,
           0.0034617616329342127,
           0.00582633214071393,
           0.016461992636322975,
           0.010014916770160198,
           0.01569216698408127,
           0.007868917658925056,
           0.017152171581983566,
           0.0019375606207177043,
           0.009262661449611187,
           0.018900129944086075,
           0.006117672193795443,
           0.01249238196760416,
           0.011507430113852024,
           0.001880145282484591,
           0.026540696620941162,
           0.009295839816331863,
           0.014093349687755108,
           0.00852531660348177,
           0.028000788763165474,
           0.005065044853836298,
           0.00346131413243711,
           0.009946945123374462,
           0.031096680089831352,
           0.014978972263634205,
           0.021678417921066284,
           0.00987936370074749,
           0.011685304343700409,
           0.0061912983655929565,
           0.008100015111267567,
           0.01110855396836996,
           0.0036811570171266794,
           0.009564945474267006,
           0.0010812794789671898,
           0.015086903236806393,
           0.016938747838139534,
           0.006303706672042608,
           0.11518604308366776,
           0.000864877481944859,
           0.008761950768530369,
           0.013365554623305798,
           0.05069127678871155,
           0.013750124722719193,
           0.011333569884300232
          ],
          [
           0.0017232204554602504,
           0.008971164003014565,
           0.012308740057051182,
           0.008424495346844196,
           0.010500358417630196,
           0.014925911091268063,
           0.07786467671394348,
           0.020463811233639717,
           0.029056144878268242,
           0.005645247176289558,
           0.009370698593556881,
           0.006738254334777594,
           0.00021397345699369907,
           0.006078239530324936,
           0.12586237490177155,
           0.01968824490904808,
           0.013821807689964771,
           0.02017056941986084,
           0.07566206902265549,
           0.0007724507013335824,
           0.003508456749841571,
           0.008654570206999779,
           0.001979751279577613,
           0.02674729749560356,
           0.009084329940378666,
           0.019654810428619385,
           0.010899373330175877,
           0.01247924193739891,
           0.004336854908615351,
           0.01006774790585041,
           0.012292428873479366,
           0.005264316685497761,
           0.016001952812075615,
           0.00846418272703886,
           0.0012029454810544848,
           0.01469383668154478,
           0.009672951884567738,
           0.010241195559501648,
           0.00962875783443451,
           0.05143329128623009,
           0.010665829293429852,
           0.0026648431085050106,
           0.008724604733288288,
           0.015239634551107883,
           0.009006880223751068,
           0.010754188522696495,
           0.006120922975242138,
           0.011617369018495083,
           0.004774091765284538,
           0.010336332954466343,
           0.014167983084917068,
           0.0022883531637489796,
           0.027341675013303757,
           0.013452704064548016,
           0.025746073573827744,
           0.0169073473662138,
           0.004925951361656189,
           0.013034733012318611,
           0.0007932417211122811,
           0.0077238236553967,
           0.005889534950256348,
           0.01862391270697117,
           0.015046559274196625,
           0.029582692310214043
          ],
          [
           0.0012868529884144664,
           0.008658090606331825,
           0.014264875091612339,
           0.014340035617351532,
           0.01152243372052908,
           0.013186812400817871,
           0.09006857126951218,
           0.033568162471055984,
           0.025257010012865067,
           0.005255826748907566,
           0.020286865532398224,
           0.007654677610844374,
           0.00026274967240169644,
           0.004414530470967293,
           0.013069607317447662,
           0.013244402594864368,
           0.008993398398160934,
           0.01074010506272316,
           0.019541911780834198,
           0.0029062265530228615,
           0.0026431705337017775,
           0.006952094845473766,
           0.005166276823729277,
           0.031508125364780426,
           0.009293043985962868,
           0.01642887480556965,
           0.01468778308480978,
           0.024772774428129196,
           0.0031406430061906576,
           0.014208970591425896,
           0.0158526711165905,
           0.010989757254719734,
           0.017296597361564636,
           0.005331047810614109,
           0.001568407635204494,
           0.05728549882769585,
           0.008238268084824085,
           0.017632797360420227,
           0.03195454552769661,
           0.030068814754486084,
           0.010993601754307747,
           0.0022251519840210676,
           0.013640688732266426,
           0.030882779508829117,
           0.012267077341675758,
           0.009670768864452839,
           0.007926748134195805,
           0.016788603737950325,
           0.05062907189130783,
           0.015351377427577972,
           0.014212035574018955,
           0.0063780820928514,
           0.014763982966542244,
           0.01446898840367794,
           0.015534641221165657,
           0.019134309142827988,
           0.010144111700356007,
           0.019373895600438118,
           0.000891978619620204,
           0.010021918453276157,
           0.0057439301162958145,
           0.010967869311571121,
           0.014410488307476044,
           0.03003448061645031
          ],
          [
           0.005000948440283537,
           0.09519344568252563,
           0.017236927524209023,
           0.014573377557098866,
           0.013877861201763153,
           0.006196351256221533,
           0.0526343397796154,
           0.004136386327445507,
           0.024412769824266434,
           0.013713665306568146,
           0.014122395776212215,
           0.006891545373946428,
           0.005353199318051338,
           0.008773650042712688,
           0.004663284868001938,
           0.013000591658055782,
           0.013639762997627258,
           0.011494666337966919,
           0.013872208073735237,
           0.004937176126986742,
           0.018686436116695404,
           0.007803988177329302,
           0.0015347872395068407,
           0.025872286409139633,
           0.008671390824019909,
           0.012550553306937218,
           0.01616203784942627,
           0.01238272339105606,
           0.003849348984658718,
           0.014912353828549385,
           0.01540245208889246,
           0.01926489546895027,
           0.011581704951822758,
           0.01361140888184309,
           0.0016999624203890562,
           0.11196034401655197,
           0.01222188025712967,
           0.01470222882926464,
           0.010357476770877838,
           0.0030719502829015255,
           0.012718653306365013,
           0.012621023692190647,
           0.010668470524251461,
           0.0027951980009675026,
           0.011162937618792057,
           0.015032485127449036,
           0.007127356715500355,
           0.014206231571733952,
           0.020732441917061806,
           0.023777473717927933,
           0.010247185826301575,
           0.01028447411954403,
           0.013384466990828514,
           0.010660448111593723,
           0.012847834266722202,
           0.012059062719345093,
           0.019921304658055305,
           0.006872772239148617,
           0.02499799057841301,
           0.016156639903783798,
           0.00679988507181406,
           0.019823238253593445,
           0.013491671532392502,
           0.011585934087634087
          ],
          [
           0.0011752675054594874,
           0.02171005681157112,
           0.01541410107165575,
           0.0289258174598217,
           0.009744996204972267,
           0.012091470882296562,
           0.06639394164085388,
           0.02772168442606926,
           0.010450106114149094,
           0.005873170215636492,
           0.01697746105492115,
           0.01477007381618023,
           0.00022563057427760214,
           0.003080418799072504,
           0.004644603934139013,
           0.013708856888115406,
           0.0063468292355537415,
           0.01288522221148014,
           0.0060795447789132595,
           0.005971730221062899,
           0.0024939123541116714,
           0.005645059049129486,
           0.022970866411924362,
           0.020903315395116806,
           0.04654340445995331,
           0.013534939847886562,
           0.004007894080132246,
           0.02657550759613514,
           0.002415155526250601,
           0.01461250800639391,
           0.018626835197210312,
           0.0057584005407989025,
           0.013735249638557434,
           0.0039432719349861145,
           0.001588116167113185,
           0.008281368762254715,
           0.007475303020328283,
           0.01191483810544014,
           0.03422349691390991,
           0.0167581494897604,
           0.011249092407524586,
           0.0025345131289213896,
           0.03466977924108505,
           0.02102554589509964,
           0.0201732087880373,
           0.011883343569934368,
           0.02989671565592289,
           0.0179195087403059,
           0.011313690803945065,
           0.015615930780768394,
           0.017430327832698822,
           0.0020125270821154118,
           0.013360342010855675,
           0.013868224807083607,
           0.018037091940641403,
           0.03585496544837952,
           0.011903413571417332,
           0.03846056014299393,
           0.0019460994517430663,
           0.01928548701107502,
           0.048900723457336426,
           0.006906276103109121,
           0.00916730985045433,
           0.020386818796396255
          ],
          [
           0.001528882305137813,
           0.09151008725166321,
           0.016763003543019295,
           0.0109486673027277,
           0.011398211121559143,
           0.044877875596284866,
           0.051020048558712006,
           0.008961205370724201,
           0.020606383681297302,
           0.01435842551290989,
           0.009602731093764305,
           0.008546761237084866,
           0.0009540313039906323,
           0.010937287472188473,
           0.010658666491508484,
           0.013684219680726528,
           0.010796288028359413,
           0.007776284124702215,
           0.01452720444649458,
           0.007596066687256098,
           0.002677277894690633,
           0.005857552867382765,
           0.004218415357172489,
           0.02210640162229538,
           0.01530884113162756,
           0.011421941220760345,
           0.012461107224225998,
           0.011289565823972225,
           0.00192896684166044,
           0.01323260460048914,
           0.012854449450969696,
           0.016925517469644547,
           0.016725149005651474,
           0.006276931148022413,
           0.0008541570277884603,
           0.008801310323178768,
           0.011798976920545101,
           0.006086199544370174,
           0.014555386267602444,
           0.00942936073988676,
           0.011272809468209743,
           0.005131300073117018,
           0.010448547080159187,
           0.012650782242417336,
           0.011978969909250736,
           0.017812954261898994,
           0.0066348109394311905,
           0.01269102655351162,
           0.01772153377532959,
           0.012299459427595139,
           0.010028035379946232,
           0.0031335570383816957,
           0.013296754099428654,
           0.01612182706594467,
           0.01898992247879505,
           0.01866040751338005,
           0.0726834163069725,
           0.008051151409745216,
           0.01499911304563284,
           0.01359102688729763,
           0.01940070651471615,
           0.0794035866856575,
           0.0099919643253088,
           0.011143846437335014
          ],
          [
           0.002545368392020464,
           0.06165728718042374,
           0.014730839058756828,
           0.019910847768187523,
           0.022158829495310783,
           0.03645699843764305,
           0.0637902244925499,
           0.012122448533773422,
           0.10168477892875671,
           0.003103099297732115,
           0.009409633465111256,
           0.012616428546607494,
           0.00036840568645857275,
           0.009262102656066418,
           0.009708467870950699,
           0.015240337699651718,
           0.005586537998169661,
           0.008634274825453758,
           0.012744447216391563,
           0.006650396157056093,
           0.0028582715895026922,
           0.008260353468358517,
           0.04714469984173775,
           0.0228414386510849,
           0.016398273408412933,
           0.008593348786234856,
           0.013221178203821182,
           0.011160480789840221,
           0.0037832704838365316,
           0.012936907820403576,
           0.010161410085856915,
           0.01430888008326292,
           0.01603173278272152,
           0.004969913978129625,
           0.002319966210052371,
           0.011836545541882515,
           0.01155870035290718,
           0.012871714308857918,
           0.011548582464456558,
           0.02220791019499302,
           0.011731263250112534,
           0.0018886278849095106,
           0.010756740346550941,
           0.015064227394759655,
           0.008101298473775387,
           0.014365820214152336,
           0.0074753109365701675,
           0.011288362555205822,
           0.012321468442678452,
           0.03190358728170395,
           0.010114495642483234,
           0.0026318607851862907,
           0.015205533243715763,
           0.018688581883907318,
           0.015482327900826931,
           0.005845488980412483,
           0.024448631331324577,
           0.015608614310622215,
           0.0018812654307112098,
           0.010493800975382328,
           0.015031023882329464,
           0.016248898580670357,
           0.015510954894125462,
           0.014516497030854225
          ],
          [
           0.00461296271532774,
           0.011868877336382866,
           0.018660636618733406,
           0.0259023979306221,
           0.01631125621497631,
           0.015644142404198647,
           0.08250314742326736,
           0.01570478454232216,
           0.0190691277384758,
           0.0039084432646632195,
           0.012339066714048386,
           0.014097367413341999,
           0.0008923170389607549,
           0.009366575628519058,
           0.010334884747862816,
           0.016234060749411583,
           0.00540970591828227,
           0.00810550432652235,
           0.019867852330207825,
           0.008651642128825188,
           0.0036446168087422848,
           0.01211695559322834,
           0.006353120319545269,
           0.01919539086520672,
           0.006942469626665115,
           0.014183145016431808,
           0.028322186321020126,
           0.018078060820698738,
           0.005483430810272694,
           0.015859635546803474,
           0.010446300730109215,
           0.010518637485802174,
           0.017751941457390785,
           0.00790257565677166,
           0.005173533223569393,
           0.08233340829610825,
           0.011710167862474918,
           0.013441546820104122,
           0.09701406955718994,
           0.016562361270189285,
           0.009756747633218765,
           0.0021941515151411295,
           0.009243825450539589,
           0.016132960096001625,
           0.010807616636157036,
           0.014654087834060192,
           0.006766622420400381,
           0.017016122117638588,
           0.012198248878121376,
           0.016998935490846634,
           0.010867349803447723,
           0.0040308949537575245,
           0.016340389847755432,
           0.016921203583478928,
           0.01615840569138527,
           0.0103951096534729,
           0.011938750743865967,
           0.011372778564691544,
           0.002638742094859481,
           0.00863949116319418,
           0.006030978634953499,
           0.009372186847031116,
           0.016546580940485,
           0.02045941725373268
          ],
          [
           0.006520855240523815,
           0.010547919198870659,
           0.017205113545060158,
           0.008373781107366085,
           0.012436321005225182,
           0.08556422591209412,
           0.07145336270332336,
           0.0010723742889240384,
           0.017472533509135246,
           0.018862426280975342,
           0.011415599845349789,
           0.005945590324699879,
           0.0062859742902219296,
           0.015315942466259003,
           0.011349680833518505,
           0.014987648464739323,
           0.013449564576148987,
           0.007549540139734745,
           0.07054507732391357,
           0.006611069664359093,
           0.0032486070413142443,
           0.006370984949171543,
           0.0022763984743505716,
           0.01907419227063656,
           0.0059851231053471565,
           0.011149365454912186,
           0.02275669015944004,
           0.01176532357931137,
           0.00583050400018692,
           0.011141737923026085,
           0.008409688249230385,
           0.015870437026023865,
           0.01594957336783409,
           0.0071586742997169495,
           0.00242837006226182,
           0.021220935508608818,
           0.011662781238555908,
           0.011660365387797356,
           0.04932777211070061,
           0.006956010591238737,
           0.010343644767999649,
           0.05835605412721634,
           0.011255485936999321,
           0.0024614520370960236,
           0.01023072563111782,
           0.010677957907319069,
           0.0064617423340678215,
           0.011568455956876278,
           0.014884226024150848,
           0.01441275142133236,
           0.009152800776064396,
           0.0068591139279305935,
           0.011294509284198284,
           0.013257630169391632,
           0.01592840440571308,
           0.014760690741240978,
           0.01608884520828724,
           0.0032087424769997597,
           0.03549801558256149,
           0.009183923713862896,
           0.008574812673032284,
           0.017016684636473656,
           0.017486996948719025,
           0.007828203029930592
          ],
          [
           0.0026025925762951374,
           0.011691926047205925,
           0.01943550631403923,
           0.014701603911817074,
           0.00903258752077818,
           0.032237708568573,
           0.08244404941797256,
           0.02081385627388954,
           0.01473830733448267,
           0.002938441699370742,
           0.05671420693397522,
           0.005227676127105951,
           0.00083277648082003,
           0.007417169865220785,
           0.014095041900873184,
           0.017541896551847458,
           0.015296265482902527,
           0.017031218856573105,
           0.01410622987896204,
           0.026497069746255875,
           0.0024714316241443157,
           0.0058035291731357574,
           0.01305344607681036,
           0.031149286776781082,
           0.009896878153085709,
           0.015075965784490108,
           0.014818580821156502,
           0.01582622528076172,
           0.003435583086684346,
           0.011938553303480148,
           0.027815259993076324,
           0.014417245052754879,
           0.014211412519216537,
           0.007364514749497175,
           0.0023922238033264875,
           0.013597875833511353,
           0.016079120337963104,
           0.019089438021183014,
           0.018843846395611763,
           0.02245575562119484,
           0.01233671698719263,
           0.0023840453941375017,
           0.016048144549131393,
           0.01781897246837616,
           0.019229697063565254,
           0.014332559891045094,
           0.005899995099753141,
           0.015223762020468712,
           0.011829973198473454,
           0.012004329822957516,
           0.009877027943730354,
           0.008647237904369831,
           0.012582086026668549,
           0.01921926625072956,
           0.017111459746956825,
           0.011262251995503902,
           0.03022419475018978,
           0.010189839638769627,
           0.0024642380885779858,
           0.012261820957064629,
           0.01189119927585125,
           0.015907350927591324,
           0.018497517332434654,
           0.027624037116765976
          ]
         ]
        },
        {
         "colorbar": {
          "title": {
           "text": "Probability"
          }
         },
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "hovertemplate": "Token: %{y}<br>Expert: %{x}<br>Probability: %{z:.4f}<extra></extra>",
         "type": "heatmap",
         "x": [
          "Expert 0",
          "Expert 1",
          "Expert 2",
          "Expert 3",
          "Expert 4",
          "Expert 5",
          "Expert 6",
          "Expert 7",
          "Expert 8",
          "Expert 9",
          "Expert 10",
          "Expert 11",
          "Expert 12",
          "Expert 13",
          "Expert 14",
          "Expert 15",
          "Expert 16",
          "Expert 17",
          "Expert 18",
          "Expert 19",
          "Expert 20",
          "Expert 21",
          "Expert 22",
          "Expert 23",
          "Expert 24",
          "Expert 25",
          "Expert 26",
          "Expert 27",
          "Expert 28",
          "Expert 29",
          "Expert 30",
          "Expert 31",
          "Expert 32",
          "Expert 33",
          "Expert 34",
          "Expert 35",
          "Expert 36",
          "Expert 37",
          "Expert 38",
          "Expert 39",
          "Expert 40",
          "Expert 41",
          "Expert 42",
          "Expert 43",
          "Expert 44",
          "Expert 45",
          "Expert 46",
          "Expert 47",
          "Expert 48",
          "Expert 49",
          "Expert 50",
          "Expert 51",
          "Expert 52",
          "Expert 53",
          "Expert 54",
          "Expert 55",
          "Expert 56",
          "Expert 57",
          "Expert 58",
          "Expert 59",
          "Expert 60",
          "Expert 61",
          "Expert 62",
          "Expert 63"
         ],
         "xaxis": "x2",
         "y": [
          "What",
          "'s",
          " the",
          " musical",
          " pitch",
          " of",
          " that",
          " note",
          "?"
         ],
         "yaxis": "y2",
         "z": [
          [
           0.0007150631863623857,
           0.014498774893581867,
           0.0143189188092947,
           0.009278750978410244,
           0.017114834859967232,
           0.007646370679140091,
           0.012734444811940193,
           0.025528963655233383,
           0.010768857784569263,
           0.004774519708007574,
           0.00769026018679142,
           0.00664216373115778,
           0.0006192145519889891,
           0.0012796428054571152,
           0.09381332993507385,
           0.006694125011563301,
           0.009542389772832394,
           0.02006347104907036,
           0.0061677261255681515,
           0.008577563799917698,
           0.0018202080391347408,
           0.002131069777533412,
           0.011644050478935242,
           0.013997133821249008,
           0.014135211706161499,
           0.011888917535543442,
           0.008825700730085373,
           0.015709202736616135,
           0.0010072886943817139,
           0.010061569511890411,
           0.023212110623717308,
           0.007536520250141621,
           0.007914633490145206,
           0.00585193419829011,
           0.0014362044166773558,
           0.013977261260151863,
           0.010163550265133381,
           0.011770120821893215,
           0.008734005503356457,
           0.023235002532601357,
           0.003684678114950657,
           0.0026714839041233063,
           0.014630804769694805,
           0.02538663148880005,
           0.008799179457128048,
           0.01688050851225853,
           0.006741199642419815,
           0.015058952383697033,
           0.013866757042706013,
           0.009057154878973961,
           0.01234621461480856,
           0.0061834403313696384,
           0.012453954666852951,
           0.0009229508577845991,
           0.009313314221799374,
           0.010359136387705803,
           0.011034135706722736,
           0.1212511956691742,
           0.0006663443637080491,
           0.011260691098868847,
           0.012702278792858124,
           0.10168629139661789,
           0.07523451000452042,
           0.014287082478404045
          ],
          [
           0.0013314225943759084,
           0.01046223845332861,
           0.015411864034831524,
           0.007394303102046251,
           0.011513092555105686,
           0.01462777890264988,
           0.06789302825927734,
           0.010082808323204517,
           0.06272399425506592,
           0.0010776473209261894,
           0.012339453212916851,
           0.0010050766868516803,
           0.0011165138566866517,
           0.009316256269812584,
           0.12027794122695923,
           0.01958400011062622,
           0.008127971552312374,
           0.0157140102237463,
           0.016051020473241806,
           0.0010571671882644296,
           0.008638313971459866,
           0.0036134086549282074,
           0.0022230648901313543,
           0.025214795023202896,
           0.009863173589110374,
           0.01435688603669405,
           0.009789633564651012,
           0.014090729877352715,
           0.0004212732892483473,
           0.008567806333303452,
           0.010389196686446667,
           0.005227041430771351,
           0.0129947355017066,
           0.006626010872423649,
           0.0009018690325319767,
           0.008597740903496742,
           0.01155698299407959,
           0.008949728682637215,
           0.008880896493792534,
           0.03607286885380745,
           0.006412707734853029,
           0.0013238330138847232,
           0.00950451847165823,
           0.011741634458303452,
           0.005474814213812351,
           0.013600529171526432,
           0.004903289023786783,
           0.015518905594944954,
           0.015863345935940742,
           0.009208987466990948,
           0.011554337106645107,
           0.002111422596499324,
           0.03296038508415222,
           0.012324808165431023,
           0.015643885359168053,
           0.015761742368340492,
           0.01187684666365385,
           0.008295812644064426,
           0.0012598433531820774,
           0.01052034180611372,
           0.0053830621764063835,
           0.04658334702253342,
           0.08570963144302368,
           0.03237830474972725
          ],
          [
           0.007631602697074413,
           0.011638465337455273,
           0.0143359350040555,
           0.02125275880098343,
           0.010673822835087776,
           0.010095627047121525,
           0.08425582200288773,
           0.011981506831943989,
           0.01952802948653698,
           0.0013251827331259847,
           0.04442872479557991,
           0.006255134008824825,
           0.002895108424127102,
           0.003942981828004122,
           0.012521157041192055,
           0.014320981688797474,
           0.00444467319175601,
           0.012201373465359211,
           0.01266888901591301,
           0.0022691332269459963,
           0.002830330515280366,
           0.009238462895154953,
           0.008251231163740158,
           0.030741576105356216,
           0.008086898364126682,
           0.014029762707650661,
           0.00400175154209137,
           0.014395922422409058,
           0.011402280069887638,
           0.010450144298374653,
           0.011923117563128471,
           0.011163740418851376,
           0.01493746042251587,
           0.0018181304913014174,
           0.008353366516530514,
           0.006929103285074234,
           0.01036374643445015,
           0.011929023079574108,
           0.05578014999628067,
           0.017668554559350014,
           0.009618557058274746,
           0.001782950246706605,
           0.010276823304593563,
           0.013057617470622063,
           0.00941524375230074,
           0.014965154230594635,
           0.007810051087290049,
           0.010542169213294983,
           0.049657586961984634,
           0.013574453070759773,
           0.013055569492280483,
           0.0025557067710906267,
           0.018192660063505173,
           0.016841351985931396,
           0.013656281866133213,
           0.026126928627490997,
           0.12646080553531647,
           0.005243191495537758,
           0.002332241740077734,
           0.005537972319871187,
           0.005245637614279985,
           0.009066480211913586,
           0.01876162178814411,
           0.02326124906539917
          ],
          [
           0.005172318313270807,
           0.011889123357832432,
           0.017440492287278175,
           0.011162388138473034,
           0.01261467207223177,
           0.0837249755859375,
           0.09194423258304596,
           0.002219380345195532,
           0.01301236916333437,
           0.0026284419000148773,
           0.015512487851083279,
           0.002770716091617942,
           0.01833862066268921,
           0.012833916582167149,
           0.009641274809837341,
           0.018274495378136635,
           0.01433905865997076,
           0.012531557120382786,
           0.08601642400026321,
           0.0032051177695393562,
           0.002631106646731496,
           0.003437760751694441,
           0.003568917280063033,
           0.02241806499660015,
           0.007971853017807007,
           0.010730098932981491,
           0.012490526773035526,
           0.009517945349216461,
           0.0032819786574691534,
           0.010316343978047371,
           0.011664080433547497,
           0.01944064348936081,
           0.014211030676960945,
           0.004144872073084116,
           0.0013074122834950686,
           0.010085303336381912,
           0.013366076163947582,
           0.010964379645884037,
           0.03985358402132988,
           0.0038545685820281506,
           0.009271184913814068,
           0.04935035482048988,
           0.011866121552884579,
           0.005618283525109291,
           0.013935844413936138,
           0.012163500301539898,
           0.00806848518550396,
           0.014453461393713951,
           0.022371193394064903,
           0.015176445245742798,
           0.011808577924966812,
           0.00849997904151678,
           0.009236211888492107,
           0.014040659181773663,
           0.013506374321877956,
           0.015792423859238625,
           0.021811753511428833,
           0.0025257503148168325,
           0.017585085704922676,
           0.008426136337220669,
           0.01395911630243063,
           0.016444358974695206,
           0.014192823320627213,
           0.009367186576128006
          ],
          [
           0.004136050119996071,
           0.005012438166886568,
           0.014740124344825745,
           0.01635323092341423,
           0.01561033632606268,
           0.013175691477954388,
           0.048635054379701614,
           0.0040745302103459835,
           0.008611081168055534,
           0.0017186586046591401,
           0.008340390399098396,
           0.0028480554465204477,
           0.027026714757084846,
           0.011662241071462631,
           0.0073656952008605,
           0.008110115304589272,
           0.007559486199170351,
           0.012851842679083347,
           0.015614376403391361,
           0.00824472215026617,
           0.00224172230809927,
           0.0022680875845253468,
           0.012987726368010044,
           0.015991708263754845,
           0.02262190915644169,
           0.016232343390583992,
           0.009500623680651188,
           0.010786965489387512,
           0.0026658622082322836,
           0.006716980133205652,
           0.029363416135311127,
           0.009142598137259483,
           0.00511451018974185,
           0.0031620285008102655,
           0.0013007494853809476,
           0.0066872588358819485,
           0.008281051181256771,
           0.007851329632103443,
           0.009569388814270496,
           0.002900887280702591,
           0.01212390512228012,
           0.05765082314610481,
           0.010380229912698269,
           0.002080261241644621,
           0.12287843227386475,
           0.011796382255852222,
           0.023884212598204613,
           0.012933018617331982,
           0.007917477749288082,
           0.027889514341950417,
           0.06856746226549149,
           0.01626177877187729,
           0.010324847884476185,
           0.007570217363536358,
           0.010672279633581638,
           0.016720741987228394,
           0.012451911345124245,
           0.003611046588048339,
           0.016696656122803688,
           0.016128001734614372,
           0.0642545148730278,
           0.020407255738973618,
           0.010606599040329456,
           0.00911441445350647
          ],
          [
           0.003895001020282507,
           0.011900879442691803,
           0.014752490445971489,
           0.02278420515358448,
           0.006197061389684677,
           0.013424745760858059,
           0.05199545621871948,
           0.015205394476652145,
           0.046524953097105026,
           0.0011906801955774426,
           0.006762048229575157,
           0.004907269962131977,
           0.0011532172793522477,
           0.004124879837036133,
           0.008794762194156647,
           0.009631719440221786,
           0.006850055418908596,
           0.012762492522597313,
           0.005920117255300283,
           0.01042646449059248,
           0.0014940985711291432,
           0.005501859821379185,
           0.017521923407912254,
           0.015105705708265305,
           0.023151349276304245,
           0.008258100599050522,
           0.006397633347660303,
           0.014241687022149563,
           0.003758101724088192,
           0.00800611637532711,
           0.008656692691147327,
           0.00522471172735095,
           0.010295198298990726,
           0.006285592447966337,
           0.002110953675583005,
           0.007093868684023619,
           0.008259730413556099,
           0.008966897614300251,
           0.00833061896264553,
           0.013377886265516281,
           0.010743602178990841,
           0.0017054233467206359,
           0.02987079508602619,
           0.013818462379276752,
           0.017882347106933594,
           0.010029241442680359,
           0.25864243507385254,
           0.011310802772641182,
           0.009283664636313915,
           0.009926559403538704,
           0.0169401653110981,
           0.0034752318169921637,
           0.011065333150327206,
           0.013325209729373455,
           0.01504941564053297,
           0.021560145542025566,
           0.01161024160683155,
           0.011019408702850342,
           0.0011780429631471634,
           0.018749356269836426,
           0.0358416885137558,
           0.004720332566648722,
           0.007439321372658014,
           0.013570181094110012
          ],
          [
           0.005848601460456848,
           0.00618076603859663,
           0.01654738560318947,
           0.013471460901200771,
           0.01502461452037096,
           0.007479492109268904,
           0.06848030537366867,
           0.01069608423858881,
           0.020093996077775955,
           0.0017914880299940705,
           0.013470948673784733,
           0.0038043404929339886,
           0.001294161076657474,
           0.03604349493980408,
           0.015424163080751896,
           0.009006025269627571,
           0.019609736278653145,
           0.010498637333512306,
           0.09479061514139175,
           0.008480913937091827,
           0.0014144818997010589,
           0.006052638869732618,
           0.016640296205878258,
           0.01828119531273842,
           0.0045482260175049305,
           0.018741803243756294,
           0.04485655575990677,
           0.016037948429584503,
           0.0049261609092354774,
           0.01159198209643364,
           0.010203012265264988,
           0.008755042217671871,
           0.015270542353391647,
           0.009634752757847309,
           0.00456204591318965,
           0.010255520232021809,
           0.012146975845098495,
           0.020276878029108047,
           0.01074577122926712,
           0.00851821806281805,
           0.011007644236087799,
           0.0014053345657885075,
           0.016504798084497452,
           0.009104082360863686,
           0.01231774315237999,
           0.013250407762825489,
           0.008440290577709675,
           0.013233866542577744,
           0.039297763258218765,
           0.011904893442988396,
           0.013381091877818108,
           0.0059821950271725655,
           0.011716707609593868,
           0.01807258278131485,
           0.01273369137197733,
           0.09565424919128418,
           0.012013459578156471,
           0.009241841733455658,
           0.0004988695727661252,
           0.010754305869340897,
           0.00578645896166563,
           0.007689096964895725,
           0.012086393311619759,
           0.016424961388111115
          ],
          [
           0.004589233547449112,
           0.01285697054117918,
           0.017308758571743965,
           0.008771723136305809,
           0.009704316966235638,
           0.005145675037056208,
           0.05610710754990578,
           0.008743668906390667,
           0.008488588035106659,
           0.006915263365954161,
           0.013895433396100998,
           0.0037419626023620367,
           0.011407535523176193,
           0.019905420020222664,
           0.018976805731654167,
           0.017524387687444687,
           0.01719776913523674,
           0.007025075610727072,
           0.028637606650590897,
           0.01700069196522236,
           0.01459739450365305,
           0.002780655398964882,
           0.0013014312135055661,
           0.013754167594015598,
           0.006811068393290043,
           0.011550788767635822,
           0.02504052221775055,
           0.016258517280220985,
           0.0027629632968455553,
           0.015362638980150223,
           0.009320535697042942,
           0.011293093673884869,
           0.013915724121034145,
           0.010280991904437542,
           0.0016787059139460325,
           0.010622965171933174,
           0.017036613076925278,
           0.15272654592990875,
           0.015375083312392235,
           0.007833419367671013,
           0.015486589632928371,
           0.05291248857975006,
           0.01391708105802536,
           0.00851564109325409,
           0.014831180684268475,
           0.007851590402424335,
           0.009414042346179485,
           0.014898265711963177,
           0.015733763575553894,
           0.014049132354557514,
           0.012525811791419983,
           0.020543044432997704,
           0.01108968909829855,
           0.01548572164028883,
           0.011392781510949135,
           0.031033514067530632,
           0.007704499643296003,
           0.0022387688513845205,
           0.009236657060682774,
           0.011101332493126392,
           0.009093633852899075,
           0.015401270240545273,
           0.015297274105250835,
           0.005998427979648113
          ],
          [
           0.0064194281585514545,
           0.023827433586120605,
           0.016356967389583588,
           0.014427579939365387,
           0.011411396786570549,
           0.011142401956021786,
           0.0691806897521019,
           0.016692617908120155,
           0.01747395470738411,
           0.0014564162120223045,
           0.017400352284312248,
           0.002919097663834691,
           0.0019075185991823673,
           0.009665893390774727,
           0.013303174637258053,
           0.017207304015755653,
           0.015597964636981487,
           0.018387321382761,
           0.0074487230740487576,
           0.026393089443445206,
           0.008366590365767479,
           0.0054155453108251095,
           0.018990598618984222,
           0.023263875395059586,
           0.04266950115561485,
           0.018696945160627365,
           0.015097416006028652,
           0.02086140587925911,
           0.004015057347714901,
           0.01881307177245617,
           0.015161698684096336,
           0.013926609419286251,
           0.013064018450677395,
           0.009206769987940788,
           0.0035509171430021524,
           0.010336129926145077,
           0.019476722925901413,
           0.014305876567959785,
           0.01105901412665844,
           0.01877547614276409,
           0.013390026055276394,
           0.0017374241724610329,
           0.01056581363081932,
           0.012239026837050915,
           0.006600574590265751,
           0.01734798215329647,
           0.006163179408758879,
           0.021837931126356125,
           0.013726955279707909,
           0.03635095804929733,
           0.013497336767613888,
           0.009042645804584026,
           0.016359353438019753,
           0.017250584438443184,
           0.014534699730575085,
           0.00641194824129343,
           0.014198699034750462,
           0.008293255232274532,
           0.0012412845389917493,
           0.03174639120697975,
           0.0363495796918869,
           0.020437952131032944,
           0.01673472858965397,
           0.030269060283899307
          ]
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Input 1 (How will you pitch your idea to the investors?) - Layer 0",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Input 2 (What's the musical pitch of that note?) - Layer 0",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Expert vs Token Heatmaps for Two Inputs - Layer 0"
        },
        "width": 2000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Experts"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_heatmap(probabilities, tokens, title, layer_num):\n",
    "    return go.Heatmap(\n",
    "        z=probabilities,\n",
    "        x=[f'Expert {i}' for i in range(probabilities.shape[1])],\n",
    "        y=tokens,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Probability'),\n",
    "        hovertemplate='Token: %{y}<br>Expert: %{x}<br>Probability: %{z:.4f}<extra></extra>'\n",
    "    )\n",
    "\n",
    "# Convert router logits to probabilities for both inputs\n",
    "probabilities_1 = F.softmax(last_layer_router_logits, dim=-1).cpu().numpy()\n",
    "probabilities_2 = F.softmax(last_layer_router_logits_2, dim=-1).cpu().numpy()\n",
    "\n",
    "# Create lists of tokens for both inputs\n",
    "tokens_1 = [tokenizer.decode([token_id]) for token_id in tokenizer.encode(input_text, return_tensors=\"pt\")[0]]\n",
    "tokens_2 = [tokenizer.decode([token_id]) for token_id in tokenizer.encode(input_text_2, return_tensors=\"pt\")[0]]\n",
    "\n",
    "# Create DataFrames for the heatmaps\n",
    "df_1 = pd.DataFrame(probabilities_1, index=tokens_1)\n",
    "df_2 = pd.DataFrame(probabilities_2, index=tokens_2)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(f'Input 1 ({input_text}) - Layer {layer_num}', f'Input 2 ({input_text_2}) - Layer {layer_num}'))\n",
    "\n",
    "# Add heatmaps to subplots\n",
    "fig.add_trace(create_heatmap(probabilities_1, tokens_1, f'Input 1 - Layer {layer_num}', layer_num), row=1, col=1)\n",
    "fig.add_trace(create_heatmap(probabilities_2, tokens_2, f'Input 2 - Layer {layer_num}', layer_num), row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'Expert vs Token Heatmaps for Two Inputs - Layer {layer_num}',\n",
    "    xaxis_title='Experts',\n",
    "    yaxis_title='Tokens',\n",
    "    width=2000,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plot as an HTML file (optional)\n",
    "fig.write_html(f\"expert_token_heatmaps_comparison_layer_{layer_num}.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
